[
  {
    "content": "1. Introduction Table Of Contents Environment Perception Open Data Civil IoT Taiwan Civil IoT Taiwan Data Service Platform References Environment Perception Human beings are full of curiosity about the changes in the natural world, which can probably trace back to the activities of ancient agricultural civilizations, looking up at the stars and stargazing at night. By the Renaissance in the 16th century AD, Copernicus deduced the “heliocentric theory” based on the existing astronomical data. After Galileo and Newton, it became the origin of modern science. Since modern times, due to the rapid development of semiconductor manufacturing technology, the tools that people use to perceive changes in the living environment have become more and more diverse, sophisticated and miniaturized. In order to catch up with the ever-changing trends of information technology, combined with the real-time perception of sensors and the connection of the Internet, data transmission is getting faster and more popular, and a large amount of observation data is generated, transmitted and archived. In response to massive data, scientists have been working hard to find the laws of environmental changes, explore the relationship between these laws and disasters, and obtain prediction results to improve people’s quality of life, thereby improving the efficiency of disaster prevention and relief, and promoting a more friendly relationship between people and the environment. The goal of creating a beautiful and sustainable living environment has become a significant issue of common concern to individuals, groups, societies and countries in the world today.\nOpen Data “Open data” refers to a type of electronic data (including but not limited to text, data, pictures, videos, sounds, etc.) that is publicly permitted and encouraged to be used by all sectors of society through certain procedures and formats. According to the definition of the Open Knowledge Foundation (https://opendefinition.org/od/2.1/en/), open data must meet the following conditions:\nOpen license or status: The content of the open data must be in the public domain and released with an open license without any additional restrictions. Access: The content of open data must be freely accessible from the Internet, but under reasonable conditions, access to open data may also allow a one-time fee and allow for conditional incidental terms. Machine readability: The content of open data must be easily accessible, processed and modified by computers. Open format: The content of open data must be in an available form, which can be accessed using free or open software. With the prevalence of open trends such as open source code, open government, and open science, open data has gradually become a standard that governments and scientific communities follow when engaging in various policy advocacy and academic research. The recently booming environment-aware IoT has become one of the most anticipated open data projects due to its widespread distribution in the public domain and its observed environmental information being closely related to the public.\nAt present, most of the common open data on the Internet use data formats such as JSON, CSV or XML. The full name of JSON format is Javascript Object Notation, which is a lightweight structured data exchange standard. Its content consists of properties and values, which is easy for computers or users to read and use, and is often used for data presentation, transmission and storage on the website.\nThe full name of the CSV format is Comma-Separated Values. As the name suggests, it is a form of storing table data in plain text, where each data is a column and each attribute in the data is a column. All properties must be stored as plain text in a specific order, with a specific symbol to separate different properties. Commonly used for file import and export of applications, network data transmission, historical data storage, etc.\nThe full name of the XML format is Extensible Markup Language, which is an extensible markup language simplified from the Standard Generalized Markup Language (SGML), allowing users to define the required tags. It is often used for the presentation of document file data and the exchange of network data in order to create documents containing structured information.\nCurrently, open data commonly used in Taiwan are aggregated on the following platforms:\nTaiwan government open data platform (https://data.gov.tw/en/) Taiwan open weather data (https://opendata.cwb.gov.tw/devManual/insrtuction) Taiwan EPA open data platform (https://data.epa.gov.tw/en) Civil IoT Taiwan The “Civil IoT Taiwan” project is developed to address the four urgent needs of the public in order to integrate and close to the public services related to people’s livelihood, including air quality, earthquake, water resources, disaster prevention and other issues. In the “Digital Technology” section of the “Forward-looking Infrastructure Development Program” project, the Ministry of Science and Technology, the Ministry of Communications, the Ministry of Economic Affairs, the Ministry of the Interior, the Environmental Protection Agency, the Academia Sinica, and the Council of Agriculture have jointly constructed a large-scale inter-ministerial government project, which applies big data, artificial intelligence, and Internet of things technologies to build various smart life service systems to help the government and the public face the challenges brought about by environmental changes. At the same time, this project also considers the experience of different end-users, including government decision-making units, academia, industry, and the general public. The goal is to enhance intelligent governance, assist the development of industry/academia, and improve the happiness of the public.\nAt present, the four major areas covered by the Civil IoT Taiwan project are:\nWater resources: The Civil IoT Taiwan project cooperates with the water conservancy units of the central and local governments to develop and deploy a variety of water resources sensors, and build various hydrological observation facilities and farmland irrigation water sensors. The goal of this project is to effectively integrate surface, groundwater and emerging water source information and related monitoring information, and to establish a water resource IoT portal and a dynamic analysis and management platform for irrigation water distribution, through big data analysis, so as to strengthen the flood control operation system of each river bureau and establish he sewer cloud management cloud that can perform remote, automated and intelligent management for relevant units at all levels, achieve the goal of combined use, and promote various applications such as intelligent control and management of water sources, dynamic analysis and management of irrigation water distribution, value-added applications of sewage and sewer, and road flood warning. With the opening of data, it will promote the development of water resources data applications and public-private partnership decision-making. Air quality: The Civil IoT Taiwan project, in conjunction with the Environmental Protection Agency, the Ministry of Economic Affairs, the Ministry of Science and Technology, and Academia Sinica, started with the deployment of air quality sensing infrastructure, developed air quality sensors, and established a sensor performance testing and verification center. And a large number of air quality sensors for different purposes are widely distributed throughout Taiwan. Through the collection of a large amount of sensing data at smaller time and space scales, this project establishes a computing operation platform for the Internet of Things and a smart environment sensing data center, and builds a visualization platform for air quality sensing data. On the one hand, it promotes the development of the Internet of Things for air quality sensing, and on the other hand, it provides high-resolution evidence data for intelligent environmental protection inspections to identify polluted hot spots and effectively conduct environmental governance. At the same time, the project also strengthens the research and development capacity of domestic own technology during the plan period, and establishes the development of domestic independent airborne product sensing technology. Earthquakes: The Civil IoT Taiwan project also significantly increases the number of on-site earthquake quick-reporting stations to provide high-density, high-quality earthquake quick-reporting information for the common seismic activity in Taiwan. At the same time, seismic and geophysical observatories were added and upgraded to improve the quality and resolution of observational data, including Global Navigation Satellite System (GNSS) stations, underground seismic stations, strong earthquake stations, geomagnetic stations, and groundwater stations. In addition, the project is enhancing its GNSS stations, underground seismic stations, and drone observations for regional monitoring of the Datun Volcano. In view of Taiwan’s unique island characteristics, it has also strengthened its forecasting capabilities for strong earthquakes and tsunamis, and expanded submarine optical cables and related submarine and land stations. In addition, through the integration and application of big data, the project has constructed an integrated data management system for Taiwan’s seismic and geophysical data, which can facilitate the integration of local and regional earthquake quick report information, strengthen the monitoring of Taiwan’s fault areas and the Datun volcanic area, and provide fast and complete earthquake quick report information transmission, thereby providing the public with real-time early warning of strong earthquakes and promoting the development of the earthquake disaster prevention industry. Disaster prevention and relief: The Civil IoT Taiwan project gathers a total of 58 types of warning data covering air, water, land, disaster and people’s livelihood into a single “Civil Alert Information Platform” to provide the public with real-time disaster prevention and relief information. Moreover, incorporating the emergency management information cloud system (EMIC2.0) and other decision-making assisting systems, the platform provides disaster prevention personnel with various disaster situations, notifications and disaster relief resources to assist in various decision-making work, and at the same time, the relevant historical data is collected and released in a unified data format, which is made available for disaster prevention industry to analyze and use, and promote the development of disaster information industry chain. Civil IoT Taiwan Data Service Platform In addition, the Civil IoT Taiwan project also plans the Civil IoT Taiwan data service platform to accommodate various data generated by the Civil IoT Taiwan project and provide stable and high-quality sensing data for various environmental management purposes. In the spirit of open data, the platform adopts a unified data format, provides real-time data interfaces and historical data query services, improves user browsing and search speed, and sets up sensor data storage mechanisms to help scientific computing and artificial intelligence applications. The establishment of the Civil IoT Taiwan data service platform can not only narrow the gap in environmental information, but also provide more real-time and comprehensive environmental data, allowing the public to view perception information and temporal and spatial changes in real time, and to understand the living environment at any time. The data provided by the platform can also be used as a basis for industrial value-added, thereby stimulating the creativity of the people and providing high-quality services for solving the problems of the people.\nCurrently, the Civil IoT Taiwan Data Service Platform uses the open data format of the OGC SensorThings API. Please refer to the following slides and pictures for relevant data format descriptions and data content in various fields:\nIntroduction to Civil IoT Taiwan Data Service Platform [PPT] Introduction to Civil IoT Taiwan Data Service Platform and OGC SensorThings API [Video] (in Chinese) Water Resources [PPT] Water Resources IoT [Video] (in Chinese) Air Quality [PPT] Environment Quality Sensing IoT [Video] (in Chinese) [PPT] Deployment of Micro PM2.5 Sensors [Video] (in Chinese) Earthquake [PPT] Joint Sea-Land Earthquake Observation [Video] (in Chinese) [PPT] Composite Earthquake Quick Report [Video] (in Chinese) Disaster prevention and relief [PPT] Civil Alert Open Data [Video] (in Chinese) [PPT] Integration of Disaster Prevention and Relief Information Systems [Video] (in Chinese) In order to allow more people to participate in the Civil IoT Taiwan project and its data platform, since 2018, the Civil IoT Taiwan project has also held a series of data application competitions, data innovation hackathons, and physical and virtual exhibitions. The project also designed a series of training materials and business coaching, from the establishment and development of the team, from the inception to the formation of the idea, and from the conception to the implementation of the application service. After several years of accumulation, the project has successfully developed a successful case, which shows that the Civil IoT Taiwan project is not only a hardware construction for government units, but also has successfully transformed into a basic information construction for people’s livelihood. The project is providing a steady stream of high-quality sensor data for improving people’s lives and promoting more innovative, convenient, and caring information services.\nFor examples of applications and solutions in various fields of Civil IoT Taiwan data, please refer to the following website resources:\nCivil IoT Taiwan Service and Solution Guide: Water resources Civil IoT Taiwan Service and Solution Guide: Air qualuty Civil IoT Taiwan Service and Solution Guide: Earthquake Civil IoT Taiwan Service and Solution Guide: Disaster prevention and relief References Open Definition: defining open in open data, open content and open knowledge. Open Knowledge Foundation (https://opendefinition.org/od/2.1/en/) Civil IoT Taiwan (https://ci.taiwan.gov.tw) Civil IoT Taiwan Virtual Expo: Dialogue in Civil IoT (https://ci.taiwan.gov.tw/dialogue-in-civil-iot) Civil IoT Taiwan Service and Solution Guide (https://www.civiliottw.tadpi.org.tw) Civil IoT Taiwan Data Service Platform (https://ci.taiwan.gov.tw/dsp/) XML - Wikipedia (https://en.wikipedia.org/wiki/XML) JSON - Wikipedia (https://en.wikipedia.org/wiki/JSON) Comma-separated values - Wikipedia (https://en.wikipedia.org/wiki/Comma-separated_values) Standard Generalized Markup Language - Wikipedia (https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language) OGC SensorThings API Documentation (https://developers.sensorup.com/docs/) ",
    "description": "Introduction",
    "tags": [
      "Introduction"
    ],
    "title": "1. Introduction",
    "uri": "/en/ch1/"
  },
  {
    "content": "2. Overview of the Materials In this topic, we will introduce the overall structure of the developed materials of Civil IoT Taiwan Open Data, as well as the programming language Python and the development platform Google Colab used in the materials. In addition to conceptual descriptions, we also provide a large number of extended learning resources from shallow to deep, so that interested readers can further explore and learn according to their own needs.\n2.1. Material ArchitectureIntroduction of the material architecture\n2.2. Material ToolsA brief introduction of the programming language Python and the development platform Google Colab used in the materials\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "2. Overview of the Materials",
    "uri": "/en/ch2/"
  },
  {
    "content": "4. Time Series Data Analysis In this topic, we will introduce time series data analysis methods for IoT data. We will develop the following three units for a more in-depth exploration by using the Civil IoT Taiwan Data Service Platform.\n4.1. Time Series Data ProcessingWe use the sensing data of Civil IoT Taiwan Data Service Platform to guide readers to understand the use of moving average, perform periodic analysis of time series data, and then disassemble the time series data into long-term trends, seasonal changes and residual fluctuations. At the same time, we apply the existing Python language suites to perform change point detection and outlier detection to check the existing Civil IoT Taiwan data, and discuss potential implications of such values detected.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "4. Time Series Data Analysis",
    "uri": "/en/ch4/"
  },
  {
    "content": "7. System Integration and Applications In this theme, we will focus on the integration and application of the Civil IoTTaiwan Open Data and other application software, and through the professional functions of other application software, to further deepen and develop the value of the Civil IoT Taiwan Open Data.\n7.3. Leafmap ApplicationsWe introduce the capability of leafmap package to use different types of data for geographic information representation and spatial analysis in Civil IoT Taiwan Data Service Platform, and demonstrate the combination of leafmap and streamlit packages to build Web GIS applications. Through cross-domain and cross-tool resource integration, readers will be able to expand their imagination of the future of data analysis and information services.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "7. System Integration and Applications",
    "uri": "/en/ch7/"
  },
  {
    "content": "This set of materials includes seven themes including “Introduction”, “Overview of the Materials”, “Data Access Method”, “Data Analysis in Time Dimension”, “Data Analysis in Space Dimension”, “Data Applications” and “System Applications”. The major themes are described as follows:\nIntroduction: We introduce the core of the entire project - “Civil IoT Taiwan Project”, including various achievements of the project in the past few years, as well as its open data, covering water resources, air quality, earthquakes, and disaster prevention and relief fields, which can bring daily life various applications and services in. We also present various existing materials, videos and successful cases, so that everyone can deeply understand the importance of “Civil IoT Taiwan project” to people’s livelihood.\nOverview of the Materials: We present the structure of the entire material, as well as the Python programming language and Google Colab platform used in the material. Through simple text descriptions, we guide readers to quickly understand the structure of the entire material, and provides a rich list of external resources for those who are interested and in need to further explore related technologies.\nData Access Method: We introduce how to use simple Python syntax to directly access the data of the Civil IoT Taiwan Data Service Platform through the pyCIOT tool we developed, and cut out two units for a more in-depth introduction according to different needs:\nBasic Data Access Method: We introduce how to obtain the latest sensing data of a single station for different aspects of water resources, air quality, earthquake, and disaster information in the Civil IoT Taiwan Data Service Platform for people’s livelihood, as well as how to obtain a list of all stations, and the latest current data of all stations. Specific Spatio-temporal Data Access Method: We introduce how to obtain the data of a certain station at a specific time or time range, find the latest current data of the nearest station, and find the current latest data of all stations around the given coordinates from the Civil IoT Taiwan Data Service Platform. In addition to introducing data access methods, we also intersperse with basic exploratory data analysis (EDA) methods, use commonly used statistical methods to describe the data characteristics of different aspects of the data, and draw simple diagrams to let readers learn by doing, and experience what it’s like to analyze data firsthand.\nData Analysis in Time Dimension: In view of the time series characteristics of IoT data, we design three topics to demonstrate time-series data analysis using Civil IoT Taiwan data.\nTime Series Data Processing: We use the sensing data of Civil IoT Taiwan Data Platform to guide readers to understand the use of moving averages, perform periodic analysis on time series data, and then disassemble the time series data into long-term trends, seasonal changes, periodic changes, and random fluctuations. At the same time, change point detection and outlier detection techniques are applied to check the existing Civil IoT Taiwan Data by applying the existing Python packages, and the possible meanings behind the detection results are discussed. Time Series Data Forecast: We use sensory data from the Civil IoT Taiwan Data Platform and apply existing Python data science packages (such as scikit-learn, Kats, etc.) to compare different data prediction methods in a hands-on fashion. Combine the forecast results, we present the data in a graph, and discuss the implications behind the forecast results for different datasets and temporal resolutions. Then, we also discuss possible derivative applications. Time Series Data Clustering: We introduce more advanced data clustering analysis. We first introduce two time series feature extraction methods, namely Fourier transform and wavelet transform, and briefly explain the difference between the two transform methods. We then introduce two distance functions for time series data, Euclidean distance and Dynamic Time Warping (DTW) distance, and apply existing clustering packages using these two functions. Finally, we discuss the implications behind clustering results using different datasets and different temporal resolutions in real-world applications, as well as possible derived applications. Data Analysis in Space Dimension: Given the geospatial characteristics of Civil IoT Taiwan data, we introduce a series of spatial data analysis methods according to different needs and applications.\nGeospatial Filtering: We use the earthquake and disaster prevention and relief data of the Civil IoT Taiwan Data Platform, superimpose the administrative region boundary map data obtained from the government’s open data platform, and generate a picture file of geospatial data distribution after generating the overlay map. Additionally, we demonstrate how to nest specific regions of geometric topology, output the nested results to a file, and perform drawing operations. Geospatial Analysis: We use sensing data from Civil IoT Taiwan Data Platform to introduce more advanced geospatial analysis. Using the GPS location coordinates of the sensors, we first use an existing Python package to find the largest convex polygon (Convex Hull) to frame the geographic area covered by the sensors; then we apply the Voronoi Diagram algorithm to cut the area on the map based on the distribution of the sensors, cutting out the sphere of influence of each sensor. For the area between the sensors, we apply different spatial interpolation algorithms, fill the values on the spatial map according to the sensor values, and produce the corresponding image output. Data Applications: In this topic, we will focus on the derivative applications after accessing the Civil IoT Taiwan data platform, and by importing other library suites and analysis algorithms, we will enhance the value and application services of Civil IoT Taiwan’s open data. We will develop the following three subtopics in turn:\nA First Look at Machine Learning: We use air quality and water level category data, combined with weather observations, to perform predictive analysis of sensory values using machine learning packages. We demonstrate the standard process of machine learning, and describe how to evaluate the effectiveness of predictive analytics, and how to avoid the bias and overfitting problems that machine learning is prone to. Anomaly Detection: We use air quality category data to demonstrate sensor anomaly detection algorithms commonly used in Taiwanese micro air quality sensing data. In a learning-by-doing way, including data preparation, feature extraction, data analysis, statistics, and induction, the principle and implementation process of anomaly detection algorithms are reproduced step by step, allowing readers to experience how to superimpose basic data analysis and machine learning to achieve advanced and practical data application service. Dynamic Calibration Model: We use air quality category data to demonstrate the dynamic calibration algorithm for Taiwanese micro air quality sensors and official stations. In the way of learning by doing, including data preparation, feature extraction, machine learning, data analysis, statistics, etc., it reproduces the principle and implementation process of the sensor dynamic calibration model algorithm, allowing readers to experience how to gradually realize advanced and basic data analysis by overlaying and machine learning steps to provide practical data application services. System Applications: This topic focuses on the integration and application of Civil IoT Taiwan open data and other application software, and further deepens and exerts the value of Civil IoT Taiwan open data through the professional functions of other application software. Units we develop include:\nQGIS Applications: QGIS is a free and open-source GIS software. With this software, users can organize, analyze and draw different geospatial data and thematic maps, presenting the distribution and information types of geographical phenomena. We use QGIS to process Civil IoT Taiwan data, including water, air quality, earthquake, and disaster prevention and relief categories; and overlay different information by clicking and dragging to perform as described in the “Geospatial Filtering” and “Geospatial Analysis” topics. We also discuss the pros and cons of QGIS software and when to use it. Tableau Applications: Tableau is an easy-to-use, powerful data visualization software that can easily connect databases and data files in various formats to create various beautiful statistical charts. We demonstrate how to use drag-and-drop to import data from four aspects of Civil IoT Taiwan, including water resources, air quality, earthquakes, and disaster prevention and relief categories, and make simple charts to compare numerical data. Geospatial data is also overlaid on the map, and time-series data is presented in the form of graphs that show trends in values. In addition, through the interface of external data sources, we can integrate datasets from multiple different sources into a single map to generate beautiful reports in a very short time. Leafmap Applications: Leafmap is a Python development toolkit deeply integrated with Google Earth Engine to directly manipulate and visualize analysis results on platforms such as Google Colab or Jupyter Notebook. We demonstrate the use of leafmap to process Civil IoT Taiwan data, including categories such as water resources, air quality, earthquake, disaster prevention and relief, and build a simple GIS system combined with Google Earth Engine’s rich satellite imagery. At the same time, Streamlit is a tool for Python users to quickly build a web application framework. We demonstrate the integration of leafmap application and Streamlit to form a web version of GIS information service, which expands readers’ imagination space for data analysis and future information services. References Civil IoT Taiwan. https://ci.taiwan.gov.tw Civil IoT Taiwan Data Platform. https://ci.taiwan.gov.tw/dsp/ QGIS - A Free and Open Source Geographic Information System. https://qgis.org/ Tableau - Business Intelligence and Analytics Software. https://www.tableau.com/ Leafmap - A Python package for geospatial analysis and interactive mapping in a Jupyter environment. https://leafmap.org/ Streamlit - The fastest way to build and share data apps. https://streamlit.io/ Google Colaboratory. https://colab.research.google.com/ ",
    "description": "Introduction of the material architecture",
    "tags": [
      "Introduction"
    ],
    "title": "2.1. Material Architecture",
    "uri": "/en/ch2/ch2.1/"
  },
  {
    "content": "\nTable Of Contents Goal Package Installation and Importing Data Access Air Quality Data Water Level Data Meteorological Data Data Visualization Data Resample Moving Average Multi-line Charts Calendar Heatmap Data Quality Inspection Outlier Detection Change Point Detection) Missing Data Handling Data Decomposition References Time series data is data formed in the order of appearance in time. Usually, the time interval in the data will be the same (for example, one data every five minutes, or one data per hour), and the application fields are quite wide, such as financial information, space engineering, signal processing, etc. There are also many statistical related tools that can used in analysis. In addition, time series data is very close to everyday life. For example, with the intensification of global climate change, the global average temperature has become higher and higher in recent years, and the summer is unbearably hot. Also, certain seasons of the year tend to have particularly poor air quality, or certain times of the year tend to have worse air quality than others. If you want to know more about these changes in living environment, and how the corresponding sensor values change, you will need to use time series data analysis, which is to observe the relationship between data and time, and then get the results. This chapter will demonstrate using three types of data (air quality, water resources, weather) in the Civil IoT Taiwan Data Service Platform.\nGoal observe time series data using visualization tools check and process time series data decompose time series data to investigate its trend and seasonality Package Installation and Importing In this article, we will use the pandas, matplotlib, numpy, seaborn, statsmodels, and warnings packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. However, we will also use two additional packages that Colab does not have pre-installed: kats and calplot, which need to be installed by :\n!pip install --upgrade pip # Kats !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 # calplot !pip install calplot After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport warnings import calplot import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import statsmodels.api as sm import os, zipfile from datetime import datetime, timedelta from dateutil import parser as datetime_parser from statsmodels.tsa.stattools import adfuller, kpss from statsmodels.tsa.seasonal import seasonal_decompose from kats.detectors.outlier import OutlierDetector from kats.detectors.cusum_detection import CUSUMDetector from kats.consts import TimeSeriesData, TimeSeriesIterator from IPython.core.pylabtools import figsize Data Access We use pandas for data processing. Pandas is a data science suite commonly used in Python language. It can also be thought of as a spreadsheet similar to Microsoft Excel in a programming language, and its Dataframe object provided by pandas can be thought of as a two-dimensional data structure. The dimensional data structure can store data in rows and columns, which is convenient for various data processing and operations.\nThe topic of this paper is the analysis and processing of time series data. We will use the air quality, water level and meteorological data on the Civil IoT Taiwan Data Service Platform for data access demonstration, and then use the air quality data for further data analysis. Among them, each type of data is the data observed by a collection of stations for a long time, and the time field name in the dataframe is set to timestamp. Because the value of the time field is unique, we also use this field as the index of the dataframe.\nAir Quality Data Since we want to use long-term historical data in this article, we do not directly use the data access methods of the pyCIOT package, but directly download the data archive of “Academia Sinica - Micro Air Quality Sensors” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Air folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Air folder.\n!mkdir Air CSV_Air !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Air/{item}') The CSV_Air folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 74DA38C7D2AC), we need to read each CSV file and put the data for that station into a dataframe called air. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' air = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'device_id==@id') air = pd.concat([air, filtered], ignore_index=True) air.dropna(subset=['timestamp'], inplace=True) for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) air.at[i, 'timestamp'] = naive air.set_index('timestamp', inplace=True) !rm -rf Air CSV_Air Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nair.drop(columns=['device_id', 'SiteName'], inplace=True) air.sort_values(by='timestamp', inplace=True) air.info() print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 Water Level Data Like the example of air quality data, since we are going to use long-term historical data this time, we do not directly use the data access methods of the pyCIOT suite, but directly download the data archive of “Water Resources Agency - Groundwater Level Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Water folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Water folder.\n!mkdir Water CSV_Water !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') The CSV_Water folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 338c9c1c-57d8-41d7-9af2-731fb86e632c), we need to read each CSV file and put the data for that station into a dataframe called water. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' water = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') water = pd.concat([water, filtered], ignore_index=True) water.dropna(subset=['timestamp'], inplace=True) for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive water.set_index('timestamp', inplace=True) !rm -rf Water CSV_Water Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nwater.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) water.sort_values(by='timestamp', inplace=True) water.info() print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 Meteorological Data We download the data archive of “Central Weather Bureau - Automatic Weather Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Weather folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Weather folder.\n!mkdir Weather CSV_Weather !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Weather/{item}') The CSV_Weather folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code C0U750), we need to read each CSV file and put the data for that station into a dataframe called weather. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' weather = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') weather = pd.concat([weather, filtered], ignore_index=True) weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) weather.dropna(subset=['timestamp'], inplace=True) for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive weather.set_index('timestamp', inplace=True) !rm -rf Weather CSV_Weather Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nweather.drop(columns=['station_id'], inplace=True) weather.sort_values(by='timestamp', inplace=True) weather.info() print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN Above, we have successfully demonstrated the reading example of air quality data (air), water level data (water) and meteorological data (weather). In the following discussion, we will use air quality data to demonstrate basic time series data processing. The same methods can also be easily applied to water level data or meteorological data and obtain similar results. You are encouraged to try it yourself.\nData Visualization The first step in the processing of time series data is nothing more than to present the information one by one in chronological order so that users can see the changes in the overall data and derive more ideas and concepts for data analysis. Among them, using line charts to display data is the most commonly used data visualization method. For example, take the air quality data as an example:\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() Data Resample As can be seen from the air quality data sequence diagram in the above figure, the distribution of data is actually very dense, and the changes in data values are sometimes small and violent. This is because the current sampling frequency of air quality data is about once every five minutes, and the collected environment is the surrounding environment information in life, so data density and fluctuation are inevitable. Because sampling every 5 minutes is too frequent, it is difficult to show general trends in ambient air pollution. Therefore, we adopt the method of resampling to calculate the average value of the data in a fixed time interval, so as to present the data of different time scales. For example, we use the following syntax to resample at a larger scale (hour, day, month) sampling rate according to the characteristics of the existing air quality data:\nair_hour = air.resample('H').mean() # hourly average air_day = air.resample('D').mean() # daily average air_month = air.resample('M').mean() # monthly average print(air_hour.head()) print(air_day.head()) print(air_month.head()) PM25 timestamp 2018-08-01 00:00:00 18.500000 2018-08-01 01:00:00 20.750000 2018-08-01 02:00:00 24.000000 2018-08-01 03:00:00 27.800000 2018-08-01 04:00:00 22.833333 PM25 timestamp 2018-08-01 23.384615 2018-08-02 13.444444 2018-08-03 14.677419 2018-08-04 14.408451 2018-08-05 NaN PM25 timestamp 2018-08-31 21.704456 2018-09-30 31.797806 2018-10-31 37.217788 2018-11-30 43.228939 Then we plot again with the hourly averaged resampling data, and we can see that the curve becomes clearer, but the fluctuation of the curve is still very large.\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air_hour[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() Moving Average For the chart of the original data, if you want to see a smoother change trend of the curve, you can apply the moving average method. The idea is to set a sampling window on the time axis of the original data, move the position of the sampling window smoothly, and calculate the average value of all values in the sampling window.\nFor example, if the size of the sampling window is 10, it means that the current data and the previous 9 data are averaged each time. After such processing, the meaning of each data is not only a certain time point, but the average value of the original time point and the previous time point. This removes abrupt changes and makes the overall curve smoother, thereby making it easier to observe overall trends.\n# plt.figure(figsize=(15, 10), dpi=60) MA = air_hour MA10 = MA.rolling(window=500, min_periods=1).mean() MA.join(MA10.add_suffix('_mean_500')).plot(figsize=(20, 15)) # MA10.plot(figsize(15, 10)) The blue line in the above figure is the original data, and the orange line is the curve after moving average. It can be clearly found that the orange line can better represent the change trend of the overall value, and there is also a certain degree of regular fluctuation, which is worthy of further analysis.\nMulti-line Charts In addition to presenting the original data in the form of a simple line chart, another common data visualization method is to cut the data into several continuous segments periodically in the time dimension, draw line charts separately, and superimpose them on the same multi-line chart. For example, we can cut the above air quality data into four sub-data sets of 2019, 2020, 2021, and 2022 according to different years, and draw their respective line charts on the same multi-line chart, as shown in the figure below.\nair_month.reset_index(inplace=True) air_month['year'] = [d.year for d in air_month.timestamp] air_month['month'] = [d.strftime('%b') for d in air_month.timestamp] years = air_month['year'].unique() print(air) np.random.seed(100) mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False) plt.figure(figsize=(15, 10), dpi=60) for i, y in enumerate(years): if i \u003e 0: plt.plot('month', 'PM25', data=air_month.loc[air_month.year==y, :], color=mycolors[i], label=y) plt.text(air_month.loc[air_month.year==y, :].shape[0]-.9, air_month.loc[air_month.year==y, 'PM25'][-1:].values[0], y, fontsize=12, color=mycolors[i]) # plt.gca().set(xlim=(-0.3, 11), ylim=(2, 30), ylabel='PM25', xlabel='Month') # plt.yticks(fontsize=12, alpha=.7) # plt.title('Seasonal Plot of PM25 Time Series', fontsize=20) plt.show() In this multi-line chart, we can see that the 2019 data has a significant portion of missing values, while the 2022 data was only recorded till July at the time of writing. At the same time, it can be found that in the four-year line chart, the curves of different years all reached the lowest point in summer, began to rise in autumn, and reached the highest point in winter, showing roughly the same trend of change.\nCalendar Heatmap The calendar heat map is a data visualization method that combines the calendar map and the heat map, which can more intuitively browse the distribution of the data and find the regularity of different time scales. We use calplot, a calendar heatmap suite for Python, and input the daily PM2.5 averages. Then we select the specified color (the parameter name is cmap, and we set it to GnBu in the following example. for detailed color options, please refer to the reference materials), and we can get the effect of the following figure, in which blue represents the greater value, green or white means lower values, if not colored or a value of 0 means there is no data for the day. From the resulting plot, we can see that the months in the middle part (summer) are lighter and the months in the left part (winter) are darker, just in line with our previous observations using the multi-line chart.\n# cmap: color map (https://matplotlib.org/stable/gallery/color/colormap_reference.html) # textformat: specify the format of the text pl1 = calplot.calplot(data = air_day['PM25'], cmap = 'GnBu', textformat = '{:.0f}', figsize = (24, 12), suptitle = \"PM25 by Month and Year\") Data Quality Inspection After the basic visualization of time series data, we will introduce the basic detection and processing methods of data quality. We will use kats, a Python language data processing and analysis suite, to perform outlier detection, change point detection, and handling missing values in sequence.\nOutlier Detection Outliers are those values in the data that are significantly different from other values. These differences may affect our judgment and analysis of the data. Therefore, outliers need to be identified and then flagged, removed, or treated specially. .\nWe first convert the data stored in the variable air_hour from its original dataframe format to the TimeSeriesData format used by the kats package and save the converted data into a variable named air_ts. Then we re-plot the line chart of the time series data.\nair_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') air_ts.plot(cols=[\"PM25\"]) We then used the OutlierDetector tool in the kats suite to detect outliers in the time series data, where outliers were less than 1.5 times the first quartile (Q1) minus the interquartile range (IQR) or greater than The third quartile (Q3) value plus 1.5 times the interquartile range.\noutlierDetection = OutlierDetector(air_ts, 'additive') outlierDetection.detector() outlierDetection.outliers [[Timestamp('2018-08-10 16:00:00'), Timestamp('2018-08-10 17:00:00'), Timestamp('2018-08-20 00:00:00'), Timestamp('2018-08-23 03:00:00'), Timestamp('2018-08-23 04:00:00'), Timestamp('2018-09-02 11:00:00'), Timestamp('2018-09-11 00:00:00'), Timestamp('2018-09-13 14:00:00'), Timestamp('2018-09-13 15:00:00'), Timestamp('2018-09-13 16:00:00'), Timestamp('2018-09-15 08:00:00'), Timestamp('2018-09-15 09:00:00'), Timestamp('2018-09-15 10:00:00'), Timestamp('2018-09-15 11:00:00'), Timestamp('2018-09-22 05:00:00'), Timestamp('2018-09-22 06:00:00'), Timestamp('2018-10-26 01:00:00'), Timestamp('2018-11-06 13:00:00'), Timestamp('2018-11-06 15:00:00'), Timestamp('2018-11-06 16:00:00'), Timestamp('2018-11-06 19:00:00'), Timestamp('2018-11-06 20:00:00'), Timestamp('2018-11-06 21:00:00'), Timestamp('2018-11-06 22:00:00'), Timestamp('2018-11-07 07:00:00'), Timestamp('2018-11-07 08:00:00'), Timestamp('2018-11-07 09:00:00'), Timestamp('2018-11-09 00:00:00'), Timestamp('2018-11-09 01:00:00'), Timestamp('2018-11-09 02:00:00'), Timestamp('2018-11-09 03:00:00'), Timestamp('2018-11-10 02:00:00'), Timestamp('2018-11-10 03:00:00'), Timestamp('2018-11-16 01:00:00'), Timestamp('2018-11-16 02:00:00'), Timestamp('2018-11-16 03:00:00'), Timestamp('2018-11-16 04:00:00'), Timestamp('2018-11-21 00:00:00'), Timestamp('2018-11-21 18:00:00'), Timestamp('2018-11-21 19:00:00'), Timestamp('2018-11-25 08:00:00'), Timestamp('2018-11-30 14:00:00'), Timestamp('2018-12-01 06:00:00'), Timestamp('2018-12-01 16:00:00'), Timestamp('2018-12-01 17:00:00'), Timestamp('2018-12-15 02:00:00'), Timestamp('2018-12-19 03:00:00'), Timestamp('2018-12-19 04:00:00'), Timestamp('2018-12-19 05:00:00'), Timestamp('2018-12-19 06:00:00'), Timestamp('2018-12-19 07:00:00'), Timestamp('2018-12-19 08:00:00'), Timestamp('2018-12-19 10:00:00'), Timestamp('2018-12-19 11:00:00'), Timestamp('2018-12-19 12:00:00'), Timestamp('2018-12-19 13:00:00'), Timestamp('2018-12-19 14:00:00'), Timestamp('2018-12-19 15:00:00'), Timestamp('2018-12-19 16:00:00'), Timestamp('2018-12-19 17:00:00'), Timestamp('2018-12-20 03:00:00'), Timestamp('2018-12-20 04:00:00'), Timestamp('2018-12-20 05:00:00'), Timestamp('2018-12-20 06:00:00'), Timestamp('2018-12-20 07:00:00'), Timestamp('2018-12-20 08:00:00'), Timestamp('2018-12-20 11:00:00'), Timestamp('2018-12-20 12:00:00'), Timestamp('2018-12-20 13:00:00'), Timestamp('2018-12-20 14:00:00'), Timestamp('2018-12-20 15:00:00'), Timestamp('2019-01-05 02:00:00'), Timestamp('2019-01-05 08:00:00'), Timestamp('2019-01-05 09:00:00'), Timestamp('2019-01-05 22:00:00'), Timestamp('2019-01-19 06:00:00'), Timestamp('2019-01-19 07:00:00'), Timestamp('2019-01-19 08:00:00'), Timestamp('2019-01-19 09:00:00'), Timestamp('2019-01-19 13:00:00'), Timestamp('2019-01-19 14:00:00'), Timestamp('2019-01-19 15:00:00'), Timestamp('2019-01-25 18:00:00'), Timestamp('2019-01-25 19:00:00'), Timestamp('2019-01-25 20:00:00'), Timestamp('2019-01-26 00:00:00'), Timestamp('2019-01-26 01:00:00'), Timestamp('2019-01-26 02:00:00'), Timestamp('2019-01-26 03:00:00'), Timestamp('2019-01-26 04:00:00'), Timestamp('2019-01-30 06:00:00'), Timestamp('2019-01-30 11:00:00'), Timestamp('2019-01-30 12:00:00'), Timestamp('2019-01-30 13:00:00'), Timestamp('2019-01-30 14:00:00'), Timestamp('2019-02-02 16:00:00'), Timestamp('2019-02-02 17:00:00'), Timestamp('2019-02-02 18:00:00'), Timestamp('2019-02-02 19:00:00'), Timestamp('2019-02-02 20:00:00'), Timestamp('2019-02-03 03:00:00'), Timestamp('2019-02-03 04:00:00'), Timestamp('2019-02-03 05:00:00'), Timestamp('2019-02-03 06:00:00'), Timestamp('2019-02-03 07:00:00'), Timestamp('2019-02-03 10:00:00'), Timestamp('2019-02-03 11:00:00'), Timestamp('2019-02-03 12:00:00'), Timestamp('2019-02-03 13:00:00'), Timestamp('2019-02-03 22:00:00'), Timestamp('2019-02-03 23:00:00'), Timestamp('2019-02-07 05:00:00'), Timestamp('2019-02-07 06:00:00'), Timestamp('2019-02-16 22:00:00'), Timestamp('2019-02-16 23:00:00'), Timestamp('2019-02-18 18:00:00'), Timestamp('2019-02-18 20:00:00'), Timestamp('2019-02-18 21:00:00'), Timestamp('2019-02-19 10:00:00'), Timestamp('2019-02-19 11:00:00'), Timestamp('2019-02-19 12:00:00'), Timestamp('2019-02-19 13:00:00'), Timestamp('2019-02-19 14:00:00'), Timestamp('2019-02-19 15:00:00'), Timestamp('2019-02-19 16:00:00'), Timestamp('2019-02-19 23:00:00'), Timestamp('2019-02-20 00:00:00'), Timestamp('2019-02-20 03:00:00'), Timestamp('2019-03-02 17:00:00'), Timestamp('2019-03-03 06:00:00'), Timestamp('2019-03-05 13:00:00'), Timestamp('2019-03-09 23:00:00'), Timestamp('2019-03-12 01:00:00'), Timestamp('2019-03-16 01:00:00'), Timestamp('2019-03-16 02:00:00'), Timestamp('2019-03-16 03:00:00'), Timestamp('2019-03-20 00:00:00'), Timestamp('2019-03-20 01:00:00'), Timestamp('2019-03-20 02:00:00'), Timestamp('2019-03-20 03:00:00'), Timestamp('2019-03-20 11:00:00'), Timestamp('2019-03-27 00:00:00'), Timestamp('2019-03-27 01:00:00'), Timestamp('2019-04-05 03:00:00'), Timestamp('2019-04-18 17:00:00'), Timestamp('2019-04-20 16:00:00'), Timestamp('2019-05-10 07:00:00'), Timestamp('2019-05-22 20:00:00'), Timestamp('2019-05-23 03:00:00'), Timestamp('2019-05-23 16:00:00'), Timestamp('2019-05-26 18:00:00'), Timestamp('2019-05-27 05:00:00'), Timestamp('2019-07-28 01:00:00'), Timestamp('2019-08-23 08:00:00'), Timestamp('2019-08-24 02:00:00'), Timestamp('2019-08-24 03:00:00'), Timestamp('2019-08-24 04:00:00'), Timestamp('2019-08-24 05:00:00'), Timestamp('2019-08-24 07:00:00'), Timestamp('2019-08-24 08:00:00'), Timestamp('2019-12-10 11:00:00'), Timestamp('2019-12-10 12:00:00'), Timestamp('2019-12-10 13:00:00'), Timestamp('2019-12-10 20:00:00'), Timestamp('2019-12-11 04:00:00'), Timestamp('2019-12-16 20:00:00'), Timestamp('2019-12-17 11:00:00'), Timestamp('2020-01-03 15:00:00'), Timestamp('2020-01-05 08:00:00'), Timestamp('2020-01-05 09:00:00'), Timestamp('2020-01-06 08:00:00'), Timestamp('2020-01-07 10:00:00'), Timestamp('2020-01-07 15:00:00'), Timestamp('2020-01-10 11:00:00'), Timestamp('2020-01-15 08:00:00'), Timestamp('2020-01-22 14:00:00'), Timestamp('2020-01-22 17:00:00'), Timestamp('2020-01-22 22:00:00'), Timestamp('2020-01-22 23:00:00'), Timestamp('2020-01-23 00:00:00'), Timestamp('2020-01-23 01:00:00'), Timestamp('2020-01-23 02:00:00'), Timestamp('2020-01-23 10:00:00'), Timestamp('2020-01-23 11:00:00'), Timestamp('2020-01-23 12:00:00'), Timestamp('2020-01-23 13:00:00'), Timestamp('2020-01-23 15:00:00'), Timestamp('2020-01-23 16:00:00'), Timestamp('2020-01-23 17:00:00'), Timestamp('2020-01-23 18:00:00'), Timestamp('2020-01-23 20:00:00'), Timestamp('2020-01-23 21:00:00'), Timestamp('2020-01-23 22:00:00'), Timestamp('2020-01-23 23:00:00'), Timestamp('2020-01-24 00:00:00'), Timestamp('2020-01-24 01:00:00'), Timestamp('2020-01-24 02:00:00'), Timestamp('2020-01-24 03:00:00'), Timestamp('2020-02-12 10:00:00'), Timestamp('2020-02-12 11:00:00'), Timestamp('2020-02-12 12:00:00'), Timestamp('2020-02-12 13:00:00'), Timestamp('2020-02-12 14:00:00'), Timestamp('2020-02-12 19:00:00'), Timestamp('2020-02-12 20:00:00'), Timestamp('2020-02-12 22:00:00'), Timestamp('2020-02-12 23:00:00'), Timestamp('2020-02-13 20:00:00'), Timestamp('2020-02-14 00:00:00'), Timestamp('2020-02-14 01:00:00'), Timestamp('2020-02-15 10:00:00'), Timestamp('2020-02-19 08:00:00'), Timestamp('2020-02-19 09:00:00'), Timestamp('2020-02-19 10:00:00'), Timestamp('2020-02-25 02:00:00'), Timestamp('2020-02-25 03:00:00'), Timestamp('2020-03-09 07:00:00'), Timestamp('2020-03-18 21:00:00'), Timestamp('2020-03-18 22:00:00'), Timestamp('2020-03-19 01:00:00'), Timestamp('2020-03-20 04:00:00'), Timestamp('2020-03-21 09:00:00'), Timestamp('2020-03-21 10:00:00'), Timestamp('2020-03-28 22:00:00'), Timestamp('2020-04-15 03:00:00'), Timestamp('2020-04-28 03:00:00'), Timestamp('2020-04-28 04:00:00'), Timestamp('2020-05-01 13:00:00'), Timestamp('2020-05-01 15:00:00'), Timestamp('2020-05-01 23:00:00'), Timestamp('2020-05-02 00:00:00'), Timestamp('2020-11-17 14:00:00'), Timestamp('2020-11-17 20:00:00'), Timestamp('2020-11-17 21:00:00'), Timestamp('2020-11-17 22:00:00'), Timestamp('2020-11-18 19:00:00'), Timestamp('2020-11-18 20:00:00'), Timestamp('2020-11-18 23:00:00'), Timestamp('2020-11-19 00:00:00'), Timestamp('2020-11-19 01:00:00'), Timestamp('2020-12-21 15:00:00'), Timestamp('2020-12-27 14:00:00'), Timestamp('2020-12-27 15:00:00'), Timestamp('2020-12-27 16:00:00'), Timestamp('2020-12-27 21:00:00'), Timestamp('2021-01-16 09:00:00'), Timestamp('2021-01-16 10:00:00'), Timestamp('2021-01-16 11:00:00'), Timestamp('2021-02-01 10:00:00'), Timestamp('2021-02-03 09:00:00'), Timestamp('2021-02-03 10:00:00'), Timestamp('2021-02-06 11:00:00'), Timestamp('2021-02-06 17:00:00'), Timestamp('2021-02-08 11:00:00'), Timestamp('2021-02-11 14:00:00'), Timestamp('2021-02-25 22:00:00'), Timestamp('2021-03-12 08:00:00'), Timestamp('2021-03-19 15:00:00'), Timestamp('2021-03-19 20:00:00'), Timestamp('2021-03-29 13:00:00'), Timestamp('2021-04-06 07:00:00'), Timestamp('2021-04-12 15:00:00'), Timestamp('2021-04-13 16:00:00'), Timestamp('2021-11-04 14:00:00'), Timestamp('2021-11-04 15:00:00'), Timestamp('2021-11-04 23:00:00'), Timestamp('2021-11-05 00:00:00'), Timestamp('2021-11-05 01:00:00'), Timestamp('2021-11-05 05:00:00'), Timestamp('2021-11-05 06:00:00'), Timestamp('2021-11-05 11:00:00'), Timestamp('2021-11-05 15:00:00'), Timestamp('2021-11-28 15:00:00'), Timestamp('2021-11-29 10:00:00'), Timestamp('2021-12-21 11:00:00')]] Finally, we delete the detected outliers from the original data, and re-plot the chart to compare it with the original one. We can clearly find some outliers (for example, there is an abnormal peak in 2022-07) have been removed.\noutliers_removed = outlierDetection.remover(interpolate=False) outliers_removed outliers_removed.plot(cols=['y_0']) Change Point Detection) A change point is a point in time at which the data suddenly changes significantly, representing the occurrence of an event, a change in the state of the data, or a change in the distribution of the data. Therefore, change point detection is often regarded as an important preprocessing step for data analysis and data prediction.\nIn the following example, we use daily averages of air quality data for change point detection. We use the TimeSeriesData data format of the kats package to store the data and use the CUSUMDetector detector provided by kats for detection. We use red dots to represent detected change points in the plot. Unfortunately, in this example, no obvious point of change was observed. Readers are advised to refer to this example and bring in other data for more exercise and testing.\nair_ts = TimeSeriesData(air_day.reset_index(), time_col_name='timestamp') detector = CUSUMDetector(air_ts) change_points = detector.detector(change_directions=[\"increase\", \"decrease\"]) # print(\"The change point is on\", change_points[0][0].start_time) # plot the results plt.xticks(rotation=45) detector.plot(change_points) plt.show() Missing Data Handling Missing data is a common problem when conducting big data analysis. Some of these missing values are already missing at the time of data collection (such as sensor failure, network disconnection, etc.), and some are eliminated during data preprocessing (outliers or abnormality). However, for subsequent data processing and analysis, we often need the data to maintain a fixed sampling rate to facilitate the application of various methods and tools. Therefore, various methods for imputing missing data have been derived. Below we introduce three commonly used methods:\nMark missing data as Nan (Not a number): Nan stands for not a number and is used to represent undefined or unrepresentable values. If it is known that subsequent data analysis will additionally deal with these special cases of Nan, this method can be adopted to maintain the authenticity of the information. Forward filling method: If Nan has difficulty in subsequent data analysis, missing values must be filled with appropriate numerical data. The easiest way to do this is forward fill, which uses the previous value to fill in the current missing value. # forward fill df_ffill = air.ffill(inplace=False) df_ffill.plot() 3. \u0006K-Nearest Neighbor (KNN) method: As the name suggests, the KNN method finds the k values that are closest to the missing value, and then fills the missing value with the average of these k values.\ndef knn_mean(ts, n): out = np.copy(ts) for i, val in enumerate(ts): if np.isnan(val): n_by_2 = np.ceil(n/2) lower = np.max([0, int(i-n_by_2)]) upper = np.min([len(ts)+1, int(i+n_by_2)]) ts_near = np.concatenate([ts[lower:i], ts[i:upper]]) out[i] = np.nanmean(ts_near) return out # KNN df_knn = air.copy() df_knn['PM25'] = knn_mean(air.PM25.to_numpy(), 5000) df_knn.plot() Data Decomposition In the previous example of basic data processing, we have been able to roughly observe the changing trend of data values and discover potential regular changes. In order to further explore the regularity of time series data changes, we introduce the data decomposition method. In this way, the original time series data can be disassembled into trend waves (trend), periodic waves (seasonal) and residual waves (residual).\nWe first replicated the daily average data of air quality data as air_process and processed the missing data using forward filling. Then, we first presented the raw data directly in the form of a line chart.\nair_process = air_day.copy() # new.round(1).head(12) air_process.ffill(inplace=True) air_process.plot() Then we use the seasonal_decompose method to decompose the air_process data, in which we need to set a period parameter, which refers to the period in which the data is decomposed. We first set it to 30 days, and then after execution, it will produce four pictures in sequence: raw data, trend chart, seasonal chart, and residual chart.\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=30) decompose.plot().set_size_inches((15, 15)) plt.show() In the trend graph (trend), we can also find that the graph of the original data has very similar characteristics, with higher values around January and lower values around July; while in the seasonal graph (seasonal), we can find that the data has a fixed periodic change in each cycle (30 days), which means that the air quality data has a one-month change.\nIf we change the periodic variable to 365, i.e. decompose the data on a larger time scale (one year), we can see a trend of higher values around January and lower values around July from the seasonal plot, and this trend change occurs on a regular and regular basis. At the same time, it can be seen from the trend chart that the overall trend is slowing down, indicating that the concentration of PM2.5 is gradually decreasing under the overall trend. The results also confirmed our previous findings that no change points were detected, as the change trend of PM2.5 was a steady decline without abrupt changes.\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=365) decompose.plot().set_size_inches((15, 15)) plt.show() References Civil IoT Taiwan: Historical Data (https://history.colife.org.tw/) Matplotlib - Colormap reference (https://matplotlib.org/stable/gallery/color/colormap_reference.html) Decomposition of time series - Wikipedia (https://en.wikipedia.org/wiki/Decomposition_of_time_series) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057?gi=36d1c3d8372) Decomposition in Time Series Data | by Abhilasha Chourasia | Analytics Vidhya | Medium (https://medium.com/analytics-vidhya/decomposition-in-time-series-data-b20764946d63) ",
    "description": "We use the sensing data of Civil IoT Taiwan Data Service Platform to guide readers to understand the use of moving average, perform periodic analysis of time series data, and then disassemble the time series data into long-term trends, seasonal changes and residual fluctuations. At the same time, we apply the existing Python language suites to perform change point detection and outlier detection to check the existing Civil IoT Taiwan data, and discuss potential implications of such values detected.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "4.1. Time Series Data Processing",
    "uri": "/en/ch4/ch4.1/"
  },
  {
    "content": " Table Of Contents Programming Language - Python Development Platform – Google Colab References Programming Language - Python The subject of this material is the data application of Civil IoT Taiwan. This set of articles will use Python, a programming language commonly used in the data science community, as the main programming language, and adopt an easy-to-understand demonstration method to guide you to read while doing it, step-by-step into the content of each chapter of the topic, and obtain first-hand experience with Civil IoT Taiwan data applications, and the ability to draw inferences from other data science topics in the future.\nOverall, the Python programming language quickly became the most popular programming language in the data science community in a short period of time, mainly due to the following three advantages:\nLower barriers to learning: Compared with other text-based programming languages (such as C and Java), the Python language has fewer special symbols, and literally looks closer to the common English articles in daily life. If you master Python syntax and program execution logic, coupled with the ability of English words, it is easy to understand the semantics of Python code, so the threshold for learning Python is very low! A wide variety of libraries: After more than 30 years of development, the Python language has grown into an ecosystem. In addition to the basic syntax, various packages can be added, which can be advanced and transformed into various tools, suitable for solving various problems. By customizing a Python toolkit (pyCIOT) in this project, the ceiling of learning can be raised faster, so as to understand how to more conveniently apply Civil IoT Taiwan data. More suitable for self-study: With Kit, code reading comprehension is more important than code writing ability! If you want to control an existing kit, you have to understand the manuals, then line them up and piece together the code that solves the problem. Therefore, the problem-solving mode will be “stacking programs on the basis of existing program masters, rather than writing Python code from scratch”; especially the development of code reading and comprehension skills, it is more suitable to use the self-learning mode. After you are familiar with the logic of Python, learning to read code will have the opportunity to be as fun as reading a storybook. Due to the above advantages of the Python language, the Python language has become the most commonly used programming language in data science and the first programming language used by many programming learners. Therefore, in addition to various Python language teaching books on the market, many practical learning resources can also be found online. It is worth further exploration and learning for readers interested in the Python language.\nFree Courses Python for Everybody Specialization, Coursera (https://www.coursera.org/specializations/python) Python for Data Science, AI \u0026 Development, Coursera (https://www.coursera.org/learn/python-for-applied-data-science-ai) Introduction to Python Programming, Udemy (https://www.udemy.com/course/pythonforbeginnersintro/) Learn Python for Total Beginners, Udemy (https://www.udemy.com/course/python-3-for-total-beginners/) Google’s Python Class (https://developers.google.com/edu/python) Introduction to Python, Microsoft (https://docs.microsoft.com/en-us/learn/modules/intro-to-python/) Learn Python 3 from Scratch, Educative (https://www.educative.io/courses/learn-python-3-from-scratch) Free e-Book Non-Programmer’s Tutorial for Python 3, Josh Cogliati (https://en.wikibooks.org/wiki/Non-Programmer’s_Tutorial_for_Python_3) Python 101, Michael Driscoll (https://python101.pythonlibrary.org/) The Python Coding Book, Stephen Gruppetta (https://thepythoncodingbook.com/) Python Data Science Handbook, Jake VanderPlas (https://jakevdp.github.io/PythonDataScienceHandbook/) Intro to Machine Learning with Python, Bernd Klein (https://python-course.eu/machine-learning/) Applied Data Science, Ian Langmore and Daniel Krasner (https://columbia-applied-data-science.github.io/) Development Platform – Google Colab Unlike C language, which can be compiled into executable files in advance, Python itself is an interpreted language, that is, it is translated into machine language for computer execution before execution. In other words, it is a literal translation when executed, in situations similar to everyday life. In this way, it is as if translators are helping us to translate and communicate in a language acceptable to foreigners. When we speak a sentence, the translator directly helps us translate the sentence. On the contrary, when the foreigner speaks, the staff will help us translate the foreigner’s words so that we can understand.\nDue to this feature of the interpreted language, in addition to the program editor officially provided by Python, there are many other editors with different functions. Based on the similarity between Python code and natural language, some people have proposed to make the Python editor into a notebook-like mode, an editor that can mix natural language articles and Python code on one page, the most famous of which is Jupyter.\nOn the basis of Jupyter, Google moved the Python interpretation language to the Internet cloud. As long as you apply for a Google account, you can install the free Colab application on Google Drive, and use the browser directly to enjoy the Python program editing function. Due to its simplicity and rich functionality, it has become the most used development platform for Python users.\nOverall, Google Colab has four major advantages:\nPre-installed packages: The Google Colab development platform has pre-installed most of the Python packages, and users can use them directly, avoiding their own installation, and even eliminating the problem of version conflicts caused by different packages during the installation process, greatly reducing the entry threshold for using Python to develop programs. Cloud storage: With Google’s own cloud storage space, Google Colab can store program notes during development in the cloud space. As long as it is accessed through the network, it can be seamlessly connected even on different computers, solving the problems of data storage, backup and portability. Collaboration：Google Colab provides network sharing and online collaboration features, allowing users to share program notes with other users through cloud storage, and allowing users to invite other users to browse, annotate, and even edit their own program notes, speeding up team collaboration . Free GPU and TPU resources: With Google’s own rich cloud computing resources, Google Colab provides GPU and TPU processors, allowing users to use high-end processors to execute their own personal program notes, which is conducive to large-capacity program development needs. For related learning resources of Google Colab, please refer to the following links:\nGetting Started With Google Colab, Anne Bonner (https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c) Use Google Colab Like A Pro, Wing Poon (https://pub.towardsai.net/use-google-colab-like-a-pro-39a97184358d) References Python (https://www.python.org/) Google Colaboratory (https://colab.research.google.com/) Jupyter: Free software, open standards, and web services for interactive computing across all programming languages (https://jupyter.org/) 4 Reasons Why You Should Use Google Colab for Your Next Project, Orhan G. Yalçın (https://towardsdatascience.com/4-reasons-why-you-should-use-google-colab-for-your-next-project-b0c4aaad39ed) ",
    "description": "A brief introduction of the programming language Python and the development platform Google Colab used in the materials",
    "tags": [
      "Introduction"
    ],
    "title": "2.2. Material Tools",
    "uri": "/en/ch2/ch2.2/"
  },
  {
    "content": "\nTable Of Contents Package Installation and Importing Data Access Leafmap Basics Basic Data Presentation Cluster Data Presentation Change Leafmap Basemap Integrate OSM Resources Heatmap Presentation Split Window Presentation Leafmap for Web Applications Conclusion References In the previous chapters, we have demonstrated how to use programming languages to analyze data on geographic attributes, and we have also demonstrated how to use GIS software for simple geographic data analysis and presentation. Next, we will introduce how to use the Leafmap suite in Python language for GIS applications, and the Streamlit suite for website development. Finally, we will combine Leafmap and Streamlit to make a simple web GIS system by ourselves, and present the results of data processing and analysis through web pages.\nPackage Installation and Importing In this chapter, we will use packages such as pandas, geopandas, leafmap, ipyleaflet, osmnx, streamlit, geocoder, and pyCIOT. Apart from pandas, our development platform Google Colab does not provide these packages, so we need to install them ourselves first. Since there are many packages installed this time, in order to avoid a large amount of information output after the command is executed, we have added the ‘-q’ parameter to each installation command, which can make the output of the screen more concise.\n!pip install -q geopandas !pip install -q leafmap !pip install -q ipyleaflet !pip install -q osmnx !pip install -q streamlit !pip install -q geocoder !pip install -q pyCIOT After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport pandas as pd import geopandas as gpd import leafmap import ipyleaflet import osmnx import geocoder import streamlit from pyCIOT.data import * Data Access In the examples in this article, we use several datasets on the Civil IoT Taiwan Data Service Platform, including air quality data from the EPA, and seismic monitoring station measurements from the National Earthquake Engineering Research Center and the Central Weather Bureau.\nFor the EPA air quality data, we use the pyCIOT package to obtain the latest measurement results of all EPA air quality stations, and convert the resulting JSON format data into a DataFrame through the json_normalize() method in the pandas package format. We only keep the station name, latitude, longitude and ozone (O3) concentration information for future use. The code for this part of data collection and processing is as follows:\nepa_station = Air().get_data(src=\"OBS:EPA\") df_air = pd.json_normalize(epa_station) df_air['O3'] = 0 for index, row in df_air.iterrows(): sensors = row['data'] for sensor in sensors: if sensor['name'] == 'O3': df_air.at[index, 'O3'] = sensor['values'][0]['value'] df_air = df_air[['name','location.latitude','location.longitude','O3']] df_air Then we extract the seismic monitoring station data from the National Earthquake Engineering Research Center and the Central Weather Bureau in a similar way, leaving only the station name, longitude and latitude information for subsequent operations. The code for this part of data collection and processing is as follows:\nquake_station = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") df_quake = pd.json_normalize(quake_station) df_quake = df_quake[['name','location.latitude','location.longitude']] df_quake We have successfully demonstrated the reading examples of air quality data (air) and seismic data (quake). In the following discussion, we will use these data for the operation and application using the leafmap suite. The same methods can also be easily applied to other datasets on the Civil IoT Taiwan Data Service Platform. You are encouraged to try it yourself.\nLeafmap Basics Basic Data Presentation Using the data df_air and seismic data df_quake that we are currently processing, we first convert the format of these two data from the DataFrame format provided by the pandas package to the GeoDataFrame format provided by the geopandas package which supports geographic information attributes. We then use Leafmap’sadd_gdf() method to create a presentation layer for each dataset and add them to the map in one go.\ngdf_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') gdf_quake = gpd.GeoDataFrame(df_quake, geometry=gpd.points_from_xy(df_quake['location.longitude'], df_quake['location.latitude']), crs='epsg:4326') m1 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m1.add_gdf(gdf_air, layer_name=\"EPA Station\") m1.add_gdf(gdf_quake, layer_name=\"Quake Station\") m1 From the map output by the program, we can see the names of the two data in the upper right corner of the map, which have been added to the map in the form of two layers. Users can click the layer to be queried to browse according to their own needs. However, when we want to browse the data of two layers at the same time, we will find that both layers are rendering the same icon, so there will be confusion on the map.\nTo solve this problem, we introduce another way of data presentation. We use the GeoData layer data format provided by the ipyleaflet suite and add the GeoData layer to the map using leafmap’s add_layer() method. For easy identification, we use the small blue circle icon to represent the data of the empty station, and the small red circle icon to represent the data of the seismic station.\ngeo_data_air = ipyleaflet.GeoData( geo_dataframe=gdf_air, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'blue', 'weight': 3}, name=\"EPA stations\", ) geo_data_quake = ipyleaflet.GeoData( geo_dataframe=gdf_quake, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'red', 'weight': 3}, name=\"Quake stations\", ) m2 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m2.add_layer(geo_data_air) m2.add_layer(geo_data_quake) m2 Cluster Data Presentation In some data applications, when there are too many data points on the map, it is not easy to observe. If this is the case, we can use clustering to present the data. i.e. when there are too many data points for a small area, we cluster the points together to show the number of points. When the user zooms in on the map, these originally clustered points are slowly pulled apart. When there is only one point left in a small area, the information of that point can be directly seen.\nLet’s take data from seismic stations as an example. Using leafmap’s add_points_from_xy() method, the data of df2 can be placed on the map in a clustered manner.\nm3 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m3.add_points_from_xy(data=df_quake, x = 'location.longitude', y = 'location.latitude', layer_name=\"Quake Station\") m3 Change Leafmap Basemap Leafmap uses OpenStreetMap as the default basemap and provides over 100 other basemap options. Users can change the basemap according to their own preferences and needs. You can use the following syntax to learn which basemaps currently supported by leafmap:\nlayers = list(leafmap.basemaps.keys()) layers We select SATELLITE and Stamen.Terrain from these basemaps as demonstrations, and use the add_basemap() method of the leafmap package to add the basemap as a new layer. After adding, the leafmap preset will open all layers and stack them in the order of addition. You can select the layer you want to use through the layer menu in the upper right corner.\nm4 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m4.add_gdf(gdf_air, layer_name=\"EPA Station\") m4.add_basemap(\"SATELLITE\") m4.add_basemap(\"Stamen.Terrain\") m4 In addition to using the basemap provided by leafmap, you can also use Google Map’s XYZ Tiles service to add layers of Google satellite imagery. The methods are as follows:\nm4.add_tile_layer( url=\"https://mt1.google.com/vt/lyrs=y\u0026x={x}\u0026y={y}\u0026z={z}\", name=\"Google Satellite\", attribution=\"Google\", ) m4 Integrate OSM Resources In addition to some built-in resources, Leafmap also integrates many external geographic information resources. Among them, OSM (OpenStreetMap) is a well-known and rich open source geographic information resource. Various resources provided by OSM can be found on the OSM website, with a complete list of properties.\nIn the following example, we use the add_osm_from_geocode() method provided by the leafmap package to demonstrate how to get the outline of a city and render it on the map. Taking Taichung City as an example, combined with the location information in the EPA air quality monitoring station data, we can clearly see which stations are in Taichung City.\ncity_name = \"Taichung, Taiwan\" m5 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m5.add_layer(geo_data_air) m5.add_osm_from_geocode(city_name, layer_name=city_name) m5 Then we continue to use the add_osm_from_place() method provided by the leafmap package to further search for specific facilities in Taichung City and add them to the map layer. The following procedure takes factory facilities as an example and uses the land use data of OSM to find out the relevant factory locations and areas in Taichung City, which can be analyzed and explained in combination with the locations of EPA air quality monitoring stations. For more types of OSM facilities, you can refer to the complete properties list.\nm5.add_osm_from_place(city_name, tags={\"landuse\": \"industrial\"}, layer_name=city_name+\": Industrial\") m5 In addition, the leafmap package also provides a method for searching for OSM nearby facilities centered on a specific location, providing a very convenient function for analyzing and interpreting data. For example, in the following example, we use the add_osm_from_address() method to search for related religious facilities (attribute “amenity”: “place_of_worship”) within a 1,000-meter radius of Qingshui Station, Taichung; at the same time, we use the add_osm_from_point() method to search for relevant school facilities (attributes “amenity”: “school”) within 1,000 meters of the GPS coordinates (24.26365, 120.56917) of Taichung Qingshui Station. Finally, we overlay the results of these two queries on the existing map with different layers.\nm5.add_osm_from_address( address=\"Qingshui Station, Taichung\", tags={\"amenity\": \"place_of_worship\"}, dist=1000, layer_name=\"Shalu worship\" ) m5.add_osm_from_point( center_point=(24.26365, 120.56917), tags={\"amenity\": \"school\"}, dist=1000, layer_name=\"Shalu schools\" ) m5 Heatmap Presentation A heatmap is a two-dimensional representation of event intensity through color changes. When matching a heatmap to a map, the state of event intensity can be expressed at different scales depending on the scale of the map used. It is a very common and powerful data representation tool. However, when drawing a heatmap, the user must confirm that the characteristics of the data are suitable for presentation by a heatmap, otherwise it is easy to be confused with the graphical data interpolation representations such as IDW and Kriging that we introduced in Chap 5. For example, we take the O3 concentration data of the EPA air quality data as an example, and draw the corresponding heat map as follows:\nm6 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m6.add_layer(geo_data_air) m6.add_heatmap( df_air, latitude='location.latitude', longitude='location.longitude', value=\"O3\", name=\"O3 Heat map\", radius=100, ) m6 There is nothing obvious about this image at first glance, but if we zoom in on the Taichung city area, we can see that the appearance of the heatmap has changed a lot, showing completely different results at different scales.\nThe above example is actually an example of misuse of the heatmap, because the O3 concentration data reflects the local O3 concentration. Due to the change of the map scale, its values cannot be directly accumulated or distributed to adjacent areas. Therefore, the O3 concentration data used in the example is not suitable for heatmap representation and should be plotted using the geographic interpolation method described in Chapter 5.\nTo show the real effect of the heatmap, we use the location data of the seismic stations instead and add a field num with a default value of 10. Then we use the code below to generate a heatmap of the status of Taiwan Seismic Monitoring Stations.\ndf_quake['num'] = 10 m7 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m7.add_layer(geo_data_quake) m7.add_heatmap( df_quake, latitude='location.latitude', longitude='location.longitude', value=\"num\", name=\"Number of Quake stations\", radius=200, ) m7 Split Window Presentation In the process of data analysis and interpretation, it is often necessary to switch between different basemaps to obtain different geographic information. Therefore, the leafmap package provides the split_map() method, which can split the original map output into two submaps, each applying a different basemap. Its sample code is as follows:\nm8 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m8.add_gdf(gdf_air, layer_name=\"EPA Station\") m8.split_map( left_layer=\"SATELLITE\", right_layer=\"Stamen.Terrain\" ) m8 Leafmap for Web Applications In order to quickly share the processed map information, Leafmap suite also provides an integrated way of Streamlit suite, combining Leafmap’s GIS technical expertise with Streamlit’s web technical expertise to quickly build a Web GIS system. Below we demonstrate how it works through a simple example, you can extend and build your own Web GIS service according to this principle.\nIn the use of the Streamlit package, there are two steps to build a web system:\nPackage the Python program to be executed into a Streamlit object, and write the packaging process into the app.py file; and Execute app.py on the system. Since our operation process all use the Google Colab platform, in this platform we can directly write app.py into the temporary storage area with the special syntax %%writefile, and then Colab directly reads and runs the codes from the temporary storage area. Therefore, for the file writing part of step 1, we can proceed as follows:\n%%writefile app.py import streamlit as st import leafmap.foliumap as leafmap import json import pandas as pd import geopandas as gpd from pyCIOT.data import * contnet = \"\"\" Hello World! \"\"\" st.title('Streamlit Demo') st.write(\"## Leafmap Example\") st.markdown(contnet) epa_station = Air().get_data(src=\"OBS:EPA\") from pandas import json_normalize df_air = json_normalize(epa_station) geodata_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') with st.expander(\"See source code\"): with st.echo(): m = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m.add_gdf(geodata_air, layer_name=\"EPA Station\") m.to_streamlit() For the second part, we use the following instructions:\n!streamlit run app.py \u0026 npx localtunnel --port 8501 After execution, an execution result similar to the following will appear:\nThen you can click on the URL after the string “your url is:”, and something similar to the following will appear in the browser\nFinally, we click “Click to Continue” to execute the Python code packaged in app.py. In this example, we can see the distribution map of EPA’s air quality monitoring stations presented by the leafmap package.\nConclusion In this article, we introduced the Leafmap package to render geographic data and integrate external resources, and demonstrated the combination of the Leagmap and Streamlit packages to build a simple web-based GIS service on the Google Colab platform. It should be noted that Leafmap also has many more advanced functions, which are not introduced in this article. You can refer to the following references for more in-depth and extensive learning.\nReferences Leafmap Tutorial (https://www.youtube.com/watch?v=-UPt7x3Gn60\u0026list=PLAxJ4-o7ZoPeMITwB8eyynOG0-CY3CMdw) leafmap: A Python package for geospatial analysis and interactive mapping in a Jupyter environment (https://leafmap.org/) Streamlit Tutorial (https://www.youtube.com/watch?v=fTzlyayFXBM) Map features - OpenStreetMap Wiki (https://wiki.openstreetmap.org/wiki/Map_features) Heat map - Wikipedia (https://en.wikipedia.org/wiki/Heat_map) ",
    "description": "We introduce the capability of leafmap package to use different types of data for geographic information representation and spatial analysis in Civil IoT Taiwan Data Service Platform, and demonstrate the combination of leafmap and streamlit packages to build Web GIS applications. Through cross-domain and cross-tool resource integration, readers will be able to expand their imagination of the future of data analysis and information services.",
    "tags": [
      "Python",
      "Air",
      "Quake"
    ],
    "title": "7.3. Leafmap Applications",
    "uri": "/en/ch7/ch7.3/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "advanced",
    "uri": "/en/levels/advanced/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Air",
    "uri": "/en/tags/air/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Author",
    "uri": "/en/author/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "beginner",
    "uri": "/en/levels/beginner/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/en/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Huei-Jun Kao",
    "uri": "/en/author/huei-jun-kao/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/en/tags/introduction/"
  },
  {
    "content": "Learning Civil IoT Taiwan Open Data and Its Applications The “Civil IoT Taiwan” project is developed to address the four urgent needs of the public in order to integrate and close to the public services related to people’s livelihood, including air quality, earthquake, water resources, disaster prevention and other issues. In the “Digital Technology” section of the “Forward-looking Infrastructure Development Program” project, the Ministry of Science and Technology, the Ministry of Communications, the Ministry of Economic Affairs, the Ministry of the Interior, the Environmental Protection Agency, the Academia Sinica, and the Council of Agriculture have jointly constructed a large-scale inter-ministerial government project, which applies big data, artificial intelligence, and Internet of things technologies to build various smart life service systems to help the government and the public face the challenges brought about by environmental changes; at the same time, this project also considers the experience of different end-users, including government decision-making units, academia, industry, and the general public. The goal is to enhance intelligent governance, assist the development of industry/academia, and improve the happiness of the public.\nThe Civil IoT Taiwan project has also developed the “Civil IoT Taiwan Data Service Platform”, which provides real-time data access and historical data query services in a unified data format, improves user browsing and search speed, and promotes applications of model-based scientific computing and artificial intelligence. The platform not only collects various types of data generated by the Civil IoT Taiwan project, but also provides reliable and high-quality sensing data for various environmental management purposes. In addition, it narrows the gap of environmental information and provides more real-time and comprehensive environmental data, so that the public can view real-time sensor information at any time and observe the temporal and spatial changes of the surrounding environment. The data service platform also serves as the basis for the development of value-added applications, giving full play to the creativity of the people and producing high-quality services that solve the problems of the masses.\nIn order to maintain the good basis for the development of the Civil IoT Taiwan project and its data platform, and to continue to take root and cultivate more diverse user groups to step into the various application areas of the Civil IoT Taiwan project, there are three goals of this project: 1) Rooted downward, providing step-by-step self-study learning materials for college and high school students, and conducting cross-domain learning across the fields of information, geography, earth science, and humanities and society; 2) Demonstration application, aiming at the existing applications of Civil IoT Taiwan, in the way of stripping the cocoon, lowering the entry threshold and guiding deeper innovation of production system; 3) Horizontal expansion, providing additional data access methods, using the popular Python language, redeveloping the data access suite, drawing on more diverse technical energy, and enriching the user group of the data platform.\n",
    "description": "",
    "tags": null,
    "title": "Learning Civil IoT Taiwan Open Data and Its Applications",
    "uri": "/en/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Levels",
    "uri": "/en/levels/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Ling-Jyh Chen",
    "uri": "/en/author/ling-jyh-chen/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Ming-Kuang Chung",
    "uri": "/en/author/ming-kuang-chung/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Python",
    "uri": "/en/tags/python/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Quake",
    "uri": "/en/tags/quake/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/en/tags/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Water",
    "uri": "/en/tags/water/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Yu-Chi Peng",
    "uri": "/en/author/yu-chi-peng/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Yu-Shen Cheng",
    "uri": "/en/author/yu-shen-cheng/"
  }
]
