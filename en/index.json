[
  {
    "content": "1. Introduction Table Of Contents Environment Perception Open Data Civil IoT Taiwan Civil IoT Taiwan Data Service Platform References Environment Perception Human beings are full of curiosity about the changes in the natural world, which can probably trace back to the activities of ancient agricultural civilizations, looking up at the stars and stargazing at night. By the Renaissance in the 16th century AD, Copernicus deduced the “heliocentric theory” based on the existing astronomical data. After Galileo and Newton, it became the origin of modern science. Since modern times, due to the rapid development of semiconductor manufacturing technology, the tools that people use to perceive changes in the living environment have become more and more diverse, sophisticated and miniaturized. In order to catch up with the ever-changing trends of information technology, combined with the real-time perception of sensors and the connection of the Internet, data transmission is getting faster and more popular, and a large amount of observation data is generated, transmitted and archived. In response to massive data, scientists have been working hard to find the laws of environmental changes, explore the relationship between these laws and disasters, and obtain prediction results to improve people’s quality of life, thereby improving the efficiency of disaster prevention and relief, and promoting a more friendly relationship between people and the environment. The goal of creating a beautiful and sustainable living environment has become a significant issue of common concern to individuals, groups, societies and countries in the world today.\nOpen Data “Open data” refers to a type of electronic data (including but not limited to text, data, pictures, videos, sounds, etc.) that is publicly permitted and encouraged to be used by all sectors of society through certain procedures and formats. According to the definition of the Open Knowledge Foundation (https://opendefinition.org/od/2.1/en/), open data must meet the following conditions:\nOpen license or status: The content of the open data must be in the public domain and released with an open license without any additional restrictions. Access: The content of open data must be freely accessible from the Internet, but under reasonable conditions, access to open data may also allow a one-time fee and allow for conditional incidental terms. Machine readability: The content of open data must be easily accessible, processed and modified by computers. Open format: The content of open data must be in an available form, which can be accessed using free or open software. With the prevalence of open trends such as open source code, open government, and open science, open data has gradually become a standard that governments and scientific communities follow when engaging in various policy advocacy and academic research. The recently booming environment-aware IoT has become one of the most anticipated open data projects due to its widespread distribution in the public domain and its observed environmental information being closely related to the public.\nAt present, most of the common open data on the Internet use data formats such as JSON, CSV or XML. The full name of JSON format is Javascript Object Notation, which is a lightweight structured data exchange standard. Its content consists of properties and values, which is easy for computers or users to read and use, and is often used for data presentation, transmission and storage on the website.\nThe full name of the CSV format is Comma-Separated Values. As the name suggests, it is a form of storing table data in plain text, where each data is a column and each attribute in the data is a column. All properties must be stored as plain text in a specific order, with a specific symbol to separate different properties. Commonly used for file import and export of applications, network data transmission, historical data storage, etc.\nThe full name of the XML format is Extensible Markup Language, which is an extensible markup language simplified from the Standard Generalized Markup Language (SGML), allowing users to define the required tags. It is often used for the presentation of document file data and the exchange of network data in order to create documents containing structured information.\nCurrently, open data commonly used in Taiwan are aggregated on the following platforms:\nTaiwan government open data platform (https://data.gov.tw/en/) Taiwan open weather data (https://opendata.cwb.gov.tw/devManual/insrtuction) Taiwan EPA open data platform (https://data.epa.gov.tw/en) Civil IoT Taiwan The “Civil IoT Taiwan” project is developed to address the four urgent needs of the public in order to integrate and close to the public services related to people’s livelihood, including air quality, earthquake, water resources, disaster prevention and other issues. In the “Digital Technology” section of the “Forward-looking Infrastructure Development Program” project, the Ministry of Science and Technology, the Ministry of Communications, the Ministry of Economic Affairs, the Ministry of the Interior, the Environmental Protection Agency, the Academia Sinica, and the Council of Agriculture have jointly constructed a large-scale inter-ministerial government project, which applies big data, artificial intelligence, and Internet of things technologies to build various smart life service systems to help the government and the public face the challenges brought about by environmental changes. At the same time, this project also considers the experience of different end-users, including government decision-making units, academia, industry, and the general public. The goal is to enhance intelligent governance, assist the development of industry/academia, and improve the happiness of the public.\nAt present, the four major areas covered by the Civil IoT Taiwan project are:\nWater resources: The Civil IoT Taiwan project cooperates with the water conservancy units of the central and local governments to develop and deploy a variety of water resources sensors, and build various hydrological observation facilities and farmland irrigation water sensors. The goal of this project is to effectively integrate surface, groundwater and emerging water source information and related monitoring information, and to establish a water resource IoT portal and a dynamic analysis and management platform for irrigation water distribution, through big data analysis, so as to strengthen the flood control operation system of each river bureau and establish he sewer cloud management cloud that can perform remote, automated and intelligent management for relevant units at all levels, achieve the goal of combined use, and promote various applications such as intelligent control and management of water sources, dynamic analysis and management of irrigation water distribution, value-added applications of sewage and sewer, and road flood warning. With the opening of data, it will promote the development of water resources data applications and public-private partnership decision-making. Air quality: The Civil IoT Taiwan project, in conjunction with the Environmental Protection Agency, the Ministry of Economic Affairs, the Ministry of Science and Technology, and Academia Sinica, started with the deployment of air quality sensing infrastructure, developed air quality sensors, and established a sensor performance testing and verification center. And a large number of air quality sensors for different purposes are widely distributed throughout Taiwan. Through the collection of a large amount of sensing data at smaller time and space scales, this project establishes a computing operation platform for the Internet of Things and a smart environment sensing data center, and builds a visualization platform for air quality sensing data. On the one hand, it promotes the development of the Internet of Things for air quality sensing, and on the other hand, it provides high-resolution evidence data for intelligent environmental protection inspections to identify polluted hot spots and effectively conduct environmental governance. At the same time, the project also strengthens the research and development capacity of domestic own technology during the plan period, and establishes the development of domestic independent airborne product sensing technology. Earthquakes: The Civil IoT Taiwan project also significantly increases the number of on-site earthquake quick-reporting stations to provide high-density, high-quality earthquake quick-reporting information for the common seismic activity in Taiwan. At the same time, seismic and geophysical observatories were added and upgraded to improve the quality and resolution of observational data, including Global Navigation Satellite System (GNSS) stations, underground seismic stations, strong earthquake stations, geomagnetic stations, and groundwater stations. In addition, the project is enhancing its GNSS stations, underground seismic stations, and drone observations for regional monitoring of the Datun Volcano. In view of Taiwan’s unique island characteristics, it has also strengthened its forecasting capabilities for strong earthquakes and tsunamis, and expanded submarine optical cables and related submarine and land stations. In addition, through the integration and application of big data, the project has constructed an integrated data management system for Taiwan’s seismic and geophysical data, which can facilitate the integration of local and regional earthquake quick report information, strengthen the monitoring of Taiwan’s fault areas and the Datun volcanic area, and provide fast and complete earthquake quick report information transmission, thereby providing the public with real-time early warning of strong earthquakes and promoting the development of the earthquake disaster prevention industry. Disaster prevention and relief: The Civil IoT Taiwan project gathers a total of 58 types of warning data covering air, water, land, disaster and people’s livelihood into a single “Civil Alert Information Platform” to provide the public with real-time disaster prevention and relief information. Moreover, incorporating the emergency management information cloud system (EMIC2.0) and other decision-making assisting systems, the platform provides disaster prevention personnel with various disaster situations, notifications and disaster relief resources to assist in various decision-making work, and at the same time, the relevant historical data is collected and released in a unified data format, which is made available for disaster prevention industry to analyze and use, and promote the development of disaster information industry chain. Civil IoT Taiwan Data Service Platform In addition, the Civil IoT Taiwan project also plans the Civil IoT Taiwan data service platform to accommodate various data generated by the Civil IoT Taiwan project and provide stable and high-quality sensing data for various environmental management purposes. In the spirit of open data, the platform adopts a unified data format, provides real-time data interfaces and historical data query services, improves user browsing and search speed, and sets up sensor data storage mechanisms to help scientific computing and artificial intelligence applications. The establishment of the Civil IoT Taiwan data service platform can not only narrow the gap in environmental information, but also provide more real-time and comprehensive environmental data, allowing the public to view perception information and temporal and spatial changes in real time, and to understand the living environment at any time. The data provided by the platform can also be used as a basis for industrial value-added, thereby stimulating the creativity of the people and providing high-quality services for solving the problems of the people.\nCurrently, the Civil IoT Taiwan Data Service Platform uses the open data format of the OGC SensorThings API. Please refer to the following slides and pictures for relevant data format descriptions and data content in various fields:\nIntroduction to Civil IoT Taiwan Data Service Platform [PPT] Introduction to Civil IoT Taiwan Data Service Platform and OGC SensorThings API [Video] (in Chinese) Water Resources [PPT] Water Resources IoT [Video] (in Chinese) Air Quality [PPT] Environment Quality Sensing IoT [Video] (in Chinese) [PPT] Deployment of Micro PM2.5 Sensors [Video] (in Chinese) Earthquake [PPT] Joint Sea-Land Earthquake Observation [Video] (in Chinese) [PPT] Composite Earthquake Quick Report [Video] (in Chinese) Disaster prevention and relief [PPT] Civil Alert Open Data [Video] (in Chinese) [PPT] Integration of Disaster Prevention and Relief Information Systems [Video] (in Chinese) In order to allow more people to participate in the Civil IoT Taiwan project and its data platform, since 2018, the Civil IoT Taiwan project has also held a series of data application competitions, data innovation hackathons, and physical and virtual exhibitions. The project also designed a series of training materials and business coaching, from the establishment and development of the team, from the inception to the formation of the idea, and from the conception to the implementation of the application service. After several years of accumulation, the project has successfully developed a successful case, which shows that the Civil IoT Taiwan project is not only a hardware construction for government units, but also has successfully transformed into a basic information construction for people’s livelihood. The project is providing a steady stream of high-quality sensor data for improving people’s lives and promoting more innovative, convenient, and caring information services.\nFor examples of applications and solutions in various fields of Civil IoT Taiwan data, please refer to the following website resources:\nCivil IoT Taiwan Service and Solution Guide: Water resources Civil IoT Taiwan Service and Solution Guide: Air qualuty Civil IoT Taiwan Service and Solution Guide: Earthquake Civil IoT Taiwan Service and Solution Guide: Disaster prevention and relief References Open Definition: defining open in open data, open content and open knowledge. Open Knowledge Foundation (https://opendefinition.org/od/2.1/en/) Civil IoT Taiwan (https://ci.taiwan.gov.tw) Civil IoT Taiwan Virtual Expo: Dialogue in Civil IoT (https://ci.taiwan.gov.tw/dialogue-in-civil-iot) Civil IoT Taiwan Service and Solution Guide (https://www.civiliottw.tadpi.org.tw) Civil IoT Taiwan Data Service Platform (https://ci.taiwan.gov.tw/dsp/) XML - Wikipedia (https://en.wikipedia.org/wiki/XML) JSON - Wikipedia (https://en.wikipedia.org/wiki/JSON) Comma-separated values - Wikipedia (https://en.wikipedia.org/wiki/Comma-separated_values) Standard Generalized Markup Language - Wikipedia (https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language) OGC SensorThings API Documentation (https://developers.sensorup.com/docs/) ",
    "description": "Introduction",
    "tags": [
      "Introduction"
    ],
    "title": "1. Introduction",
    "uri": "/en/ch1/"
  },
  {
    "content": "2. Overview of the Materials In this topic, we will introduce the overall structure of the developed materials of Civil IoT Taiwan Open Data, as well as the programming language Python and the development platform Google Colab used in the materials. In addition to conceptual descriptions, we also provide a large number of extended learning resources from shallow to deep, so that interested readers can further explore and learn according to their own needs.\n2.1. Material ArchitectureIntroduction of the material architecture\n2.2. Material ToolsA brief introduction of the programming language Python and the development platform Google Colab used in the materials\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "2. Overview of the Materials",
    "uri": "/en/ch2/"
  },
  {
    "content": "3. Data Access In this topic, we will describe how to directly access the data of the Civil IoT Taiwan Data Service Platform using simple Python syntax through the pyCIOT package. To gain a deeper understanding of the different ways to access data with examples, we divide this topic into two units for a more in-depth introduction:\n3.1. Basic Data Access MethodsWe introduce how to obtain water, air, earthquake, and disaster data in the Civil IoT Taiwan Data Service Platform, including the latest sensing data for a single site, a list of all sites, and the latest current sensing data for all sites.\n3.2. Data Access under Spatial or Temporal ConditionsWe introduce how to obtain the data of a project in a specific time or time period, and the data of a project in a specific geographical area in the Civil IoT Taiwan Data Service Platform. We also demonstrate the application through a simple example.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "3. Data Access",
    "uri": "/en/ch3/"
  },
  {
    "content": "4. Time Series Data Analysis In this topic, we will introduce time series data analysis methods for IoT data. We will develop the following three units for a more in-depth exploration by using the Civil IoT Taiwan Data Service Platform.\n4.1. Time Series Data ProcessingWe use the sensing data of Civil IoT Taiwan Data Service Platform to guide readers to understand the use of moving average, perform periodic analysis of time series data, and then disassemble the time series data into long-term trends, seasonal changes and residual fluctuations. At the same time, we apply the existing Python language suites to perform change point detection and outlier detection to check the existing Civil IoT Taiwan data, and discuss potential implications of such values detected.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "4. Time Series Data Analysis",
    "uri": "/en/ch4/"
  },
  {
    "content": "5. Spatial Data Analysis In this topic, we introduce a series of spatial data processing and analysis methods according to the geospatial characteristics of IoT data. Through the use of the Civil IoT Taiwan Data Service Platform, we will develop the following units for a more in-depth introduction.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "5. Spatial Data Analysis",
    "uri": "/en/ch5/"
  },
  {
    "content": "6. Data Applications In this topic, we will focus on derivative applications using Civil IoT Taiwan open data. We strengthen the value and application services of Civil IoT Taiwan’s open data by importing other library packages and analysis algorithms. The units we expect to develop include:\n6.3. Joint Data CalibrationWe use air quality category data of the Civil IoT Taiwan project to demonstrate the dynamic calibration algorithm for Taiwanese micro air quality sensors and official monitoring stations. In a learning-by-doing way, from data preparation, feature extraction, to machine learning, data analysis, statistics and induction, the principle and implementation process of the multi-source sensor dynamic calibration model algorithm are reproduced step by step, allowing readers to experience how to gradually realize by superimposing basic data analysis and machine learning steps to achieve advanced and practical data application services.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "6. Data Applications",
    "uri": "/en/ch6/"
  },
  {
    "content": "7. System Integration and Applications In this theme, we will focus on the integration and application of the Civil IoTTaiwan Open Data and other application software, and through the professional functions of other application software, to further deepen and develop the value of the Civil IoT Taiwan Open Data.\n7.1. QGIS ApplicationWe introduce the presentation of geographic data using the QGIS system, and use the data from Civil IoT Taiwan as an example to perform geospatial analysis by clicking and dragging. We also discuss the advantages and disadvantages of QGIS software and when to use it.\n7.2. Tableau ApplicationWe introduce the use of Tableau tools to render Civil IoT Taiwan data and conduct two example cases using air quality data and disaster notification data. We demonstrate how worksheets, dashboards, and stories can be used to create interactive data visualizations for users to explore data. We also provide a wealth of reference materials for people to further study reference.\n7.3. Leafmap ApplicationsWe introduce the capability of leafmap package to use different types of data for geographic information representation and spatial analysis in Civil IoT Taiwan Data Service Platform, and demonstrate the combination of leafmap and streamlit packages to build Web GIS applications. Through cross-domain and cross-tool resource integration, readers will be able to expand their imagination of the future of data analysis and information services.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "7. System Integration and Applications",
    "uri": "/en/ch7/"
  },
  {
    "content": "This set of materials includes seven themes including “Introduction”, “Overview of the Materials”, “Data Access Method”, “Data Analysis in Time Dimension”, “Data Analysis in Space Dimension”, “Data Applications” and “System Applications”. The major themes are described as follows:\nIntroduction: We introduce the core of the entire project - “Civil IoT Taiwan Project”, including various achievements of the project in the past few years, as well as its open data, covering water resources, air quality, earthquakes, and disaster prevention and relief fields, which can bring daily life various applications and services in. We also present various existing materials, videos and successful cases, so that everyone can deeply understand the importance of “Civil IoT Taiwan project” to people’s livelihood.\nOverview of the Materials: We present the structure of the entire material, as well as the Python programming language and Google Colab platform used in the material. Through simple text descriptions, we guide readers to quickly understand the structure of the entire material, and provides a rich list of external resources for those who are interested and in need to further explore related technologies.\nData Access Method: We introduce how to use simple Python syntax to directly access the data of the Civil IoT Taiwan Data Service Platform through the pyCIOT tool we developed, and cut out two units for a more in-depth introduction according to different needs:\nBasic Data Access Method: We introduce how to obtain the latest sensing data of a single station for different aspects of water resources, air quality, earthquake, and disaster information in the Civil IoT Taiwan Data Service Platform for people’s livelihood, as well as how to obtain a list of all stations, and the latest current data of all stations. Specific Spatio-temporal Data Access Method: We introduce how to obtain the data of a certain station at a specific time or time range, find the latest current data of the nearest station, and find the current latest data of all stations around the given coordinates from the Civil IoT Taiwan Data Service Platform. In addition to introducing data access methods, we also intersperse with basic exploratory data analysis (EDA) methods, use commonly used statistical methods to describe the data characteristics of different aspects of the data, and draw simple diagrams to let readers learn by doing, and experience what it’s like to analyze data firsthand.\nData Analysis in Time Dimension: In view of the time series characteristics of IoT data, we design three topics to demonstrate time-series data analysis using Civil IoT Taiwan data.\nTime Series Data Processing: We use the sensing data of Civil IoT Taiwan Data Platform to guide readers to understand the use of moving averages, perform periodic analysis on time series data, and then disassemble the time series data into long-term trends, seasonal changes, periodic changes, and random fluctuations. At the same time, change point detection and outlier detection techniques are applied to check the existing Civil IoT Taiwan Data by applying the existing Python packages, and the possible meanings behind the detection results are discussed. Time Series Data Forecast: We use sensory data from the Civil IoT Taiwan Data Platform and apply existing Python data science packages (such as scikit-learn, Kats, etc.) to compare different data prediction methods in a hands-on fashion. Combine the forecast results, we present the data in a graph, and discuss the implications behind the forecast results for different datasets and temporal resolutions. Then, we also discuss possible derivative applications. Time Series Data Clustering: We introduce more advanced data clustering analysis. We first introduce two time series feature extraction methods, namely Fourier transform and wavelet transform, and briefly explain the difference between the two transform methods. We then introduce two distance functions for time series data, Euclidean distance and Dynamic Time Warping (DTW) distance, and apply existing clustering packages using these two functions. Finally, we discuss the implications behind clustering results using different datasets and different temporal resolutions in real-world applications, as well as possible derived applications. Data Analysis in Space Dimension: Given the geospatial characteristics of Civil IoT Taiwan data, we introduce a series of spatial data analysis methods according to different needs and applications.\nGeospatial Filtering: We use the earthquake and disaster prevention and relief data of the Civil IoT Taiwan Data Platform, superimpose the administrative region boundary map data obtained from the government’s open data platform, and generate a picture file of geospatial data distribution after generating the overlay map. Additionally, we demonstrate how to nest specific regions of geometric topology, output the nested results to a file, and perform drawing operations. Geospatial Analysis: We use sensing data from Civil IoT Taiwan Data Platform to introduce more advanced geospatial analysis. Using the GPS location coordinates of the sensors, we first use an existing Python package to find the largest convex polygon (Convex Hull) to frame the geographic area covered by the sensors; then we apply the Voronoi Diagram algorithm to cut the area on the map based on the distribution of the sensors, cutting out the sphere of influence of each sensor. For the area between the sensors, we apply different spatial interpolation algorithms, fill the values on the spatial map according to the sensor values, and produce the corresponding image output. Data Applications: In this topic, we will focus on the derivative applications after accessing the Civil IoT Taiwan data platform, and by importing other library suites and analysis algorithms, we will enhance the value and application services of Civil IoT Taiwan’s open data. We will develop the following three subtopics in turn:\nA First Look at Machine Learning: We use air quality and water level category data, combined with weather observations, to perform predictive analysis of sensory values using machine learning packages. We demonstrate the standard process of machine learning, and describe how to evaluate the effectiveness of predictive analytics, and how to avoid the bias and overfitting problems that machine learning is prone to. Anomaly Detection: We use air quality category data to demonstrate sensor anomaly detection algorithms commonly used in Taiwanese micro air quality sensing data. In a learning-by-doing way, including data preparation, feature extraction, data analysis, statistics, and induction, the principle and implementation process of anomaly detection algorithms are reproduced step by step, allowing readers to experience how to superimpose basic data analysis and machine learning to achieve advanced and practical data application service. Dynamic Calibration Model: We use air quality category data to demonstrate the dynamic calibration algorithm for Taiwanese micro air quality sensors and official stations. In the way of learning by doing, including data preparation, feature extraction, machine learning, data analysis, statistics, etc., it reproduces the principle and implementation process of the sensor dynamic calibration model algorithm, allowing readers to experience how to gradually realize advanced and basic data analysis by overlaying and machine learning steps to provide practical data application services. System Applications: This topic focuses on the integration and application of Civil IoT Taiwan open data and other application software, and further deepens and exerts the value of Civil IoT Taiwan open data through the professional functions of other application software. Units we develop include:\nQGIS Applications: QGIS is a free and open-source GIS software. With this software, users can organize, analyze and draw different geospatial data and thematic maps, presenting the distribution and information types of geographical phenomena. We use QGIS to process Civil IoT Taiwan data, including water, air quality, earthquake, and disaster prevention and relief categories; and overlay different information by clicking and dragging to perform as described in the “Geospatial Filtering” and “Geospatial Analysis” topics. We also discuss the pros and cons of QGIS software and when to use it. Tableau Applications: Tableau is an easy-to-use, powerful data visualization software that can easily connect databases and data files in various formats to create various beautiful statistical charts. We demonstrate how to use drag-and-drop to import data from four aspects of Civil IoT Taiwan, including water resources, air quality, earthquakes, and disaster prevention and relief categories, and make simple charts to compare numerical data. Geospatial data is also overlaid on the map, and time-series data is presented in the form of graphs that show trends in values. In addition, through the interface of external data sources, we can integrate datasets from multiple different sources into a single map to generate beautiful reports in a very short time. Leafmap Applications: Leafmap is a Python development toolkit deeply integrated with Google Earth Engine to directly manipulate and visualize analysis results on platforms such as Google Colab or Jupyter Notebook. We demonstrate the use of leafmap to process Civil IoT Taiwan data, including categories such as water resources, air quality, earthquake, disaster prevention and relief, and build a simple GIS system combined with Google Earth Engine’s rich satellite imagery. At the same time, Streamlit is a tool for Python users to quickly build a web application framework. We demonstrate the integration of leafmap application and Streamlit to form a web version of GIS information service, which expands readers’ imagination space for data analysis and future information services. References Civil IoT Taiwan. https://ci.taiwan.gov.tw Civil IoT Taiwan Data Platform. https://ci.taiwan.gov.tw/dsp/ QGIS - A Free and Open Source Geographic Information System. https://qgis.org/ Tableau - Business Intelligence and Analytics Software. https://www.tableau.com/ Leafmap - A Python package for geospatial analysis and interactive mapping in a Jupyter environment. https://leafmap.org/ Streamlit - The fastest way to build and share data apps. https://streamlit.io/ Google Colaboratory. https://colab.research.google.com/ ",
    "description": "Introduction of the material architecture",
    "tags": [
      "Introduction"
    ],
    "title": "2.1. Material Architecture",
    "uri": "/en/ch2/ch2.1/"
  },
  {
    "content": "\nTable Of Contents What is pyCIOT? pyCIOT Basics Installation Import Package Data Access Air Quality Data Get all project codes: Air().get_source() Get all stations: Air().get_station() Get data of a station: Air().get_data() Water Resource Data Get all project codes: Water().get_source() Get all stations: Water().get_station() Get data of a station: Water().get_data() Earthquake Data Get all project codes: Quake().get_source() Get all stations: Quake().get_station() Get data of a station: Quake().get_data() Get data of an earthquake eventQuake().get_data() Weather Data Get all project codes: Weather().get_source() Get all stations: Weather().get_station() Get data of a station: Weather().get_data() CCTV Data Get all project codes: CCTV().get_source() Get data of a station: CCTV().get_data() Disaster Alert and Notification Data Get disaster alerts: Disaster().get_alert() Get historical data of disaster notifications: Disaster().get_notice() References This article describes how to use the pyCIOT package and basic access to air, water, earthquake, weather, CCTV, and disaster warning data of the Civil IoT Taiwan Data Service Platform. For different types of data, we describe how to get the latest sensing data for a single site, get a list of all sites, and get the latest current sensing data for all sites.\nThis article requires the reader to have basic terminal operation ability and have been exposed to the basic syntax of Python programming.\nWhat is pyCIOT? The Civil IoT Taiwan project provides a wide variety of data, and different data often have different data formats and access methods. Even if the data is under an open license, organizing the data can be cumbersome due to the different ways in which it is downloaded and processed. To solve this problem, we developed the pyCIOT suite to collect all public data on people’s livelihood publicly released by the government, and strive to lower the threshold for obtaining public data and reduce the cost of data processing.\npyCIOT Basics Installation We first download and install the pyCIOT suite using pip. pip is a package management system written in Python for installing and managing Python packages. The pyCIOT suite used this time is managed by the Python package index (pypi). We can use this command in the terminal to download the pyCIOT library locally, or download other required packages together:\n!pip install pyCIOT Import Package To use this package, just enter the import syntax and import pyCIOT.data:\n# Import pyCIOT.data from pyCIOT.data import * Depending on how the package is imported, the method is called differently. If you use the from ... import ... syntax, you don’t need to add the prefix when calling a method; but if you use import ... , you need to add it every time you call a method in its package. If you use import ... as ... , you can call a method based on the custom prefix after as .\nimport pyCIOT.data a = pyCIOT.data.Air().get_source() import pyCIOT.data as CIoT a = CIoT.Air().get_source() from pyCIOT.data import * a = Air().get_source() Data Access The data of the Civil IoT Taiwan project can be obtained through the following methods, including air, water, earthquake, weather, CCTV, etc.:\n.get_source() : Return all project codes in Civil IoT Taiwan Data Service Platform in array format according to the data type. .get_station(src='') : Return basic information of all station data in array format. The src parameter is optional to specify the project code to be queried. .get_data(src='', stationIds=[]) : Return basic information of all stations and their latest measurement result in array format. The src parameter is optional to specify the project code to be queried, and the stationIds parameter is optional to specify the device ID to be queried. The following applies to disaster notification data.\n.get_alert() : Return the alert data, along with the information of the event, in JSON format. .get_notice() : Return the notification data, along with the informaiton of the event, in JSON format. Note that since this package is still under revision, if it is inconsistent with the content of the pyCIOT Package Document, it shall prevail.\nAir Quality Data Get all project codes: Air().get_source() a = Air().get_source() print(a) ['OBS:EPA', 'OBS:EPA_IoT', 'OBS:AS_IoT', 'OBS:MOST_IoT', 'OBS:NCNU_IoT'] The followings are valid project codes for air quality data:\nOBS:EPA: national level monitoring stations by EPA OBS:EPA_IoT: low-cost air quality stations by EPA OBS:AS_IoT: micro air quality stations by Academia Sinica OBS:MOST_IoT: low-cost air quality stations by MOST OBS:NCNU_IoT: low-cost air quality stations by National Chi Nan University Get all stations: Air().get_station() b = Air().get_station(src=\"OBS:EPA_IoT\") b[0:5] [ { 'name': '智慧城鄉空品微型感測器-10287974676', 'description': '智慧城鄉空品微型感測器-10287974676', 'properties': { 'city': '新北市', 'areaType': '社區', 'isMobile': 'false', 'township': '鶯歌區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10287974676', 'locationId': 'TW040203A0507221', 'Description': '廣域SAQ-210', 'areaDescription': '鶯歌區' }, 'location': { 'latitude': 24.9507, 'longitude': 121.3408416, 'address': None } }, ... ] Get data of a station: Air().get_data() f = Air().get_data(src=\"OBS:EPA_IoT\", stationIds=[\"11613429495\"]) f [ {'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 30.6}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-28T06:23:08.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 9.8}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}} ] print(f[0]['description']) for f_data in f[0]['data']: if f_data['description'] == '溫度': print(f_data['description'], ': ', f_data['values'][0]['value'], ' (', f_data['values'][0]['timestamp'], ')', sep='') 智慧城鄉空品微型感測器-11613429495 溫度: 30.6 (2022-08-28T06:22:08.000Z) Water Resource Data Get all project codes: Water().get_source() Return the project names based on the input parameter:\nwater_level_station: Return the names of water level related projects (currently valid codes are WRA, WRA2 and IA) gate: Return the names of water gate related projects (currently valid codes are WRA, WRA2 and IA) pumping_station: Return the names of pumping station related projects (currently valid codes are WRA2 and TPE) sensor: Return the names of water sensor related projects (currently valid codes are WRA, WRA2, IA and CPAMI) `` (none): Return the names of all water related projects wa = Water().get_source() wa ['WATER_LEVEL:WRA_RIVER', 'WATER_LEVEL:WRA_GROUNDWATER', 'WATER_LEVEL:WRA2_DRAINAGE', 'WATER_LEVEL:IA_POND', 'WATER_LEVEL:IA_IRRIGATION', 'GATE:WRA', 'GATE:WRA2', 'GATE:IA', 'PUMPING:WRA2', 'PUMPING:TPE', 'FLOODING:WRA', 'FLOODING:WRA2'] The followings are valid project codes for water resource data:\nWRA: Water Resource Agency WRA2: Water Resource Agency（co-constructed with county and city governments） IA: Irrigation Agency CPAMI: Construction and Planning Agency TPE: Taipei City Get all stations: Water().get_station() wa = Water().get_station(src=\"WATER_LEVEL:WRA_RIVER\") wa[0] { 'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None} } Get data of a station: Water().get_data() wa = Water().get_data(src=\"WATER_LEVEL:WRA_RIVER\", stationID=\"01790145-cd7e-4498-9240-f0fcd9061df2\") wa [{'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'data': [{'name': '水位', 'description': ' Datastream_id=016e5ea0-7c7f-41a2-af41-eabacdbb613f, Datastream_FullName=延平.水位, Datastream_Description=現場觀測, Datastream_Category_type=河川水位站, Datastream_Category=水文', 'values': [{'timestamp': '2022-08-28T06:00:00.000Z', 'value': 157.41}]}], 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None}}] Earthquake Data Get all project codes: Quake().get_source() q = Quake().get_source() q ['EARTHQUAKE:CWB+NCREE'] The followings are valid project codes for earthquake data:\nEARTHQUAKE:CWB+NCREE: seismic monitoring stations by Central Weather Bureau and National Center for Research on Earthquake Engineering Get all stations: Quake().get_station() q = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") q[0:2] [{'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'properties': {'authority': '中央氣象局', 'stationID': 'EGC', 'deviceType': 'FBA', 'stationName': 'Jiqi'}, 'location': {'latitude': 23.708, 'longitude': 121.548, 'address': None}}, {'name': '地震監測站-Xilin-ESL', 'description': '地震監測站-Xilin-ESL', 'properties': {'authority': '中央氣象局', 'stationID': 'ESL', 'deviceType': 'FBA', 'stationName': 'Xilin'}, 'location': {'latitude': 23.812, 'longitude': 121.442, 'address': None}}] Get data of a station: Quake().get_data() q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\") q[-1] {'name': '第2022083號地震', 'description': '第2022083號地震', 'properties': {'depth': 43.6, 'authority': '中央氣象局', 'magnitude': 5.4}, 'data': [ { 'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18815)' },{ 'name': '地震監測站-Taichung City-TCU', 'description': '地震監測站-Taichung City-TCU', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18816)' }, ... ] 'location': {'latitude': 22.98, 'longitude': 121.37, 'address': None}} Get data of an earthquake eventQuake().get_data() q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\", eventID=\"2022083\") q Weather Data Get all project codes: Weather().get_source() w = Weather().get_source() w ['GENERAL:CWB', 'GENERAL:CWB_IoT', 'RAINFALL:CWB', 'RAINFALL:WRA', 'RAINFALL:WRA2', 'RAINFALL:IA', 'IMAGE:CWB'] Return the project names based on the input parameter:\nGENERAL: Return the names of weather station related projects RAINFALL: Return the names of rainfall monitoring station related projects IMAGE: Return the names of radar integrated echo map related projects (none): Return the names of all weather related projects The followings are valid project codes for weather data:\nGENERAL:CWB: standard weather stations by Central Weather Bureau GENERAL:CWB_IoT: automatic weather monitoring stations by Central Weather Bureau RAINFALL:CWB: rainfall monitoring stations by Central Weather Bureau RAINFALL:WRA: rainfall monitoring stations by Water Resource Agency RAINFALL:WRA2: rainfall monitoring stations by Water Resource Agency（co-constructed with county and city governments） RAINFALL:IA: rainfall monitoring stations by Irrigation Agency IMAGE:CWB: radar integrated echo map by Central Weather Bureau Get all stations: Weather().get_station() w = Weather().get_station(src=\"RAINFALL:CWB\") w [{'name': '雨量站-C1R120-上德文', 'description': '雨量站-C1R120-上德文', 'properties': {'city': '屏東縣', 'township': '三地門鄉', 'authority': '中央氣象局', 'stationID': 'C1R120', 'stationName': '上德文', 'stationType': '局屬無人測站'}, 'location': {'latitude': 22.765, 'longitude': 120.6964, 'address': None}}, ... ] Get data of a station: Weather().get_data() w = Weather().get_data(src=\"RAINFALL:CWB\", stationID=\"U2HA40\") w [{'name': '雨量站-U2HA40-臺大內茅埔', 'description': '雨量站-U2HA40-臺大內茅埔', 'properties': {'city': '南投縣', 'township': '信義鄉', 'authority': '中央氣象局', 'stationID': 'U2HA40', 'stationName': '臺大內茅埔', 'stationType': '中央氣象局'}, 'data': [{'name': 'HOUR_12', 'description': '12小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'MIN_10', 'description': '10分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'RAIN', 'description': '60分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_6', 'description': '6小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_3', 'description': '3小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_24', 'description': '24小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'NOW', 'description': '本日累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'ELEV', 'description': '高度', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 507.0}]}], 'location': {'latitude': 23.6915, 'longitude': 120.843, 'address': None}}] CCTV Data Get all project codes: CCTV().get_source() cctv = CCTV().get_source() cctv ['IMAGE:EPA', 'IMAGE:WRA', 'IMAGE:COA'] The followings are valid project codes for CCTV data:\nIMAGE:EPA: realtime images by EPA air quality monitoring stations IMAGE:WRA: images for water conservancy and disaster prevention by Water Resource Agency IMAGE:COA: realtime images of landslide observation stations by Council of Agriculture Get data of a station: CCTV().get_data() cEPA = CCTV().get_data(\"IMAGE:EPA\") cEPA[2] { 'name': '環保署空品監測即時影像器-萬里', 'description': '環保署-萬里-空品監測即時影像器', 'properties': { 'city': '新北市', 'basin': '北部空品區', 'authority': '環保署', 'stationName': '萬里', 'CCDIdentifier': '3'}, 'data': [ { 'name': '即時影像', 'description': '環保署-萬里-空品監測即時影像器', 'values': [ { 'timestamp': '2022-06-13T09:00:00.000Z', 'value': 'https://airtw.epa.gov.tw/AirSitePic/20220613/003-202206131700.jpg' } ] } ], 'location': {'latitude': 25.179667, 'longitude': 121.689881, 'address': None} } cCOA = CCTV().get_data(\"IMAGE:COA\") cCOA[0] {'name': '行政院農委會土石流觀測站影像-大粗坑下游攝影機', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'properties': {'city': '新北市', 'township': '瑞芳鎮', 'StationId': '7', 'authority': '行政院農委會', 'stationName': '大粗坑下游攝影機', 'CCDIdentifier': '2'}, 'data': [{'name': '即時影像', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'values': [{'timestamp': '2099-12-31T00:00:00.000Z', 'value': 'http://dfm.swcb.gov.tw/debrisFinal/ShowCCDImg-LG.asp?StationID=7\u0026CCDId=2'}]}], 'location': {'latitude': 25.090878, 'longitude': 121.837815, 'address': '新北市瑞芳鎮弓橋里大粗坑'}} Disaster Alert and Notification Data The Civil IoT Taiwan Data Service Platform provides more than 58 disaster alerts, with a total of more than 41 disaster notifications. Different units are responsible for different disaster alerts, and the central disaster response center is responsible for disaster notification.\nThe complete list of project codes is available in the pyCIOT Package Document。\nGet disaster alerts: Disaster().get_alert() d = Disaster().get_alert(\"5\") d {'id': 'https://alerts.ncdr.nat.gov.tw/Json.aspx', 'title': 'NCDR_CAP-即時防災資訊(Json)', 'updated': '2021-10-12T08:32:00+08:00', 'author': {'name': 'NCDR'}, 'link': {'@rel': 'self', '@href': 'https://alerts.ncdr.nat.gov.tw/JSONAtomFeed.ashx?AlertType=5'}, 'entry': [ { 'id': 'CWB-Weather_typhoon-warning_202110102030001', 'title': '颱風', 'updated': '2021-10-10T20:30:05+08:00', 'author': {'name': '中央氣象局'}, 'link': {'@rel': 'alternate', '@href': 'https://b-alertsline.cdn.hinet.net/Capstorage/CWB/2021/Typhoon_warnings/fifows_typhoon-warning_202110102030.cap'}, 'summary': { '@type': 'html', '#text': '1SEA18KOMPASU圓規2021-10-10T12:00:00+00:0018.20,126.702330992150輕度颱風TROPICAL STORM2021-10-11T12:00:00+00:0019.20,121.902835980220輕度颱風 圓規（國際命名 KOMPASU）10日20時的中心位置在北緯 18.2 度，東經 126.7 度，即在鵝鑾鼻的東南東方約 730 公里之海面上。中心氣壓 992 百帕，近中心最大風速每秒 23 公尺（約每小時 83 公里），相當於 9 級風，瞬間最大陣風每秒 30 公尺（約每小時 108 公里），相當於 11 級風，七級風暴風半徑 150 公里，十級風暴風半徑 – 公里。以每小時21公里速度，向西進行，預測11日20時的中心位置在北緯 19.2 度，東經 121.9 度，即在鵝鑾鼻的南南東方約 320 公里之海面上。根據最新資料顯示，第18號颱風中心目前在鵝鑾鼻東南東方海面，向西移動，其暴風圈正逐漸向巴士海峽接近，對巴士海峽將構成威脅。巴士海峽航行及作業船隻應嚴加戒備。第18號颱風外圍環流影響，易有短延時強降雨，今(10日)晚至明(11)日基隆北海岸、宜蘭地區及新北山區有局部大雨發生的機率，請注意。＊第18號颱風及其外圍環流影響，今(10日)晚至明(11)日巴士海峽及臺灣附近各海面風浪逐漸增大，基隆北海岸、東半部（含蘭嶼、綠島）、西南部、恆春半島沿海易有長浪發生，前往海邊活動請特別注意安全。＊第18號颱風外圍環流影響，今(10日)晚至明(11)日臺南以北、東半部(含蘭嶼、綠島)、恆春半島、澎湖、金門、馬祖沿海及空曠地區將有9至12級強陣風，內陸地區及其他沿海空曠地區亦有較強陣風，請注意。＊第18號颱風外圍環流沉降影響，明(11)日南投、彰化至臺南及金門地區高溫炎熱，局部地區有36度以上高溫發生的機率，請注意。＊本警報單之颱風半徑為平均半徑，第18號颱風之7級風暴風半徑西南象限較小約60公里，其他象限約180公里，平均半徑約為150公里。'}, 'category': {'@term': '颱風'} },{ ... }, ... ] } Get historical data of disaster notifications: Disaster().get_notice() d = Disaster().get_notice(\"ERA2_F1\") # 交通災情通報表（道路、橋梁部分） d \"maincmt\":{ \"prj_no\":\"專案代號\", \"org_name\":\"填報機關\", \"rpt_approval\":\"核定人\", \"rpt_phone\":\"聯絡電話\", \"rpt_mobile_phone\":\"行動電話\", \"rpt_no\":\"通報別\", \"rpt_user\":\"通報人\", \"rpt_time\":\"通報時間\" }, \"main\":{ \"prj_no\":\"2014224301\", \"org_name\":\"交通部公路總局\", ... }, \"detailcmt\":{ \"trfstatus\":\"狀態\", ... }, ... } References Python pyCIOT pypi package (https://pypi.org/project/pyCIOT/) Python pyCIOT Document (https://hackmd.io/@cclljj/pyCIOT_doc) ",
    "description": "We introduce how to obtain water, air, earthquake, and disaster data in the Civil IoT Taiwan Data Service Platform, including the latest sensing data for a single site, a list of all sites, and the latest current sensing data for all sites.",
    "tags": [
      "Python",
      "API",
      "Water",
      "Air",
      "Quake",
      "Disaster"
    ],
    "title": "3.1. Basic Data Access Methods",
    "uri": "/en/ch3/ch3.1/"
  },
  {
    "content": "\nTable Of Contents Goal Package Installation and Importing Data Access Air Quality Data Water Level Data Meteorological Data Data Visualization Data Resample Moving Average Multi-line Charts Calendar Heatmap Data Quality Inspection Outlier Detection Change Point Detection) Missing Data Handling Data Decomposition References Time series data is data formed in the order of appearance in time. Usually, the time interval in the data will be the same (for example, one data every five minutes, or one data per hour), and the application fields are quite wide, such as financial information, space engineering, signal processing, etc. There are also many statistical related tools that can used in analysis. In addition, time series data is very close to everyday life. For example, with the intensification of global climate change, the global average temperature has become higher and higher in recent years, and the summer is unbearably hot. Also, certain seasons of the year tend to have particularly poor air quality, or certain times of the year tend to have worse air quality than others. If you want to know more about these changes in living environment, and how the corresponding sensor values change, you will need to use time series data analysis, which is to observe the relationship between data and time, and then get the results. This chapter will demonstrate using three types of data (air quality, water resources, weather) in the Civil IoT Taiwan Data Service Platform.\nGoal observe time series data using visualization tools check and process time series data decompose time series data to investigate its trend and seasonality Package Installation and Importing In this article, we will use the pandas, matplotlib, numpy, seaborn, statsmodels, and warnings packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. However, we will also use two additional packages that Colab does not have pre-installed: kats and calplot, which need to be installed by :\n!pip install --upgrade pip # Kats !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 # calplot !pip install calplot After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport warnings import calplot import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import statsmodels.api as sm import os, zipfile from datetime import datetime, timedelta from dateutil import parser as datetime_parser from statsmodels.tsa.stattools import adfuller, kpss from statsmodels.tsa.seasonal import seasonal_decompose from kats.detectors.outlier import OutlierDetector from kats.detectors.cusum_detection import CUSUMDetector from kats.consts import TimeSeriesData, TimeSeriesIterator from IPython.core.pylabtools import figsize Data Access We use pandas for data processing. Pandas is a data science suite commonly used in Python language. It can also be thought of as a spreadsheet similar to Microsoft Excel in a programming language, and its Dataframe object provided by pandas can be thought of as a two-dimensional data structure. The dimensional data structure can store data in rows and columns, which is convenient for various data processing and operations.\nThe topic of this paper is the analysis and processing of time series data. We will use the air quality, water level and meteorological data on the Civil IoT Taiwan Data Service Platform for data access demonstration, and then use the air quality data for further data analysis. Among them, each type of data is the data observed by a collection of stations for a long time, and the time field name in the dataframe is set to timestamp. Because the value of the time field is unique, we also use this field as the index of the dataframe.\nAir Quality Data Since we want to use long-term historical data in this article, we do not directly use the data access methods of the pyCIOT package, but directly download the data archive of “Academia Sinica - Micro Air Quality Sensors” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Air folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Air folder.\n!mkdir Air CSV_Air !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Air/{item}') The CSV_Air folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 74DA38C7D2AC), we need to read each CSV file and put the data for that station into a dataframe called air. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' air = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'device_id==@id') air = pd.concat([air, filtered], ignore_index=True) air.dropna(subset=['timestamp'], inplace=True) for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) air.at[i, 'timestamp'] = naive air.set_index('timestamp', inplace=True) !rm -rf Air CSV_Air Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nair.drop(columns=['device_id', 'SiteName'], inplace=True) air.sort_values(by='timestamp', inplace=True) air.info() print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 Water Level Data Like the example of air quality data, since we are going to use long-term historical data this time, we do not directly use the data access methods of the pyCIOT suite, but directly download the data archive of “Water Resources Agency - Groundwater Level Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Water folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Water folder.\n!mkdir Water CSV_Water !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') The CSV_Water folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 338c9c1c-57d8-41d7-9af2-731fb86e632c), we need to read each CSV file and put the data for that station into a dataframe called water. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' water = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') water = pd.concat([water, filtered], ignore_index=True) water.dropna(subset=['timestamp'], inplace=True) for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive water.set_index('timestamp', inplace=True) !rm -rf Water CSV_Water Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nwater.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) water.sort_values(by='timestamp', inplace=True) water.info() print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 Meteorological Data We download the data archive of “Central Weather Bureau - Automatic Weather Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Weather folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Weather folder.\n!mkdir Weather CSV_Weather !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Weather/{item}') The CSV_Weather folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code C0U750), we need to read each CSV file and put the data for that station into a dataframe called weather. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' weather = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') weather = pd.concat([weather, filtered], ignore_index=True) weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) weather.dropna(subset=['timestamp'], inplace=True) for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive weather.set_index('timestamp', inplace=True) !rm -rf Weather CSV_Weather Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nweather.drop(columns=['station_id'], inplace=True) weather.sort_values(by='timestamp', inplace=True) weather.info() print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN Above, we have successfully demonstrated the reading example of air quality data (air), water level data (water) and meteorological data (weather). In the following discussion, we will use air quality data to demonstrate basic time series data processing. The same methods can also be easily applied to water level data or meteorological data and obtain similar results. You are encouraged to try it yourself.\nData Visualization The first step in the processing of time series data is nothing more than to present the information one by one in chronological order so that users can see the changes in the overall data and derive more ideas and concepts for data analysis. Among them, using line charts to display data is the most commonly used data visualization method. For example, take the air quality data as an example:\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() Data Resample As can be seen from the air quality data sequence diagram in the above figure, the distribution of data is actually very dense, and the changes in data values are sometimes small and violent. This is because the current sampling frequency of air quality data is about once every five minutes, and the collected environment is the surrounding environment information in life, so data density and fluctuation are inevitable. Because sampling every 5 minutes is too frequent, it is difficult to show general trends in ambient air pollution. Therefore, we adopt the method of resampling to calculate the average value of the data in a fixed time interval, so as to present the data of different time scales. For example, we use the following syntax to resample at a larger scale (hour, day, month) sampling rate according to the characteristics of the existing air quality data:\nair_hour = air.resample('H').mean() # hourly average air_day = air.resample('D').mean() # daily average air_month = air.resample('M').mean() # monthly average print(air_hour.head()) print(air_day.head()) print(air_month.head()) PM25 timestamp 2018-08-01 00:00:00 18.500000 2018-08-01 01:00:00 20.750000 2018-08-01 02:00:00 24.000000 2018-08-01 03:00:00 27.800000 2018-08-01 04:00:00 22.833333 PM25 timestamp 2018-08-01 23.384615 2018-08-02 13.444444 2018-08-03 14.677419 2018-08-04 14.408451 2018-08-05 NaN PM25 timestamp 2018-08-31 21.704456 2018-09-30 31.797806 2018-10-31 37.217788 2018-11-30 43.228939 Then we plot again with the hourly averaged resampling data, and we can see that the curve becomes clearer, but the fluctuation of the curve is still very large.\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air_hour[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() Moving Average For the chart of the original data, if you want to see a smoother change trend of the curve, you can apply the moving average method. The idea is to set a sampling window on the time axis of the original data, move the position of the sampling window smoothly, and calculate the average value of all values in the sampling window.\nFor example, if the size of the sampling window is 10, it means that the current data and the previous 9 data are averaged each time. After such processing, the meaning of each data is not only a certain time point, but the average value of the original time point and the previous time point. This removes abrupt changes and makes the overall curve smoother, thereby making it easier to observe overall trends.\n# plt.figure(figsize=(15, 10), dpi=60) MA = air_hour MA10 = MA.rolling(window=500, min_periods=1).mean() MA.join(MA10.add_suffix('_mean_500')).plot(figsize=(20, 15)) # MA10.plot(figsize(15, 10)) The blue line in the above figure is the original data, and the orange line is the curve after moving average. It can be clearly found that the orange line can better represent the change trend of the overall value, and there is also a certain degree of regular fluctuation, which is worthy of further analysis.\nMulti-line Charts In addition to presenting the original data in the form of a simple line chart, another common data visualization method is to cut the data into several continuous segments periodically in the time dimension, draw line charts separately, and superimpose them on the same multi-line chart. For example, we can cut the above air quality data into four sub-data sets of 2019, 2020, 2021, and 2022 according to different years, and draw their respective line charts on the same multi-line chart, as shown in the figure below.\nair_month.reset_index(inplace=True) air_month['year'] = [d.year for d in air_month.timestamp] air_month['month'] = [d.strftime('%b') for d in air_month.timestamp] years = air_month['year'].unique() print(air) np.random.seed(100) mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False) plt.figure(figsize=(15, 10), dpi=60) for i, y in enumerate(years): if i \u003e 0: plt.plot('month', 'PM25', data=air_month.loc[air_month.year==y, :], color=mycolors[i], label=y) plt.text(air_month.loc[air_month.year==y, :].shape[0]-.9, air_month.loc[air_month.year==y, 'PM25'][-1:].values[0], y, fontsize=12, color=mycolors[i]) # plt.gca().set(xlim=(-0.3, 11), ylim=(2, 30), ylabel='PM25', xlabel='Month') # plt.yticks(fontsize=12, alpha=.7) # plt.title('Seasonal Plot of PM25 Time Series', fontsize=20) plt.show() In this multi-line chart, we can see that the 2019 data has a significant portion of missing values, while the 2022 data was only recorded till July at the time of writing. At the same time, it can be found that in the four-year line chart, the curves of different years all reached the lowest point in summer, began to rise in autumn, and reached the highest point in winter, showing roughly the same trend of change.\nCalendar Heatmap The calendar heat map is a data visualization method that combines the calendar map and the heat map, which can more intuitively browse the distribution of the data and find the regularity of different time scales. We use calplot, a calendar heatmap suite for Python, and input the daily PM2.5 averages. Then we select the specified color (the parameter name is cmap, and we set it to GnBu in the following example. for detailed color options, please refer to the reference materials), and we can get the effect of the following figure, in which blue represents the greater value, green or white means lower values, if not colored or a value of 0 means there is no data for the day. From the resulting plot, we can see that the months in the middle part (summer) are lighter and the months in the left part (winter) are darker, just in line with our previous observations using the multi-line chart.\n# cmap: color map (https://matplotlib.org/stable/gallery/color/colormap_reference.html) # textformat: specify the format of the text pl1 = calplot.calplot(data = air_day['PM25'], cmap = 'GnBu', textformat = '{:.0f}', figsize = (24, 12), suptitle = \"PM25 by Month and Year\") Data Quality Inspection After the basic visualization of time series data, we will introduce the basic detection and processing methods of data quality. We will use kats, a Python language data processing and analysis suite, to perform outlier detection, change point detection, and handling missing values in sequence.\nOutlier Detection Outliers are those values in the data that are significantly different from other values. These differences may affect our judgment and analysis of the data. Therefore, outliers need to be identified and then flagged, removed, or treated specially. .\nWe first convert the data stored in the variable air_hour from its original dataframe format to the TimeSeriesData format used by the kats package and save the converted data into a variable named air_ts. Then we re-plot the line chart of the time series data.\nair_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') air_ts.plot(cols=[\"PM25\"]) We then used the OutlierDetector tool in the kats suite to detect outliers in the time series data, where outliers were less than 1.5 times the first quartile (Q1) minus the interquartile range (IQR) or greater than The third quartile (Q3) value plus 1.5 times the interquartile range.\noutlierDetection = OutlierDetector(air_ts, 'additive') outlierDetection.detector() outlierDetection.outliers [[Timestamp('2018-08-10 16:00:00'), Timestamp('2018-08-10 17:00:00'), Timestamp('2018-08-20 00:00:00'), Timestamp('2018-08-23 03:00:00'), Timestamp('2018-08-23 04:00:00'), Timestamp('2018-09-02 11:00:00'), Timestamp('2018-09-11 00:00:00'), Timestamp('2018-09-13 14:00:00'), Timestamp('2018-09-13 15:00:00'), Timestamp('2018-09-13 16:00:00'), Timestamp('2018-09-15 08:00:00'), Timestamp('2018-09-15 09:00:00'), Timestamp('2018-09-15 10:00:00'), Timestamp('2018-09-15 11:00:00'), Timestamp('2018-09-22 05:00:00'), Timestamp('2018-09-22 06:00:00'), Timestamp('2018-10-26 01:00:00'), Timestamp('2018-11-06 13:00:00'), Timestamp('2018-11-06 15:00:00'), Timestamp('2018-11-06 16:00:00'), Timestamp('2018-11-06 19:00:00'), Timestamp('2018-11-06 20:00:00'), Timestamp('2018-11-06 21:00:00'), Timestamp('2018-11-06 22:00:00'), Timestamp('2018-11-07 07:00:00'), Timestamp('2018-11-07 08:00:00'), Timestamp('2018-11-07 09:00:00'), Timestamp('2018-11-09 00:00:00'), Timestamp('2018-11-09 01:00:00'), Timestamp('2018-11-09 02:00:00'), Timestamp('2018-11-09 03:00:00'), Timestamp('2018-11-10 02:00:00'), Timestamp('2018-11-10 03:00:00'), Timestamp('2018-11-16 01:00:00'), Timestamp('2018-11-16 02:00:00'), Timestamp('2018-11-16 03:00:00'), Timestamp('2018-11-16 04:00:00'), Timestamp('2018-11-21 00:00:00'), Timestamp('2018-11-21 18:00:00'), Timestamp('2018-11-21 19:00:00'), Timestamp('2018-11-25 08:00:00'), Timestamp('2018-11-30 14:00:00'), Timestamp('2018-12-01 06:00:00'), Timestamp('2018-12-01 16:00:00'), Timestamp('2018-12-01 17:00:00'), Timestamp('2018-12-15 02:00:00'), Timestamp('2018-12-19 03:00:00'), Timestamp('2018-12-19 04:00:00'), Timestamp('2018-12-19 05:00:00'), Timestamp('2018-12-19 06:00:00'), Timestamp('2018-12-19 07:00:00'), Timestamp('2018-12-19 08:00:00'), Timestamp('2018-12-19 10:00:00'), Timestamp('2018-12-19 11:00:00'), Timestamp('2018-12-19 12:00:00'), Timestamp('2018-12-19 13:00:00'), Timestamp('2018-12-19 14:00:00'), Timestamp('2018-12-19 15:00:00'), Timestamp('2018-12-19 16:00:00'), Timestamp('2018-12-19 17:00:00'), Timestamp('2018-12-20 03:00:00'), Timestamp('2018-12-20 04:00:00'), Timestamp('2018-12-20 05:00:00'), Timestamp('2018-12-20 06:00:00'), Timestamp('2018-12-20 07:00:00'), Timestamp('2018-12-20 08:00:00'), Timestamp('2018-12-20 11:00:00'), Timestamp('2018-12-20 12:00:00'), Timestamp('2018-12-20 13:00:00'), Timestamp('2018-12-20 14:00:00'), Timestamp('2018-12-20 15:00:00'), Timestamp('2019-01-05 02:00:00'), Timestamp('2019-01-05 08:00:00'), Timestamp('2019-01-05 09:00:00'), Timestamp('2019-01-05 22:00:00'), Timestamp('2019-01-19 06:00:00'), Timestamp('2019-01-19 07:00:00'), Timestamp('2019-01-19 08:00:00'), Timestamp('2019-01-19 09:00:00'), Timestamp('2019-01-19 13:00:00'), Timestamp('2019-01-19 14:00:00'), Timestamp('2019-01-19 15:00:00'), Timestamp('2019-01-25 18:00:00'), Timestamp('2019-01-25 19:00:00'), Timestamp('2019-01-25 20:00:00'), Timestamp('2019-01-26 00:00:00'), Timestamp('2019-01-26 01:00:00'), Timestamp('2019-01-26 02:00:00'), Timestamp('2019-01-26 03:00:00'), Timestamp('2019-01-26 04:00:00'), Timestamp('2019-01-30 06:00:00'), Timestamp('2019-01-30 11:00:00'), Timestamp('2019-01-30 12:00:00'), Timestamp('2019-01-30 13:00:00'), Timestamp('2019-01-30 14:00:00'), Timestamp('2019-02-02 16:00:00'), Timestamp('2019-02-02 17:00:00'), Timestamp('2019-02-02 18:00:00'), Timestamp('2019-02-02 19:00:00'), Timestamp('2019-02-02 20:00:00'), Timestamp('2019-02-03 03:00:00'), Timestamp('2019-02-03 04:00:00'), Timestamp('2019-02-03 05:00:00'), Timestamp('2019-02-03 06:00:00'), Timestamp('2019-02-03 07:00:00'), Timestamp('2019-02-03 10:00:00'), Timestamp('2019-02-03 11:00:00'), Timestamp('2019-02-03 12:00:00'), Timestamp('2019-02-03 13:00:00'), Timestamp('2019-02-03 22:00:00'), Timestamp('2019-02-03 23:00:00'), Timestamp('2019-02-07 05:00:00'), Timestamp('2019-02-07 06:00:00'), Timestamp('2019-02-16 22:00:00'), Timestamp('2019-02-16 23:00:00'), Timestamp('2019-02-18 18:00:00'), Timestamp('2019-02-18 20:00:00'), Timestamp('2019-02-18 21:00:00'), Timestamp('2019-02-19 10:00:00'), Timestamp('2019-02-19 11:00:00'), Timestamp('2019-02-19 12:00:00'), Timestamp('2019-02-19 13:00:00'), Timestamp('2019-02-19 14:00:00'), Timestamp('2019-02-19 15:00:00'), Timestamp('2019-02-19 16:00:00'), Timestamp('2019-02-19 23:00:00'), Timestamp('2019-02-20 00:00:00'), Timestamp('2019-02-20 03:00:00'), Timestamp('2019-03-02 17:00:00'), Timestamp('2019-03-03 06:00:00'), Timestamp('2019-03-05 13:00:00'), Timestamp('2019-03-09 23:00:00'), Timestamp('2019-03-12 01:00:00'), Timestamp('2019-03-16 01:00:00'), Timestamp('2019-03-16 02:00:00'), Timestamp('2019-03-16 03:00:00'), Timestamp('2019-03-20 00:00:00'), Timestamp('2019-03-20 01:00:00'), Timestamp('2019-03-20 02:00:00'), Timestamp('2019-03-20 03:00:00'), Timestamp('2019-03-20 11:00:00'), Timestamp('2019-03-27 00:00:00'), Timestamp('2019-03-27 01:00:00'), Timestamp('2019-04-05 03:00:00'), Timestamp('2019-04-18 17:00:00'), Timestamp('2019-04-20 16:00:00'), Timestamp('2019-05-10 07:00:00'), Timestamp('2019-05-22 20:00:00'), Timestamp('2019-05-23 03:00:00'), Timestamp('2019-05-23 16:00:00'), Timestamp('2019-05-26 18:00:00'), Timestamp('2019-05-27 05:00:00'), Timestamp('2019-07-28 01:00:00'), Timestamp('2019-08-23 08:00:00'), Timestamp('2019-08-24 02:00:00'), Timestamp('2019-08-24 03:00:00'), Timestamp('2019-08-24 04:00:00'), Timestamp('2019-08-24 05:00:00'), Timestamp('2019-08-24 07:00:00'), Timestamp('2019-08-24 08:00:00'), Timestamp('2019-12-10 11:00:00'), Timestamp('2019-12-10 12:00:00'), Timestamp('2019-12-10 13:00:00'), Timestamp('2019-12-10 20:00:00'), Timestamp('2019-12-11 04:00:00'), Timestamp('2019-12-16 20:00:00'), Timestamp('2019-12-17 11:00:00'), Timestamp('2020-01-03 15:00:00'), Timestamp('2020-01-05 08:00:00'), Timestamp('2020-01-05 09:00:00'), Timestamp('2020-01-06 08:00:00'), Timestamp('2020-01-07 10:00:00'), Timestamp('2020-01-07 15:00:00'), Timestamp('2020-01-10 11:00:00'), Timestamp('2020-01-15 08:00:00'), Timestamp('2020-01-22 14:00:00'), Timestamp('2020-01-22 17:00:00'), Timestamp('2020-01-22 22:00:00'), Timestamp('2020-01-22 23:00:00'), Timestamp('2020-01-23 00:00:00'), Timestamp('2020-01-23 01:00:00'), Timestamp('2020-01-23 02:00:00'), Timestamp('2020-01-23 10:00:00'), Timestamp('2020-01-23 11:00:00'), Timestamp('2020-01-23 12:00:00'), Timestamp('2020-01-23 13:00:00'), Timestamp('2020-01-23 15:00:00'), Timestamp('2020-01-23 16:00:00'), Timestamp('2020-01-23 17:00:00'), Timestamp('2020-01-23 18:00:00'), Timestamp('2020-01-23 20:00:00'), Timestamp('2020-01-23 21:00:00'), Timestamp('2020-01-23 22:00:00'), Timestamp('2020-01-23 23:00:00'), Timestamp('2020-01-24 00:00:00'), Timestamp('2020-01-24 01:00:00'), Timestamp('2020-01-24 02:00:00'), Timestamp('2020-01-24 03:00:00'), Timestamp('2020-02-12 10:00:00'), Timestamp('2020-02-12 11:00:00'), Timestamp('2020-02-12 12:00:00'), Timestamp('2020-02-12 13:00:00'), Timestamp('2020-02-12 14:00:00'), Timestamp('2020-02-12 19:00:00'), Timestamp('2020-02-12 20:00:00'), Timestamp('2020-02-12 22:00:00'), Timestamp('2020-02-12 23:00:00'), Timestamp('2020-02-13 20:00:00'), Timestamp('2020-02-14 00:00:00'), Timestamp('2020-02-14 01:00:00'), Timestamp('2020-02-15 10:00:00'), Timestamp('2020-02-19 08:00:00'), Timestamp('2020-02-19 09:00:00'), Timestamp('2020-02-19 10:00:00'), Timestamp('2020-02-25 02:00:00'), Timestamp('2020-02-25 03:00:00'), Timestamp('2020-03-09 07:00:00'), Timestamp('2020-03-18 21:00:00'), Timestamp('2020-03-18 22:00:00'), Timestamp('2020-03-19 01:00:00'), Timestamp('2020-03-20 04:00:00'), Timestamp('2020-03-21 09:00:00'), Timestamp('2020-03-21 10:00:00'), Timestamp('2020-03-28 22:00:00'), Timestamp('2020-04-15 03:00:00'), Timestamp('2020-04-28 03:00:00'), Timestamp('2020-04-28 04:00:00'), Timestamp('2020-05-01 13:00:00'), Timestamp('2020-05-01 15:00:00'), Timestamp('2020-05-01 23:00:00'), Timestamp('2020-05-02 00:00:00'), Timestamp('2020-11-17 14:00:00'), Timestamp('2020-11-17 20:00:00'), Timestamp('2020-11-17 21:00:00'), Timestamp('2020-11-17 22:00:00'), Timestamp('2020-11-18 19:00:00'), Timestamp('2020-11-18 20:00:00'), Timestamp('2020-11-18 23:00:00'), Timestamp('2020-11-19 00:00:00'), Timestamp('2020-11-19 01:00:00'), Timestamp('2020-12-21 15:00:00'), Timestamp('2020-12-27 14:00:00'), Timestamp('2020-12-27 15:00:00'), Timestamp('2020-12-27 16:00:00'), Timestamp('2020-12-27 21:00:00'), Timestamp('2021-01-16 09:00:00'), Timestamp('2021-01-16 10:00:00'), Timestamp('2021-01-16 11:00:00'), Timestamp('2021-02-01 10:00:00'), Timestamp('2021-02-03 09:00:00'), Timestamp('2021-02-03 10:00:00'), Timestamp('2021-02-06 11:00:00'), Timestamp('2021-02-06 17:00:00'), Timestamp('2021-02-08 11:00:00'), Timestamp('2021-02-11 14:00:00'), Timestamp('2021-02-25 22:00:00'), Timestamp('2021-03-12 08:00:00'), Timestamp('2021-03-19 15:00:00'), Timestamp('2021-03-19 20:00:00'), Timestamp('2021-03-29 13:00:00'), Timestamp('2021-04-06 07:00:00'), Timestamp('2021-04-12 15:00:00'), Timestamp('2021-04-13 16:00:00'), Timestamp('2021-11-04 14:00:00'), Timestamp('2021-11-04 15:00:00'), Timestamp('2021-11-04 23:00:00'), Timestamp('2021-11-05 00:00:00'), Timestamp('2021-11-05 01:00:00'), Timestamp('2021-11-05 05:00:00'), Timestamp('2021-11-05 06:00:00'), Timestamp('2021-11-05 11:00:00'), Timestamp('2021-11-05 15:00:00'), Timestamp('2021-11-28 15:00:00'), Timestamp('2021-11-29 10:00:00'), Timestamp('2021-12-21 11:00:00')]] Finally, we delete the detected outliers from the original data, and re-plot the chart to compare it with the original one. We can clearly find some outliers (for example, there is an abnormal peak in 2022-07) have been removed.\noutliers_removed = outlierDetection.remover(interpolate=False) outliers_removed outliers_removed.plot(cols=['y_0']) Change Point Detection) A change point is a point in time at which the data suddenly changes significantly, representing the occurrence of an event, a change in the state of the data, or a change in the distribution of the data. Therefore, change point detection is often regarded as an important preprocessing step for data analysis and data prediction.\nIn the following example, we use daily averages of air quality data for change point detection. We use the TimeSeriesData data format of the kats package to store the data and use the CUSUMDetector detector provided by kats for detection. We use red dots to represent detected change points in the plot. Unfortunately, in this example, no obvious point of change was observed. Readers are advised to refer to this example and bring in other data for more exercise and testing.\nair_ts = TimeSeriesData(air_day.reset_index(), time_col_name='timestamp') detector = CUSUMDetector(air_ts) change_points = detector.detector(change_directions=[\"increase\", \"decrease\"]) # print(\"The change point is on\", change_points[0][0].start_time) # plot the results plt.xticks(rotation=45) detector.plot(change_points) plt.show() Missing Data Handling Missing data is a common problem when conducting big data analysis. Some of these missing values are already missing at the time of data collection (such as sensor failure, network disconnection, etc.), and some are eliminated during data preprocessing (outliers or abnormality). However, for subsequent data processing and analysis, we often need the data to maintain a fixed sampling rate to facilitate the application of various methods and tools. Therefore, various methods for imputing missing data have been derived. Below we introduce three commonly used methods:\nMark missing data as Nan (Not a number): Nan stands for not a number and is used to represent undefined or unrepresentable values. If it is known that subsequent data analysis will additionally deal with these special cases of Nan, this method can be adopted to maintain the authenticity of the information. Forward filling method: If Nan has difficulty in subsequent data analysis, missing values must be filled with appropriate numerical data. The easiest way to do this is forward fill, which uses the previous value to fill in the current missing value. # forward fill df_ffill = air.ffill(inplace=False) df_ffill.plot() 3. \u0006K-Nearest Neighbor (KNN) method: As the name suggests, the KNN method finds the k values that are closest to the missing value, and then fills the missing value with the average of these k values.\ndef knn_mean(ts, n): out = np.copy(ts) for i, val in enumerate(ts): if np.isnan(val): n_by_2 = np.ceil(n/2) lower = np.max([0, int(i-n_by_2)]) upper = np.min([len(ts)+1, int(i+n_by_2)]) ts_near = np.concatenate([ts[lower:i], ts[i:upper]]) out[i] = np.nanmean(ts_near) return out # KNN df_knn = air.copy() df_knn['PM25'] = knn_mean(air.PM25.to_numpy(), 5000) df_knn.plot() Data Decomposition In the previous example of basic data processing, we have been able to roughly observe the changing trend of data values and discover potential regular changes. In order to further explore the regularity of time series data changes, we introduce the data decomposition method. In this way, the original time series data can be disassembled into trend waves (trend), periodic waves (seasonal) and residual waves (residual).\nWe first replicated the daily average data of air quality data as air_process and processed the missing data using forward filling. Then, we first presented the raw data directly in the form of a line chart.\nair_process = air_day.copy() # new.round(1).head(12) air_process.ffill(inplace=True) air_process.plot() Then we use the seasonal_decompose method to decompose the air_process data, in which we need to set a period parameter, which refers to the period in which the data is decomposed. We first set it to 30 days, and then after execution, it will produce four pictures in sequence: raw data, trend chart, seasonal chart, and residual chart.\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=30) decompose.plot().set_size_inches((15, 15)) plt.show() In the trend graph (trend), we can also find that the graph of the original data has very similar characteristics, with higher values around January and lower values around July; while in the seasonal graph (seasonal), we can find that the data has a fixed periodic change in each cycle (30 days), which means that the air quality data has a one-month change.\nIf we change the periodic variable to 365, i.e. decompose the data on a larger time scale (one year), we can see a trend of higher values around January and lower values around July from the seasonal plot, and this trend change occurs on a regular and regular basis. At the same time, it can be seen from the trend chart that the overall trend is slowing down, indicating that the concentration of PM2.5 is gradually decreasing under the overall trend. The results also confirmed our previous findings that no change points were detected, as the change trend of PM2.5 was a steady decline without abrupt changes.\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=365) decompose.plot().set_size_inches((15, 15)) plt.show() References Civil IoT Taiwan: Historical Data (https://history.colife.org.tw/) Matplotlib - Colormap reference (https://matplotlib.org/stable/gallery/color/colormap_reference.html) Decomposition of time series - Wikipedia (https://en.wikipedia.org/wiki/Decomposition_of_time_series) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057?gi=36d1c3d8372) Decomposition in Time Series Data | by Abhilasha Chourasia | Analytics Vidhya | Medium (https://medium.com/analytics-vidhya/decomposition-in-time-series-data-b20764946d63) ",
    "description": "We use the sensing data of Civil IoT Taiwan Data Service Platform to guide readers to understand the use of moving average, perform periodic analysis of time series data, and then disassemble the time series data into long-term trends, seasonal changes and residual fluctuations. At the same time, we apply the existing Python language suites to perform change point detection and outlier detection to check the existing Civil IoT Taiwan data, and discuss potential implications of such values detected.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "4.1. Time Series Data Processing",
    "uri": "/en/ch4/ch4.1/"
  },
  {
    "content": " Table Of Contents Goal QGIS Basic Operation Data Preparation GeoJSON Output Data Filter and Change Colors Export Thematic Maps Example 2: Distribution of Emergency Shelters Conclusion References QGIS is a free geographic information system. In addition to presenting the data collected by users in the form of geographic data, users can also process, analyze and integrate geospatial data through QGIS, and draw thematic maps. In this article, we will use QGIS to assist in the analysis and presentation of PM2.5 data obtained from Civil IoT Taiwan, and output the results as a thematic map for interpretation after the analysis is complete. We also demonstrate how to combine the disaster prevention data of Civil IoT Taiwan to draw a distribution map of disaster prevention shelters through the QGIS system, allowing citizens to easily query the nearest disaster shelters.\nNote Note: The QGIS version used in this article is 3.16.8, but the functions used in this article are the basic functions of the software. If you use another version of the software, you should still be able to run it normally.\nGoal Import data into QGIS Basic geographic and spatial data analysis in QGIS Export thematic maps from QGIS QGIS Basic Operation After entering the QGIS software, you can see the following operation interface. In addition to the data frame in the middle area, the upper part is the standard toolbar, which provides various basic operation tools and functions. There are two sub-areas on the left, which are the data catalog window and layers; on the right is the analysis toolbar, which provides various analysis tools.\nData Preparation Data source: Civil IoT Taiwan - Historical Data (https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器)\nIn this section, we will first describe how to import data into QGIS. Since some data will be split into multiple different data tables during the storage process, we should pay special attention to whether this is the case with the data at hand when analyzing, and recombine the data into the original single data table. Here’s how to recombine the data:\nImport data\nFirst, we need to import the csv file directly downloaded from Civil IoT Taiwan Historical Data into QGIS. Since there are Chinese characters in the data, garbled characters will be displayed when importing, so the import method is Layer (at the top menu) \u003e Add Layer \u003e Add Delimited Text Layer, and the import interface is as follows: Join data\nSince the original data divides PM2.5 and station coordinates (latitude and longitude) into two files, it is necessary to join PM2.5 and latitude and longitude coordinates first. The join method is as follows, right-click the file to be joined \u003e Properties \u003e joins \u003e click the + sign to enter Add Vector Join, as shown below There are four options after entering:\nJoin Layer: the layer you want to join Join field: corresponding to the Target field, which is the reference for Join (similar to Primary key) Target field: corresponding to the Join field, which is the reference for Join (similar to Foreign key) Joined field: the fields you want to join GeoJSON Output Then we use the built-in functions of QGIS to convert the original csv data into GeoJSON files, which is a geographical data representation in JSON format. The operation procedure is to click Processing \u003e Toolbox \u003e Create points from table.\nPlease note that after clicking, select the Table to be imported, select lon in X, enter lat in Y, and specify WGS 84 (latitude and longitude coordinates) in Target CRS, and then enter the name of the file to be output, as shown below.\nThen select the file format you want to export, and click “Save”.\nData Filter and Change Colors Next, we demonstrate how to use QGIS to filter the required stations, and let the color of the stations change with the PM2.5 value.\nUse Intersection to filter the required stations by county\nBefore screening, you need to download the shp files of municipalities, counties and cities from the government data open platform, and then import the shp files of the county and city boundaries into QGIS, as shown below: Note Note: In the Civil IoT Taiwan project, National Chi Nan University is responsible for the deployment of micro air quality sensors in Changhua and Nantou counties and cities. Therefore, in the data obtained this time, there is no Changhua and Nantou. material.\nThen we click on the icon for “Select Feature”, then click on “Country of Country” and select the desired county. Selected counties will be displayed in yellow. Here we take New Taipei City as an example, as shown in the following figure: We then look for the “Intersection” function in “Processing Toolbox”, and after clicking, the following interface will appear, in which there are three input options, namely:\nInput Layer Overlay Layer Intersection (Output) Next, please put the PM2.5 layer in the “Input Layer”, put the county-city boundary layer in the “Overlay Layer” and check “Select features only”, which means that only the stations that intersect with the New Taipei City selected in the previous step are filtered out. Next, enter the name of the file to be exported in the “Intersection”. The supported export file format options are the same as the previously selected options.\nThen, the following results will be obtained: Display different colors according to the value of PM2.5\nThen we right click on the PM2.5 layer \u003e Properties \u003e Symbology to see the dot color settings. The color setting steps for each PM2.5 station are as follows:\nChange the top original setting from “No Symbols” to “Graduated” as follows Select PM25 in the “Value” part Click the “Classify” button at the bottom Set the number of colors in “Classes” (Note: It is recommended to set the number of categories should not be too many) Go to “Color ramp” to set the color of each value Clock “OK” When everything is set, you will get the following image, where the color of the dots changes with the PM 2.5 value, and the Layer on the right shows the PM 2.5 value represented by the different colors. Note Note: The new version of QGIS (after 3.16) already has the basemap of OpenStreetMap, you can click XYZ Tiles -\u003e OpenStreetMap in the figure below to add the OSM basemap.\nExport Thematic Maps After completing the above settings, the next step is to output the QGIS project as a JPG image. After we click Project \u003e New Print Layout, the Layout name setting will pop up. After the setting is completed, the following screen will appear:\nClick “Add map” on the left, and select the range on the drawing area to add the PM 2.5 map, as shown below\nNext, click “Add Legend” on the left and select a range to import the label, while the “Item Properties” on the right can be used to change the font size, color, etc. in the label. Finally, we put the title, scale bar, and compass to complete the thematic map.\nFinally, click Layout \u003e Export as Image in the upper left corner to export the thematic map as an image file.\nExample 2: Distribution of Emergency Shelters Data source: https://data.gov.tw/dataset/73242\nIn the government’s public information, Taiwan’s emergency shelters have been organized into electronic files for the convenience of citizens to download and use. In this example, we’ll use this data to describe how to find the closest shelter to home via QGIS.\nWe first obtain the shelter information from the above URL, and then load the information according to the method mentioned above, as shown below:\nDue to the large number of shelters in Taiwan, this article only analyzes the shelters in Taipei City, and other counties and cities can also be analyzed in the same way. Readers are welcome to try it for themselves. We first use the above intersection method to find the shelters in Taipei City, and then use the Voronoi Polygons on the side toolbar to draw the Voronoi diagram, as shown below:\nFill in the layer of the shelters in Voronoi Polygons and press “Run”\nAccording to the characteristics of Voronoi Diagram, we can know the location of the closest shelter to our house, as shown below:\nAfter the analysis is completed, the analysis results can be produced into a thematic map according to the previous method.\nConclusion In this chapter, we introduced how to import data into QGIS, and how to use the analysis tools in QGIS to investigate the data. Finally, we introduce the method of data exporting, which can make the analyzed data into thematic maps for interpretation. Of course, there are still many functions in QGIS that are not covered in this chapter. If you are interested in QGIS, you can refer to additional resources below.\nReferences QGIS: A Free and Open Source Geographic Information System (https://qgis.org/) QGIS Tutorials and Tips (https://www.qgistutorials.com/en/) QGIS Documentation (https://www.qgis.org/en/docs/index.html) ",
    "description": "We introduce the presentation of geographic data using the QGIS system, and use the data from Civil IoT Taiwan as an example to perform geospatial analysis by clicking and dragging. We also discuss the advantages and disadvantages of QGIS software and when to use it.",
    "tags": [
      "Air",
      "Disaster"
    ],
    "title": "7.1. QGIS Application",
    "uri": "/en/ch7/ch7.1/"
  },
  {
    "content": " Table Of Contents Programming Language - Python Development Platform – Google Colab References Programming Language - Python The subject of this material is the data application of Civil IoT Taiwan. This set of articles will use Python, a programming language commonly used in the data science community, as the main programming language, and adopt an easy-to-understand demonstration method to guide you to read while doing it, step-by-step into the content of each chapter of the topic, and obtain first-hand experience with Civil IoT Taiwan data applications, and the ability to draw inferences from other data science topics in the future.\nOverall, the Python programming language quickly became the most popular programming language in the data science community in a short period of time, mainly due to the following three advantages:\nLower barriers to learning: Compared with other text-based programming languages (such as C and Java), the Python language has fewer special symbols, and literally looks closer to the common English articles in daily life. If you master Python syntax and program execution logic, coupled with the ability of English words, it is easy to understand the semantics of Python code, so the threshold for learning Python is very low! A wide variety of libraries: After more than 30 years of development, the Python language has grown into an ecosystem. In addition to the basic syntax, various packages can be added, which can be advanced and transformed into various tools, suitable for solving various problems. By customizing a Python toolkit (pyCIOT) in this project, the ceiling of learning can be raised faster, so as to understand how to more conveniently apply Civil IoT Taiwan data. More suitable for self-study: With Kit, code reading comprehension is more important than code writing ability! If you want to control an existing kit, you have to understand the manuals, then line them up and piece together the code that solves the problem. Therefore, the problem-solving mode will be “stacking programs on the basis of existing program masters, rather than writing Python code from scratch”; especially the development of code reading and comprehension skills, it is more suitable to use the self-learning mode. After you are familiar with the logic of Python, learning to read code will have the opportunity to be as fun as reading a storybook. Due to the above advantages of the Python language, the Python language has become the most commonly used programming language in data science and the first programming language used by many programming learners. Therefore, in addition to various Python language teaching books on the market, many practical learning resources can also be found online. It is worth further exploration and learning for readers interested in the Python language.\nFree Courses Python for Everybody Specialization, Coursera (https://www.coursera.org/specializations/python) Python for Data Science, AI \u0026 Development, Coursera (https://www.coursera.org/learn/python-for-applied-data-science-ai) Introduction to Python Programming, Udemy (https://www.udemy.com/course/pythonforbeginnersintro/) Learn Python for Total Beginners, Udemy (https://www.udemy.com/course/python-3-for-total-beginners/) Google’s Python Class (https://developers.google.com/edu/python) Introduction to Python, Microsoft (https://docs.microsoft.com/en-us/learn/modules/intro-to-python/) Learn Python 3 from Scratch, Educative (https://www.educative.io/courses/learn-python-3-from-scratch) Free e-Book Non-Programmer’s Tutorial for Python 3, Josh Cogliati (https://en.wikibooks.org/wiki/Non-Programmer’s_Tutorial_for_Python_3) Python 101, Michael Driscoll (https://python101.pythonlibrary.org/) The Python Coding Book, Stephen Gruppetta (https://thepythoncodingbook.com/) Python Data Science Handbook, Jake VanderPlas (https://jakevdp.github.io/PythonDataScienceHandbook/) Intro to Machine Learning with Python, Bernd Klein (https://python-course.eu/machine-learning/) Applied Data Science, Ian Langmore and Daniel Krasner (https://columbia-applied-data-science.github.io/) Development Platform – Google Colab Unlike C language, which can be compiled into executable files in advance, Python itself is an interpreted language, that is, it is translated into machine language for computer execution before execution. In other words, it is a literal translation when executed, in situations similar to everyday life. In this way, it is as if translators are helping us to translate and communicate in a language acceptable to foreigners. When we speak a sentence, the translator directly helps us translate the sentence. On the contrary, when the foreigner speaks, the staff will help us translate the foreigner’s words so that we can understand.\nDue to this feature of the interpreted language, in addition to the program editor officially provided by Python, there are many other editors with different functions. Based on the similarity between Python code and natural language, some people have proposed to make the Python editor into a notebook-like mode, an editor that can mix natural language articles and Python code on one page, the most famous of which is Jupyter.\nOn the basis of Jupyter, Google moved the Python interpretation language to the Internet cloud. As long as you apply for a Google account, you can install the free Colab application on Google Drive, and use the browser directly to enjoy the Python program editing function. Due to its simplicity and rich functionality, it has become the most used development platform for Python users.\nOverall, Google Colab has four major advantages:\nPre-installed packages: The Google Colab development platform has pre-installed most of the Python packages, and users can use them directly, avoiding their own installation, and even eliminating the problem of version conflicts caused by different packages during the installation process, greatly reducing the entry threshold for using Python to develop programs. Cloud storage: With Google’s own cloud storage space, Google Colab can store program notes during development in the cloud space. As long as it is accessed through the network, it can be seamlessly connected even on different computers, solving the problems of data storage, backup and portability. Collaboration：Google Colab provides network sharing and online collaboration features, allowing users to share program notes with other users through cloud storage, and allowing users to invite other users to browse, annotate, and even edit their own program notes, speeding up team collaboration . Free GPU and TPU resources: With Google’s own rich cloud computing resources, Google Colab provides GPU and TPU processors, allowing users to use high-end processors to execute their own personal program notes, which is conducive to large-capacity program development needs. For related learning resources of Google Colab, please refer to the following links:\nGetting Started With Google Colab, Anne Bonner (https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c) Use Google Colab Like A Pro, Wing Poon (https://pub.towardsai.net/use-google-colab-like-a-pro-39a97184358d) References Python (https://www.python.org/) Google Colaboratory (https://colab.research.google.com/) Jupyter: Free software, open standards, and web services for interactive computing across all programming languages (https://jupyter.org/) 4 Reasons Why You Should Use Google Colab for Your Next Project, Orhan G. Yalçın (https://towardsdatascience.com/4-reasons-why-you-should-use-google-colab-for-your-next-project-b0c4aaad39ed) ",
    "description": "A brief introduction of the programming language Python and the development platform Google Colab used in the materials",
    "tags": [
      "Introduction"
    ],
    "title": "2.2. Material Tools",
    "uri": "/en/ch2/ch2.2/"
  },
  {
    "content": "\nTable Of Contents Data access under temporal conditions Data access under spatial conditions Case study: Is the air quality here worse than there? Import data Remove invalid data Calculate distance Pandas package Results References This article will access the data of the Civil IoT Taiwan project from the perspective of time and space, and carry out a simple implementation with the theme of air quality monitoring. It will covers the following topics:\nthe usage of the datetime, math, numpy, and pandas packages the processing of JSON format data data processing using Pandas DataFrame Data access under temporal conditions When using get_data() in pyCIOT, data can be obtained according to the start time and end time. The format is passed in the time_range as a dictionary (Dict), which are start, end and num_of_data respectively.\nstart and end refer to the start and end time of data collection, in ISO8601 or Datetime format. num_of_data will control the number of data acquisitions not to exceed this number. If the data in the range exceeds num_of_data, it will be collected at intervals, so that the time interval between data and data tends to be the same.\nTaking air quality data as an example, the acquired data can go back one day at most, so when the end variable is set greater than one day, no data will be acquired. In addition, since the update frequency of each sensor in the Civil IoT Taiwan project is different, the number of data sets per day for different sensors will also be different. For details, please refer to: https://ci.taiwan.gov.tw/dsp/dataset_air.aspx\nfrom datetime import datetime, timedelta end_date = datetime.now() # current datetime isodate_end = end_date.isoformat().split(\".\")[0]+\"Z\" # convert to ISO8601 format start_date = datetime.now() + timedelta(days = -1) # yesterday isodate_start = start_date.isoformat().split(\".\")[0]+\"Z\" # convert to ISO8601 format time = { \"start\": isodate_start, \"end\": isodate_end, \"num_of_data\": 15 } data = Air().get_data(\"OBS:EPA_IoT\", stationIds=[\"11613429495\"], time_range=time) data The data will be stored in data in List format, and stored separately according to different properties. The data of temperature, relative humidity and PM2.5 will be stored in the ‘values’ list under the corresponding name respectively, and the time of each data record will be marked and displayed in ISO8601.\n[{'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 30.7}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T12:54:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:53:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 11.9}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 12.15}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 12.2}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 12.22}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 12.31}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 12.19}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 12.26}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 12.17}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 12.04}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 11.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 11.67}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 11.56}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 11.56}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}}] Data access under spatial conditions In pyCIOT, there are also methods to get specific data based on the region. If we take the latitude and longitude of a specific location as the center, we can use the radius distance to form a search range (circle) to get the station ID and sensing value of the specific space.\nThe data format of a specific area is also passed in as a dictionary (Dict), where the latitude, longitude and radius are “latitude”, “longitude” and “distance” respectively. The filtering functions of a specific area and a specific time can be used at the same time. When searching for a specific area, the stations to be observed can also be put into the “stationIds”, and the stations outside the area can be removed by the way.\nloc = { \"latitude\": 24.990550, # 緯度 \"longitude\": 121.507532, # 經度 \"distance\": 3.0 # 半徑(km) } c = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) c[0] { 'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': { 'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [ { 'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 35.84}] },{ 'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 59.5}] },{ 'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 11.09}] } ], 'location': { 'latitude': 24.998769, 'longitude': 121.512717, 'address': None } } The above are the basic methods commonly used to obtain pyCIOT station data, using time and space as the filtering criteria, and applicable to all data including location and timestamp types. To demonstrate, we give some simple examples and implement them using these pyCIOT packages.\nCase study: Is the air quality here worse than there? Project codes: OBS:EPA_IoT (low-cost air quality stations by EPA） Target location: Nanshijiao MRT Station Exit 1 (GPS coordinates: 24.990550, 121.507532) Region of interest: Zhonghe District, New Taipei City Import data First, we need to get all the information about the target location and the region of interest. We can use the method of “data access under spatial conditions” to set the latitude and longitude at Exit 1 of Nanshijiao MRT Station, and set the distance to three kilometers. Then, we simply use Air().get_data() to obtain the data:\nloc = { \"latitude\": 24.990550, \"longitude\": 121.507532, \"distance\": 3.0 # (km) } EPA_IoT_zhonghe_data_raw = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) print(\"len:\", len(EPA_IoT_zhonghe_data_raw)) EPA_IoT_zhonghe_data_raw[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [{'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 94.84}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 3.81}]}, {'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 25.72}]}], 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}} Remove invalid data Of the in-scope sites, not every station is still running smoothly. In order to remove potentially problematic sites, we observe the data characteristics of invalid stations and find that all three data (temperature, humidity, PM2.5 concentration) are 0. So we just need to pick and delete this data before we can move on to the next step.\n# Data cleaning EPA_IoT_zhonghe_data = [] for datajson in EPA_IoT_zhonghe_data_raw: # 確認資料存在 if \"data\" not in datajson: continue; # 將格式轉換為 Temperature, Relative_Humidity 和 PM2_5 for rawdata_array in datajson['data']: if(rawdata_array['name'] == 'Temperature'): datajson['Temperature'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'Relative humidity'): datajson['Relative_Humidity'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'PM2.5'): datajson['PM2_5'] = rawdata_array['values'][0]['value'] datajson.pop('data') # 確認所有資料皆為有效，同時去除無資料之檢測站 if \"Relative_Humidity\" not in datajson.keys(): continue if \"PM2_5\" not in datajson.keys(): continue if \"Temperature\" not in datajson.keys(): continue if(datajson['Relative_Humidity'] == 0 and datajson['PM2_5'] == 0 and datajson['Temperature'] == 0): continue EPA_IoT_zhonghe_data.append(datajson) print(\"len:\", len(EPA_IoT_zhonghe_data)) EPA_IoT_zhonghe_data[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24} Calculate distance Assuming that there is no error in the data of each station, the data of the station closest to the target position is the data to be compared. To find the nearest station, we need to calculate the distance between each station and the target location.\nWe can use the point-to-point distance formula to calculate and sort to find the closest station to the target location. But here we use the standard Haversine formula to calculate the spherical distance between two points on Earth. The following is the implementation in the WGS84 coordinate system:\nimport math def LLs2Dist(lat1, lon1, lat2, lon2): R = 6371 dLat = (lat2 - lat1) * math.pi / 180.0 dLon = (lon2 - lon1) * math.pi / 180.0 a = math.sin(dLat / 2) * math.sin(dLat / 2) + math.cos(lat1 * math.pi / 180.0) * math.cos(lat2 * math.pi / 180.0) * math.sin(dLon / 2) * math.sin(dLon / 2) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) dist = R * c return dist for data in EPA_IoT_zhonghe_data: data['distance'] = LLs2Dist(data['location']['latitude'], data['location']['longitude'], 24.990550, 121.507532)# (24.990550, 121.507532) EPA_IoT_zhonghe_data[0] {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24, 'distance': 1.052754763080127} Pandas package Pandas is a Python package of commonly used data manipulation and analysis. DataFrame format is used to store two-dimensional or multi-column data format, which is very suitable for data analysis. We convert the processed data into a DataFrame, pick out the required fields, and sort them according to the previously calculated distance from small to large.\nimport pandas as pd df = pd.json_normalize(EPA_IoT_zhonghe_data) #Results contain the required data df EPA_IoT_zhonghe_data_raw = df[['distance', 'PM2_5', 'Temperature', 'Relative_Humidity', 'properties.stationID', 'location.latitude', 'location.longitude', 'properties.areaType']] EPA_IoT_zhonghe_data_raw = EPA_IoT_zhonghe_data_raw.sort_values(by=['distance', 'PM2_5'], ascending=True) EPA_IoT_zhonghe_data_raw Results To know if the air quality at the target location is better than the area of interest, we can roughly use the distribution of air quality across all stations. You can use tools such as the numpy package, a common data science processing library in Python, or directly calculate the mean and standard deviation to get the answer.\nimport numpy as np zhonghe_target = EPA_IoT_zhonghe_data_raw.iloc[0,1] zhonghe_ave = np.mean(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) zhonghe_std = np.std(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) result = (zhonghe_target-zhonghe_ave)/zhonghe_std print('Mean:', zhonghe_ave, 'std:', zhonghe_std) print('PM2.5 of the neareat station:', zhonghe_target) print('The target is ', result, 'standard deviations from the mean.\\n') if(result\u003e0): print('Result: The air quality at the target location is worse.') else: print('Result: The air quality at the target location is better.') Mean: 6.71 std: 3.18 PM2.5 of the neareat station:7.38 The target is 0.21 standard deviations from the mean. Result: The air quality at the target location is worse. References Python pyCIOT package (https://pypi.org/project/pyCIOT/) pandas - Python Data Analysis Library (https://pandas.pydata.org/) 10 minutes to pandas — pandas documentation (https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) NumPy (https://numpy.org/) NumPy quickstart (https://numpy.org/doc/stable/user/quickstart.html) Haversine formula - Wikipedia (https://en.wikipedia.org/wiki/Haversine_formula) ",
    "description": "We introduce how to obtain the data of a project in a specific time or time period, and the data of a project in a specific geographical area in the Civil IoT Taiwan Data Service Platform. We also demonstrate the application through a simple example.",
    "tags": [
      "Python",
      "API",
      "Air"
    ],
    "title": "3.2. Data Access under Spatial or Temporal Conditions",
    "uri": "/en/ch3/ch3.2/"
  },
  {
    "content": " Table Of Contents Goal Data Source Tableau Basic Operation Data Import Worksheet Introduction Tableau Example 1: Spatio-temporal Distribution of Air Quality Data Spatial Distribution Graph Time Series Graph Tableau Example 2: Disaster Notification Dashboard Disaster data format conversion Dashboard size Add worksheets to a dashboard Add interactions Interacting multiple worksheet Additional information Story Conclusion References Data visualization is a method of expressing data in a graphical way, which can help us to have a better understanding of the data. When visualizing data, different types of graphs are suitable for different types of data. For example, when presenting data with latitude and longitude coordinates, map-type charts are mostly used; when presenting time series data, line charts or histograms can be used. However, many real-world data often have more than one characteristic. For example, the sensor data of Civil IoT Taiwan has both latitude and longitude coordinates and time series characteristics. Therefore, when presenting these data, it is often necessary to use more than one kind of chart to present the insight of the data. In this chapter, we will introduce a very popular software, Tableau, to help us present the above-mentioned complex data types.\nTableau is an easy-to-use visual analysis platform. In addition to quickly creating various charts, its dashboard function makes it easier for users to present and integrate different data charts. In addition, dashboards allow users to interact with graphs, which not only helps users understand data more easily and quickly, but also makes data analysis easier. Tableau was originally a paid commercial software, but it also provides a free version of Tableau Public for everyone to use. Note, however, that when using the free version of Tableau, all work produced with it must be made public.\nNote Note: The Tableau Public version used in this article is Tableau Desktop Public Edition (2022.2.2 (20222.22.0916.1526) 64-bit), but the functions used in this article are the basic functions of the software. If you use other versions of the software, you should still be able to run it normally.\nGoal Import data into Tableau for data visualization Create data charts using Worksheets Create interactive charts and reports using dashboards and stories in Tableau Public Data Source Civil IoT Taiwan Historical Data: Academia Sinica - Micro Air Quality Sensors (https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器) Civil IoT Taiwan Historical Data: National Fire Agency - Disaster Notifications (https://history.colife.org.tw/#/?cd=%2F災害示警與災情通報%2F消防署_災情通報) Tableau Basic Operation Data Import Tableau can read text files (csv) and spatial information files (shp, geojson). To import data, click Data \u003e Add Data Source to select the data file to be imported.\nDepending on the type of input file, we provide different examples of operations below:\nExamples using geospatial data format (shp、geojson)\nFirst, please click the data source in the lower left corner, and then you can see the data fields imported now. ![Tableau screenshot](figures/7-2-3-2.png) Then click the symbol above the field name to change the properties of that field.\nFinally, we assign the latitude and longitude coordinates to geographic roles for subsequent work.\nExamples using test data format (CSV)\nAfter importing the PM 2.5 records and station locations into Tableau, we establish the relationship between the data tables:\nNext we establish the link between the PM 2.5 station and the station coordinates. We first click the station data twice to enter the connection canvas, and drag another data table to the connection canvas.\nFinally, we click the link icon in the middle to set the link type and operation logic.\nWhen the setting is completed, you can see that the Station id in the lower right corner is from the Location data table, so that the PM 2.5 data and the station location data are linked together.\nWorksheet Introduction A worksheet is where Tableau can use to visualize data. In addition to plotting data into traditional graphs such as pie, bar, and line graphs, it can also be used to map geographic information. Next we will show how to draw different shapes.\nAfter processing the data, click the new worksheet in the lower left, and you can see the interface below after entering the worksheet.\nThere is a “Show” button in the upper right corner of the worksheet. After clicking, you will see different types of charts (the system will automatically determine the charts that can be drawn, and those that cannot be drawn will be highlighted), and then you can click the desired type to change the chart.\nTableau Example 1: Spatio-temporal Distribution of Air Quality Data Spatial Distribution Graph Before starting to draw, we need to change the latitude and longitude from a measure to a dimension. For a detailed explanation of the measure and dimension, please refer to the reference materials. The detailed operation process is shown in the following animation:\nThen we drag the latitude and longitude to the position of the column and row, and drag the PM 2.5 value to the marked position. Then we change the mark to color to generate the distribution map of PM 2.5 in Taiwan, as shown in the following animation.\nWe can click on the color to change the color of the dots, and the filter above can filter the PM 2.5 value for a specific condition. If you click PM 2.5 \u003e Metric, you can select the mean, sum, or median value of PM 2.5. The following will demonstrate how to display the PM 2.5 value for New Taipei City.\nDrag “Site Name” to filter Click on wildcards, click “Include” and enter New Taipei City Click “OK” Time Series Graph We drag “Timestamp” to the column and “Site Name” and PM 2.5 value to the row. Then we click “Timestamp” to set the time interval. For example, we set the interval to 1 hour in the image below. Note that, similar to plotting the spatial distribution, the filter function can also be used to filter the stations to be displayed. In the animation below, we demonstrate how to display the monitoring values of the Kaohsiung PM2.5 station throughout the day.\nTableau Example 2: Disaster Notification Dashboard Dashboards can combine different worksheets (charts) to present richer information and allow users to interact with thedata. In the following example, we will introduce how to create a simple dashboard using the rainstorm disaster data provided by the Civil IoT Taiwan project.\nDisaster data format conversion We use the 823 flood that occurred on 2018/08/23 as an example. Since the original data format is an xml file, we first use the following URL to convert the original data to a csv file: https://www.convertcsv.com/xml-to-csv.htm (animated below)\nDashboard size After clicking Dashboard in the Tableau menu, you can set the size of the dashboard in the tool list on the left.\nAdd worksheets to a dashboard Then you can see the previously created worksheet in the tool list on the left, you can add it to the dashboard by dragging and dropping the worksheet to the empty space of the dashboard.\nAfter Tableau receives the data of the worksheet, it will automatically read the information of the content and automatically generate the corresponding initial chart.\nThe worksheet you just dragged in has a fixed size and cannot be changed at will. There is a downward arrow at the top right of the worksheet, and you can change the size of the graph by clicking and selecting “Float”.\nAdd interactions Next, we demonstrate how to provide an interactive interface that allows users to select the point in time in the disaster data that they want to observe.\nWe start by clicking the down arrow in the upper right corner of the sheet, then selecting Filters, then Case Time.\nAfter clicking, you will see a selection field pop up in the worksheet where you can check the date. At this time, the user only needs to check the date they want to observe, and then they can see the disaster situation on that day.\nIf we want to change the date selection method, we can also click the down key at the top right of the selection field to change the style of the selection field.\nFor example, we can change the selection method from the original list method to the slider method as follows.\nInteracting multiple worksheet The above interaction buttons can only be used for one worksheet. If you want multiple worksheets on the dashboard to share the same interactive button at the same time, you can refer to the following methods:\nAccording to the above method, first create an interactive button field;\nClick the down button at the top right of the interactive button field, select Apply to worksheet and click the selected worksheet;\nSelect the worksheet to apply to complete the setting.\nAdditional information If you need to add other information on the dashboard, you can find an object column at the bottom left of the toolbar, which contains text, pictures and other objects. Just drag and drop the required objects to the top of the dashboard to display them in the dashboard, and then add text, pictures and other objects.\nStory A story is a combination of multiple dashboards or sheets used to create a slideshow-like display. Adding a story to a dashboard or sheet is the same as adding a sheet from a dashboard, just drag the sheet or dashboard created on the right to the empty space of the story.\nIf you need to add a new story page, you can click New Story at the top to add a new blank page, or copy an existing page to become a new page.\nFinally, to make the text information better fit its content, we can modify the title of the page by double-clicking the box above the text page.\nConclusion In this chapter, we briefly introduce the basic operations of Tableau, and use Tableau to design presentations/charts that interact with users. However, Tableau’s functions are far more than these, and there are many classic examples created using Tableau on the Internet. If you are interested in Tableau, you can refer to additional resources below.\nReferences Tableau Public (https://public.tableau.com/) Tableau - About data field roles and types (https://help.tableau.com/current/pro/desktop/en-us/datafields_typesandroles.htm) Get Started with Tableau (https://help.tableau.com/current/guides/get-started-tutorial/en-us/get-started-tutorial-home.htm) Tableau Tutorial — Learn Data Visualization Using Tableau (https://medium.com/edureka/tableau-tutorial-71ef4c122e55) YouTube: Tableau For Data Science 2022 (https://www.youtube.com/watch?v=Wh4sCCZjOwo) YouTube: Tableau Online Training (https://www.youtube.com/watch?v=ttCDqyfrcEc) ",
    "description": "We introduce the use of Tableau tools to render Civil IoT Taiwan data and conduct two example cases using air quality data and disaster notification data. We demonstrate how worksheets, dashboards, and stories can be used to create interactive data visualizations for users to explore data. We also provide a wealth of reference materials for people to further study reference.",
    "tags": [
      "Air",
      "Disaster"
    ],
    "title": "7.2. Tableau Application",
    "uri": "/en/ch7/ch7.2/"
  },
  {
    "content": "\nTable Of Contents Package Installation and Importing Initialization and Data Access Data Preprocessing Calibration Model Establishment and Validation Best Calibration Model of the Day Calibration Results References In this article, we will take the air quality data in Civil IoT Taiwan as an example to introduce how to allow two different levels of air quality sensing data to be systematically and dynamically calibrated through data science methods. At the same time, with the results of data fusion, different deployment projects can work together to create a more comprehensive air quality sensing result. We use the following two air quality sensing systems:\nEnvironmental Protection Agency Air Quality Monitoring Station: In traditional air quality monitoring, extremely professional, large-scale and high-cost monitoring stations are mainly used. It cannot be deployed in every community due to high deployment and maintenance costs. Instead, local environmental agencies will select specific locations for deployment and maintenance. According to the announcement on the website of the Taiwan Environmental Protection Agency, as of July 2022, the number of official monitoring stations in Taiwan is 81. Micro Air Quality Sensor: Compared with the traditional professional station, the micro air quality sensor adopts low-cost sensors, transmits data through the network, and builds a denser air quality sensor in the way of the Internet of Things network. This technology not only has low erection cost, but also provides more flexible installation conditions and expanded coverage. At the same time, this technology has the characteristics of convenient installation and maintenance, which satisfies the conditions of large-scale real-time air quality monitoring system. It can achieve the data frequency of uploading data once a minute, and also allows users to respond immediately to sudden pollution incidents, further reducing the loss. Of course, we cannot expect low-cost micro air quality sensors to have the high accuracy of professional monitoring stations. How to improve its accuracy becomes another problem to be solved. Therefore, in the following content, we will demonstrate how to use data science approaches to adjust the sensing results of the micro air quality sensor, so that the accuracy of the sensing data reaches the level of EPA air quality monitoring station, thereby facilitating system integration and further data application.\nPackage Installation and Importing In this article, we will use the pandas, numpy, datetime, sklearn, scipy, and joblib packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. Thus, we can directly use the following syntax to import the relevant packages to complete the preparations in this article.\nimport pandas as pd import numpy as np from datetime import datetime, timedelta from sklearn import linear_model, svm, tree from sklearn import metrics as sk_metrics from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split, cross_validate from sklearn.feature_selection import f_regression import scipy.stats as stats import joblib Initialization and Data Access In this example, we will use the Wanhua station of the EPA air quality station (wanhua), and two airboxes of Academia Sinica - Micro Air Quality Sensors in Civil IoT Taiwan, which are co-located with the EPA Wanhua station (the device IDs are 08BEAC028A52 and 08BEAC028690 respectively) as examples, and evaluate the use of three training models, i.e., Linear Regression, Random Forest Regression and Support Vector Regression (SVR). We will consider the PM2.5 concentration, temperature, relative humidity, and timestamp features of the data, combined with historical data for three different lengths of time (3 days, 5 days, 8 days), and conduct a series of explorations.\nFor convenience, we first make the following initial program setup based on these assumptions.\nSITE= \"wanhua\" EPA= \"EPA-Wanhua\" AIRBOXS= ['08BEAC028A52', '08BEAC028690'] DAYS= [8, 5, 3] METHODS= ['LinearRegression', 'RandomForestRegressor', 'SVR'] METHOD_SW= { 'LinearRegression':'LinearR', 'RandomForestRegressor':'RFR', 'SVR':'SVR' } METHOD_FUNTION= {'LinearRegression':linear_model.LinearRegression(), 'RandomForestRegressor': RandomForestRegressor(n_estimators= 300, random_state= 36), 'SVR': svm.SVR(C=20) } FIELD_SW= {'s_d0':'PM25', 'pm25':'PM25', 'PM2_5':\"PM25\", 'pm2_5':\"PM25\", 's_h0':\"HUM\", 's_t0':'TEM'} FEATURES_METHOD= {'PHTR':[\"PM25\", \"HR\", \"TEM\", \"HUM\"], 'PH':['PM25','HR'], 'PT':['PM25','TEM'], 'PR':['PM25', 'HUM'], 'P':['PM25'], 'PHR':[\"PM25\", \"HR\", \"HUM\"], 'PTR':[\"PM25\", \"TEM\", \"HUM\"], 'PHT':[\"PM25\", \"HR\", \"TEM\"] } Next we consider how much data needs to be downloaded for system calibration and data fusion. As shown in the figure below, assuming that we want to obtain the calibration model of the Nth day, the data of (N-1)’s day will be used as test data to evaluate the accuracy of the calibration model. So if we set the training data to be X days, then the historical data representing days N - 2 to N - (2+X) days will be used as training data. In our scenario, we set N to the current time (today) and the maximum possible X value is 8, so we need to prepare a total of ten days of historical data for the next operation.\nFor later use, we first specify the date to calibrate the model (Nth day: TODAY), the date of the test data (N-1 th day: TESTDATE), and the end date of the training data (N-2 th day: ENDDATE) with the following code .\nTODAY = datetime.today() TESTDATE = (TODAY - timedelta(days=1)).date() ENDDATE = (TODAY - timedelta(days=2)).date() In this example, we use the data access API provided by Academia Sinica - Micro Air Quality Sensors. According to the specified date and device code , the sensing data of the corresponding date and device can be downloaded in CSV file format. Note that due to the limitations of the data access API, the specified date can only be a date within the past 30 days from the date of download.\nhttps://pm25.lass-net.org/data/history-date.php?device_id=\u003cID\u003e\u0026date=\u003cYYY-MM-DD\u003e\u0026format=CSV For example, suppose we want to download the sensing data of EPA Wanhua Station on September 21, 2022, we can download it in the following ways:\nhttps://pm25.lass-net.org/data/history-date.php?device_id=EPA-Wanhua\u0026date=2022-09-21\u0026format=CSV Using this method, we write a Python function getDF, which can download the sensing data of the past 10 days for the given device code, and return the collected data in the form of a single DataFrame object.\ndef getDF(id): temp_list = [] for i in range(1,11): date = (TODAY - timedelta(days=i)).strftime(\"%Y-%m-%d\") URL = \"https://pm25.lass-net.org/data/history-date.php?device_id=\" + id + \"\u0026date=\" + date + \"\u0026format=CSV\" temp_DF = pd.read_csv( URL, index_col=0 ) temp_list.append( temp_DF ) print(\"ID: {id}, Date: {date}, Shape: {shape}\".format(id=id, date=date, shape=temp_DF.shape)) All_DF = pd.concat( temp_list ) return All_DF Next, we download the sensing data of the first airbox installed at the EPA Wanhua station and store it in the AirBox1_DF object:\n# First AirBox device AirBox1_DF = getDF(AIRBOXS[0]) AirBox1_DF.head() ID: 08BEAC028A52, Date: 2022-09-29, Shape: (208, 19) ID: 08BEAC028A52, Date: 2022-09-28, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-27, Shape: (225, 19) ID: 08BEAC028A52, Date: 2022-09-26, Shape: (230, 19) ID: 08BEAC028A52, Date: 2022-09-25, Shape: (231, 19) ID: 08BEAC028A52, Date: 2022-09-24, Shape: (232, 19) ID: 08BEAC028A52, Date: 2022-09-23, Shape: (223, 19) ID: 08BEAC028A52, Date: 2022-09-22, Shape: (220, 19) ID: 08BEAC028A52, Date: 2022-09-21, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-20, Shape: (215, 19) Using the same method, we download the sensing data of the second airbox installed at the EPA Wanhua station and the sensing data of the EPA Wanhua station, and stored them in the AirBox2_DF and EPA_DF objects, respectively.\n# Second AirBox device AirBox2_DF = getDF(AIRBOXS[1]) # EPA station EPA_DF = getDF(EPA) Data Preprocessing Since our current data contains many unneeded fields, in order to avoid taking up too much memory space, we first simplify the data we use, leaving only what is needed.\nCol_need = [\"timestamp\", \"s_d0\", \"s_t0\", \"s_h0\"] AirBox1_DF_need = AirBox1_DF[Col_need] print(AirBox1_DF_need.head()) AirBox2_DF_need = AirBox2_DF[Col_need] print(AirBox2_DF_need.head()) Col_need = [\"time\", \"date\", \"pm2_5\"] EPA_DF_need = EPA_DF[Col_need] print(EPA_DF_need.head()) del AirBox1_DF del AirBox2_DF del EPA_DF timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:03:28Z 9.0 29.75 71.0 1 2022-09-30T00:33:46Z 11.0 31.36 67.0 2 2022-09-30T00:39:51Z 10.0 31.50 67.0 3 2022-09-30T00:45:58Z 12.0 31.50 66.0 4 2022-09-30T00:52:05Z 12.0 31.86 66.0 timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:00:31Z 9.0 29.36 -53.0 1 2022-09-30T00:07:17Z 9.0 29.50 -52.0 2 2022-09-30T00:23:47Z 10.0 30.25 -45.0 3 2022-09-30T00:34:24Z 10.0 31.11 -36.0 4 2022-09-30T00:40:31Z 11.0 31.25 -35.0 time date pm2_5 index 0 00:00:00 2022-09-30 9.0 1 01:00:00 2022-09-30 10.0 2 02:00:00 2022-09-30 16.0 3 03:00:00 2022-09-30 19.0 4 04:00:00 2022-09-30 20.0 Next, in order to unify the data fields, we merge the original date and time fields of the EPA station data to generate a new timestamp field.\nEPA_DF_need['timestamp'] = pd.to_datetime( EPA_DF_need[\"date\"] + \"T\" + EPA_DF_need[\"time\"], utc=True ) print(EPA_DF_need.head()) time date pm2_5 timestamp index 0 00:00:00 2022-09-30 9.0 2022-09-30 00:00:00+00:00 1 01:00:00 2022-09-30 10.0 2022-09-30 01:00:00+00:00 2 02:00:00 2022-09-30 16.0 2022-09-30 02:00:00+00:00 3 03:00:00 2022-09-30 19.0 2022-09-30 03:00:00+00:00 4 04:00:00 2022-09-30 20.0 2022-09-30 04:00:00+00:00 Due to the different temporal resolutions of the airbox and EPA station data, to align the data on both sides, we resample the airbox data from the original every five minutes to every hour.\ndef getHourly(DF): DF = DF.set_index( pd.DatetimeIndex(DF[\"timestamp\"]) ) DF_Hourly = DF.resample('H').mean() DF_Hourly.reset_index(inplace=True) return DF_Hourly AirBox1_DF_need_Hourly = getHourly( AirBox1_DF_need) AirBox2_DF_need_Hourly = getHourly( AirBox2_DF_need) EPA_DF_need_Hourly = getHourly( EPA_DF_need) # 可省略，原始數據已經是小時平均 del AirBox1_DF_need del AirBox2_DF_need del EPA_DF_need print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp s_d0 s_t0 s_h0 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp pm2_5 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 We also replace the data fields from both sources with easily identifiable field names for easy follow-up and identification.\nCol_rename = {\"s_d0\":\"PM25\", \"s_h0\":\"HUM\", \"s_t0\":\"TEM\"} AirBox1_DF_need_Hourly.rename(columns=Col_rename, inplace=True) AirBox2_DF_need_Hourly.rename(columns=Col_rename, inplace=True) Col_rename = {\"pm2_5\":\"EPA_PM25\"} EPA_DF_need_Hourly.rename(columns=Col_rename, inplace=True) print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp PM25 TEM HUM 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp EPA_PM25 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 Since the two airboxes have the same hardware and the same location, we treat them as the same data source and combine the data of the two airboxes to generate the AirBoxs_DF object. We then intersect the new data object with the EPA air quality station data according to the time field, producing a merged ALL_DF object.\nAirBoxs_DF = pd.concat([AirBox1_DF_need_Hourly, AirBox2_DF_need_Hourly]).reset_index(drop=True) All_DF = pd.merge( AirBoxs_DF, EPA_DF_need_Hourly, on=[\"timestamp\"], how=\"inner\" ) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 NaN 5 2022-09-21 02:00:00+00:00 9.000000 30.418889 -59.222222 NaN 6 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 7 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 8 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 9 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 We first exclude the presence of null values (NaN) in the data.\nAll_DF.dropna(how=\"any\", inplace=True) All_DF.reset_index(inplace=True, drop=True) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 Finally, since the daily hour value information will be used when building the model, we add an HR field to All_DF that contains the content of the hour value in the timestamp.\ndef return_HR(row): row['HR'] = int(row[ \"timestamp\" ].hour) return row All_DF = All_DF.apply(return_HR , axis=1) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 HR 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 1 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 1 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 3 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 3 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 4 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 4 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 5 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 5 Calibration Model Establishment and Validation After completing the data preparation, we next start building candidate calibration models. Since we discussed a total of 3 regression methods, 3 historical data lengths, and 8 feature combinations in this case, we will generate a total of 3 x 3 x 8 = 72 candidate models.\nFirst, we designed the SlideDay function to retrieve a specific length of historical data when building a calibration model. The function returns the input data Hourly_DF from enddate to the total day day data according to the input historical data length day.\ndef SlideDay( Hourly_DF, day, enddate ): startdate= enddate- timedelta( days= (day-1) ) time_mask= Hourly_DF[\"timestamp\"].between( pd.Timestamp(startdate, tz='utc'), pd.Timestamp(enddate, tz='utc') ) return Hourly_DF[ time_mask ] Next, we organize the sensing data Training_DF of the site station from enddate to day days before, and organize it into training data according to the feature feature combination. Then we apply the method regression method, so that the predicted value produced by the generated calibration model can approximate the data value of the EPA_PM25 field in the training data.\nWe calculate both the Mean Absolute Error (MAE) and the Mean Squared Error (MSE) of the corrected model itself. Among them, the mean absolute error (MAE) is the sum of the absolute values of the difference between the target value and the predicted value, which may reflect the actual situation of the predicted value error. The smaller the value, the better the performance. The mean squared error (MSE) is the mean of the square of the difference between the predicted value and the actual observed value. The smaller the MSE value, the better the accuracy of the prediction model in describing the experimental data.\ndef BuildModel( site, enddate, feature, day, method, Training_DF ): X_train = Training_DF[ FEATURES_METHOD[ feature ] ] Y_train = Training_DF[ \"EPA_PM25\" ] model_result = {} model_result[\"site\"], model_result[\"day\"], model_result[\"feature\"], model_result[\"method\"] = site, day, feature, method model_result[\"datapoints\"], model_result[\"modelname\"] = X_train.shape[0], (site + \"_\" + str(day) + \"_\" + METHOD_SW[method] + \"_\" + feature) model_result[\"date\"] = enddate.strftime( \"%Y-%m-%d\" ) # add timestamp field Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) model_result['create_timestamp_utc'] = Now_Time ### training model ### print( \"[BuildR]-\\\"{method}\\\" with {day}/{feature}\".format(method=method, day=day, feature=feature) ) # fit lm = METHOD_FUNTION[ method ] lm.fit( X_train, Y_train ) # get score Y_pred = lm.predict( X_train ) model_result['Train_MSE'] = MSE = sk_metrics.mean_squared_error( Y_train, Y_pred ) model_result['Train_MAE'] = sk_metrics.mean_absolute_error( Y_train, Y_pred ) return model_result, lm In addition to evaluating the MAE and MSE of the training data when building the calibrated model, we also consider the prediction performance of the calibrated model on non-training data. Therefore, for the model lm, according to the feature data feature that it needs to bring in, as well as the test data, we generate the predicted value, and compare it with the EPA_PM25 field in the test data to calculate the MAE and MSE respectively. , as a reference for subsequent evaluation of the applicability of different calibration models.\ndef TestModel( site, feature, modelname, Testing_DF, lm ): X_test = Testing_DF[ FEATURES_METHOD[ feature ] ] Y_test = Testing_DF[ \"EPA_PM25\" ] # add timestamp field Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) ### testing model ### # predict Y_pred = lm.predict( X_test ) # get score test_result = {} test_result[\"test_MSE\"] = round( sk_metrics.mean_squared_error( Y_test, Y_pred ), 3) test_result[\"test_MAE\"] = round( sk_metrics.mean_absolute_error( Y_test, Y_pred ), 3) return test_result Finally, we integrate the just completed SlideDay, BuildModel and TestModel, and complete a total of 72 calibration models. For each calibrated model, we compute MAE and MSE for training and testing data separately, and store all results in an AllResult_DF object.\nAllResult_list = [] for day in DAYS: for method in METHODS: for feature in FEATURES_METHOD: Training_DF = SlideDay(All_DF, day, ENDDATE)[ FEATURES_METHOD[feature] + [\"EPA_PM25\"] ] result, lm = BuildModel( SITE, TESTDATE, feature, day, method, Training_DF ) test_result = TestModel(SITE, feature, result[\"modelname\"], SlideDay(All_DF, 1, TESTDATE), lm) R_DF = pd.DataFrame.from_dict( [{ **result, **test_result }] ) AllResult_list.append( R_DF ) AllResult_DF = pd.concat(AllResult_list) AllResult_DF.head() [BuildR]-\"LinearRegression\" with 8/PHTR [BuildR]-\"LinearRegression\" with 8/PH [BuildR]-\"LinearRegression\" with 8/PT [BuildR]-\"LinearRegression\" with 8/PR [BuildR]-\"LinearRegression\" with 8/P [BuildR]-\"LinearRegression\" with 8/PHR [BuildR]-\"LinearRegression\" with 8/PTR [BuildR]-\"LinearRegression\" with 8/PHT [BuildR]-\"RandomForestRegressor\" with 8/PHTR [BuildR]-\"RandomForestRegressor\" with 8/PH [BuildR]-\"RandomForestRegressor\" with 8/PT [BuildR]-\"RandomForestRegressor\" with 8/PR [BuildR]-\"RandomForestRegressor\" with 8/P [BuildR]-\"RandomForestRegressor\" with 8/PHR [BuildR]-\"RandomForestRegressor\" with 8/PTR [BuildR]-\"RandomForestRegressor\" with 8/PHT [BuildR]-\"SVR\" with 8/PHTR [BuildR]-\"SVR\" with 8/PH [BuildR]-\"SVR\" with 8/PT [BuildR]-\"SVR\" with 8/PR [BuildR]-\"SVR\" with 8/P [BuildR]-\"SVR\" with 8/PHR [BuildR]-\"SVR\" with 8/PTR [BuildR]-\"SVR\" with 8/PHT [BuildR]-\"LinearRegression\" with 5/PHTR [BuildR]-\"LinearRegression\" with 5/PH [BuildR]-\"LinearRegression\" with 5/PT [BuildR]-\"LinearRegression\" with 5/PR [BuildR]-\"LinearRegression\" with 5/P [BuildR]-\"LinearRegression\" with 5/PHR [BuildR]-\"LinearRegression\" with 5/PTR [BuildR]-\"LinearRegression\" with 5/PHT [BuildR]-\"RandomForestRegressor\" with 5/PHTR [BuildR]-\"RandomForestRegressor\" with 5/PH [BuildR]-\"RandomForestRegressor\" with 5/PT [BuildR]-\"RandomForestRegressor\" with 5/PR [BuildR]-\"RandomForestRegressor\" with 5/P [BuildR]-\"RandomForestRegressor\" with 5/PHR [BuildR]-\"RandomForestRegressor\" with 5/PTR [BuildR]-\"RandomForestRegressor\" with 5/PHT [BuildR]-\"SVR\" with 5/PHTR [BuildR]-\"SVR\" with 5/PH [BuildR]-\"SVR\" with 5/PT [BuildR]-\"SVR\" with 5/PR [BuildR]-\"SVR\" with 5/P [BuildR]-\"SVR\" with 5/PHR [BuildR]-\"SVR\" with 5/PTR [BuildR]-\"SVR\" with 5/PHT [BuildR]-\"LinearRegression\" with 3/PHTR [BuildR]-\"LinearRegression\" with 3/PH [BuildR]-\"LinearRegression\" with 3/PT [BuildR]-\"LinearRegression\" with 3/PR [BuildR]-\"LinearRegression\" with 3/P [BuildR]-\"LinearRegression\" with 3/PHR [BuildR]-\"LinearRegression\" with 3/PTR [BuildR]-\"LinearRegression\" with 3/PHT [BuildR]-\"RandomForestRegressor\" with 3/PHTR [BuildR]-\"RandomForestRegressor\" with 3/PH [BuildR]-\"RandomForestRegressor\" with 3/PT [BuildR]-\"RandomForestRegressor\" with 3/PR [BuildR]-\"RandomForestRegressor\" with 3/P [BuildR]-\"RandomForestRegressor\" with 3/PHR [BuildR]-\"RandomForestRegressor\" with 3/PTR [BuildR]-\"RandomForestRegressor\" with 3/PHT [BuildR]-\"SVR\" with 3/PHTR [BuildR]-\"SVR\" with 3/PH [BuildR]-\"SVR\" with 3/PT [BuildR]-\"SVR\" with 3/PR [BuildR]-\"SVR\" with 3/P [BuildR]-\"SVR\" with 3/PHR [BuildR]-\"SVR\" with 3/PTR [BuildR]-\"SVR\" with 3/PHT Best Calibration Model of the Day When discussing the “best calibrated model of the day”, it must be admitted that the so-called “best model” is only available after the end of the day. That is to say, the MAE and MSE of different candidate models can only be obtained after collecting complete 24-hour data, and the best model can be obtained after comparative analysis. Therefore, it will not be possible to systematically generate a truly optimized calibration model before the end of today’s 24 hours.\nHowever, based on practical needs, we often need to have a calibration model that can be applied in the data production process. So, in practice, we usually assume that “yesterday’s best model will also perform well today” and use that as a way to correct that day’s data. For example, assuming we use MSE as the criterion for deciding the best model, we can get information about the best model using the following syntax:\nFIELD= \"test_MSE\" BEST= AllResult_DF[ AllResult_DF[FIELD]== AllResult_DF[FIELD].min() ] BEST Next, to adapt to today’s situation, we use yesterday’s best calibrated model parameters (historical data length, regression method, feature data combination), recalculate training and test data from today’s date range, and regenerate today’s calibrated model .\nBEST_DC= BEST.to_dict(orient=\"index\")[0] Training_DF= SlideDay(All_DF, BEST_DC[\"day\"], TESTDATE)[ FEATURES_METHOD[BEST_DC[\"feature\"]]+ [\"EPA_PM25\"] ] result, lm= BuildModel( SITE, TODAY, BEST_DC[\"feature\"], BEST_DC[\"day\"], BEST_DC[\"method\"], Training_DF ) result [BuildR]-\"SVR\" with 3/PHT {'site': 'wanhua', 'day': 3, 'feature': 'PHT', 'method': 'SVR', 'datapoints': 80, 'modelname': 'wanhua_3_SVR_PHT', 'date': '2022-09-30', 'create_timestamp_utc': '2022-09-30 11:19:48', 'Train_MSE': 3.91517342356589, 'Train_MAE': 1.42125724796098} From this example, we can see that the newly produced calibration model has a Train_MSE of 3.915, which is much higher than the original 2.179. However, since we have no way to know the real best model of the day before the end of the day, we can only apply yesterday’s best model if there is no other better choice. Finally, we export the .joblib file with the following syntax and release the model for sharing (please refer to the reference materials for details).\nmodel_dumpname= result[\"modelname\"]+ \".joblib\" MODEL_OUTPUT_PATH= \"\" try: joblib.dump( lm, MODEL_OUTPUT_PATH+ model_dumpname ) print( \"[BuildR]-dump {}\".format( MODEL_OUTPUT_PATH+model_dumpname ) ) except Exceptionas e: print( \"ERROR! [dump model] {}\".format( result[\"modelname\"] ) ) error_msg(e) [BuildR]-dump wanhua_3_SVR_PHT.joblib Calibration Results The joint calibration method described in this article has been officially applied to “Academia Sinica - Micro Air Quality Sensors” in Civil IoT Taiwan from 2020/5, and the daily output calibration model has been published on the Dynamic Calibration Model website. In the implementation, a total of 31 EPA air quality monitoring stations were selected to install two airboxes, and we considered 3 historical data lengths, 8 data feature combinations, and 7 regression methods (i.e. a total of 3 x 8 x 7 = 168 combinations) to generate the best daily calibration model for each of these 31 sites. Then, for the sensing data of each airbox, the best calibration model of the station closest to its geographical location is used as the application model of its data calibration, and its sensing data is generated and published. From the observation of the actual operation after the mechanism is launched, the data difference between the micro air quality sensor device and the EPA air quality monitoring station can be effectively reduced (as shown in the figure below). This achievement also established a good cooperation model for cross-system data integration of air quality monitoring.\nReferences Dynamic Calibration Model Status Report (https://pm25.lass-net.org/DCF/) scikit-learn: machine learning in Python (https://scikit-learn.org/stable/) Joblib: running Python functions as pipeline jobs (https://joblib.readthedocs.io/) Jason Brownlee, Save and Load Machine Learning Models in Python with scikit-learn, Machine Learning Mastery (https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/) ",
    "description": "We use air quality category data of the Civil IoT Taiwan project to demonstrate the dynamic calibration algorithm for Taiwanese micro air quality sensors and official monitoring stations. In a learning-by-doing way, from data preparation, feature extraction, to machine learning, data analysis, statistics and induction, the principle and implementation process of the multi-source sensor dynamic calibration model algorithm are reproduced step by step, allowing readers to experience how to gradually realize by superimposing basic data analysis and machine learning steps to achieve advanced and practical data application services.",
    "tags": [
      "Python",
      "Air"
    ],
    "title": "6.3. Joint Data Calibration",
    "uri": "/en/ch6/ch6.3/"
  },
  {
    "content": "\nTable Of Contents Package Installation and Importing Data Access Leafmap Basics Basic Data Presentation Cluster Data Presentation Change Leafmap Basemap Integrate OSM Resources Heatmap Presentation Split Window Presentation Leafmap for Web Applications Conclusion References In the previous chapters, we have demonstrated how to use programming languages to analyze data on geographic attributes, and we have also demonstrated how to use GIS software for simple geographic data analysis and presentation. Next, we will introduce how to use the Leafmap suite in Python language for GIS applications, and the Streamlit suite for website development. Finally, we will combine Leafmap and Streamlit to make a simple web GIS system by ourselves, and present the results of data processing and analysis through web pages.\nPackage Installation and Importing In this chapter, we will use packages such as pandas, geopandas, leafmap, ipyleaflet, osmnx, streamlit, geocoder, and pyCIOT. Apart from pandas, our development platform Google Colab does not provide these packages, so we need to install them ourselves first. Since there are many packages installed this time, in order to avoid a large amount of information output after the command is executed, we have added the ‘-q’ parameter to each installation command, which can make the output of the screen more concise.\n!pip install -q geopandas !pip install -q leafmap !pip install -q ipyleaflet !pip install -q osmnx !pip install -q streamlit !pip install -q geocoder !pip install -q pyCIOT After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport pandas as pd import geopandas as gpd import leafmap import ipyleaflet import osmnx import geocoder import streamlit from pyCIOT.data import * Data Access In the examples in this article, we use several datasets on the Civil IoT Taiwan Data Service Platform, including air quality data from the EPA, and seismic monitoring station measurements from the National Earthquake Engineering Research Center and the Central Weather Bureau.\nFor the EPA air quality data, we use the pyCIOT package to obtain the latest measurement results of all EPA air quality stations, and convert the resulting JSON format data into a DataFrame through the json_normalize() method in the pandas package format. We only keep the station name, latitude, longitude and ozone (O3) concentration information for future use. The code for this part of data collection and processing is as follows:\nepa_station = Air().get_data(src=\"OBS:EPA\") df_air = pd.json_normalize(epa_station) df_air['O3'] = 0 for index, row in df_air.iterrows(): sensors = row['data'] for sensor in sensors: if sensor['name'] == 'O3': df_air.at[index, 'O3'] = sensor['values'][0]['value'] df_air = df_air[['name','location.latitude','location.longitude','O3']] df_air Then we extract the seismic monitoring station data from the National Earthquake Engineering Research Center and the Central Weather Bureau in a similar way, leaving only the station name, longitude and latitude information for subsequent operations. The code for this part of data collection and processing is as follows:\nquake_station = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") df_quake = pd.json_normalize(quake_station) df_quake = df_quake[['name','location.latitude','location.longitude']] df_quake We have successfully demonstrated the reading examples of air quality data (air) and seismic data (quake). In the following discussion, we will use these data for the operation and application using the leafmap suite. The same methods can also be easily applied to other datasets on the Civil IoT Taiwan Data Service Platform. You are encouraged to try it yourself.\nLeafmap Basics Basic Data Presentation Using the data df_air and seismic data df_quake that we are currently processing, we first convert the format of these two data from the DataFrame format provided by the pandas package to the GeoDataFrame format provided by the geopandas package which supports geographic information attributes. We then use Leafmap’sadd_gdf() method to create a presentation layer for each dataset and add them to the map in one go.\ngdf_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') gdf_quake = gpd.GeoDataFrame(df_quake, geometry=gpd.points_from_xy(df_quake['location.longitude'], df_quake['location.latitude']), crs='epsg:4326') m1 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m1.add_gdf(gdf_air, layer_name=\"EPA Station\") m1.add_gdf(gdf_quake, layer_name=\"Quake Station\") m1 From the map output by the program, we can see the names of the two data in the upper right corner of the map, which have been added to the map in the form of two layers. Users can click the layer to be queried to browse according to their own needs. However, when we want to browse the data of two layers at the same time, we will find that both layers are rendering the same icon, so there will be confusion on the map.\nTo solve this problem, we introduce another way of data presentation. We use the GeoData layer data format provided by the ipyleaflet suite and add the GeoData layer to the map using leafmap’s add_layer() method. For easy identification, we use the small blue circle icon to represent the data of the empty station, and the small red circle icon to represent the data of the seismic station.\ngeo_data_air = ipyleaflet.GeoData( geo_dataframe=gdf_air, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'blue', 'weight': 3}, name=\"EPA stations\", ) geo_data_quake = ipyleaflet.GeoData( geo_dataframe=gdf_quake, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'red', 'weight': 3}, name=\"Quake stations\", ) m2 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m2.add_layer(geo_data_air) m2.add_layer(geo_data_quake) m2 Cluster Data Presentation In some data applications, when there are too many data points on the map, it is not easy to observe. If this is the case, we can use clustering to present the data. i.e. when there are too many data points for a small area, we cluster the points together to show the number of points. When the user zooms in on the map, these originally clustered points are slowly pulled apart. When there is only one point left in a small area, the information of that point can be directly seen.\nLet’s take data from seismic stations as an example. Using leafmap’s add_points_from_xy() method, the data of df2 can be placed on the map in a clustered manner.\nm3 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m3.add_points_from_xy(data=df_quake, x = 'location.longitude', y = 'location.latitude', layer_name=\"Quake Station\") m3 Change Leafmap Basemap Leafmap uses OpenStreetMap as the default basemap and provides over 100 other basemap options. Users can change the basemap according to their own preferences and needs. You can use the following syntax to learn which basemaps currently supported by leafmap:\nlayers = list(leafmap.basemaps.keys()) layers We select SATELLITE and Stamen.Terrain from these basemaps as demonstrations, and use the add_basemap() method of the leafmap package to add the basemap as a new layer. After adding, the leafmap preset will open all layers and stack them in the order of addition. You can select the layer you want to use through the layer menu in the upper right corner.\nm4 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m4.add_gdf(gdf_air, layer_name=\"EPA Station\") m4.add_basemap(\"SATELLITE\") m4.add_basemap(\"Stamen.Terrain\") m4 In addition to using the basemap provided by leafmap, you can also use Google Map’s XYZ Tiles service to add layers of Google satellite imagery. The methods are as follows:\nm4.add_tile_layer( url=\"https://mt1.google.com/vt/lyrs=y\u0026x={x}\u0026y={y}\u0026z={z}\", name=\"Google Satellite\", attribution=\"Google\", ) m4 Integrate OSM Resources In addition to some built-in resources, Leafmap also integrates many external geographic information resources. Among them, OSM (OpenStreetMap) is a well-known and rich open source geographic information resource. Various resources provided by OSM can be found on the OSM website, with a complete list of properties.\nIn the following example, we use the add_osm_from_geocode() method provided by the leafmap package to demonstrate how to get the outline of a city and render it on the map. Taking Taichung City as an example, combined with the location information in the EPA air quality monitoring station data, we can clearly see which stations are in Taichung City.\ncity_name = \"Taichung, Taiwan\" m5 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m5.add_layer(geo_data_air) m5.add_osm_from_geocode(city_name, layer_name=city_name) m5 Then we continue to use the add_osm_from_place() method provided by the leafmap package to further search for specific facilities in Taichung City and add them to the map layer. The following procedure takes factory facilities as an example and uses the land use data of OSM to find out the relevant factory locations and areas in Taichung City, which can be analyzed and explained in combination with the locations of EPA air quality monitoring stations. For more types of OSM facilities, you can refer to the complete properties list.\nm5.add_osm_from_place(city_name, tags={\"landuse\": \"industrial\"}, layer_name=city_name+\": Industrial\") m5 In addition, the leafmap package also provides a method for searching for OSM nearby facilities centered on a specific location, providing a very convenient function for analyzing and interpreting data. For example, in the following example, we use the add_osm_from_address() method to search for related religious facilities (attribute “amenity”: “place_of_worship”) within a 1,000-meter radius of Qingshui Station, Taichung; at the same time, we use the add_osm_from_point() method to search for relevant school facilities (attributes “amenity”: “school”) within 1,000 meters of the GPS coordinates (24.26365, 120.56917) of Taichung Qingshui Station. Finally, we overlay the results of these two queries on the existing map with different layers.\nm5.add_osm_from_address( address=\"Qingshui Station, Taichung\", tags={\"amenity\": \"place_of_worship\"}, dist=1000, layer_name=\"Shalu worship\" ) m5.add_osm_from_point( center_point=(24.26365, 120.56917), tags={\"amenity\": \"school\"}, dist=1000, layer_name=\"Shalu schools\" ) m5 Heatmap Presentation A heatmap is a two-dimensional representation of event intensity through color changes. When matching a heatmap to a map, the state of event intensity can be expressed at different scales depending on the scale of the map used. It is a very common and powerful data representation tool. However, when drawing a heatmap, the user must confirm that the characteristics of the data are suitable for presentation by a heatmap, otherwise it is easy to be confused with the graphical data interpolation representations such as IDW and Kriging that we introduced in Chap 5. For example, we take the O3 concentration data of the EPA air quality data as an example, and draw the corresponding heat map as follows:\nm6 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m6.add_layer(geo_data_air) m6.add_heatmap( df_air, latitude='location.latitude', longitude='location.longitude', value=\"O3\", name=\"O3 Heat map\", radius=100, ) m6 There is nothing obvious about this image at first glance, but if we zoom in on the Taichung city area, we can see that the appearance of the heatmap has changed a lot, showing completely different results at different scales.\nThe above example is actually an example of misuse of the heatmap, because the O3 concentration data reflects the local O3 concentration. Due to the change of the map scale, its values cannot be directly accumulated or distributed to adjacent areas. Therefore, the O3 concentration data used in the example is not suitable for heatmap representation and should be plotted using the geographic interpolation method described in Chapter 5.\nTo show the real effect of the heatmap, we use the location data of the seismic stations instead and add a field num with a default value of 10. Then we use the code below to generate a heatmap of the status of Taiwan Seismic Monitoring Stations.\ndf_quake['num'] = 10 m7 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m7.add_layer(geo_data_quake) m7.add_heatmap( df_quake, latitude='location.latitude', longitude='location.longitude', value=\"num\", name=\"Number of Quake stations\", radius=200, ) m7 Split Window Presentation In the process of data analysis and interpretation, it is often necessary to switch between different basemaps to obtain different geographic information. Therefore, the leafmap package provides the split_map() method, which can split the original map output into two submaps, each applying a different basemap. Its sample code is as follows:\nm8 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m8.add_gdf(gdf_air, layer_name=\"EPA Station\") m8.split_map( left_layer=\"SATELLITE\", right_layer=\"Stamen.Terrain\" ) m8 Leafmap for Web Applications In order to quickly share the processed map information, Leafmap suite also provides an integrated way of Streamlit suite, combining Leafmap’s GIS technical expertise with Streamlit’s web technical expertise to quickly build a Web GIS system. Below we demonstrate how it works through a simple example, you can extend and build your own Web GIS service according to this principle.\nIn the use of the Streamlit package, there are two steps to build a web system:\nPackage the Python program to be executed into a Streamlit object, and write the packaging process into the app.py file; and Execute app.py on the system. Since our operation process all use the Google Colab platform, in this platform we can directly write app.py into the temporary storage area with the special syntax %%writefile, and then Colab directly reads and runs the codes from the temporary storage area. Therefore, for the file writing part of step 1, we can proceed as follows:\n%%writefile app.py import streamlit as st import leafmap.foliumap as leafmap import json import pandas as pd import geopandas as gpd from pyCIOT.data import * contnet = \"\"\" Hello World! \"\"\" st.title('Streamlit Demo') st.write(\"## Leafmap Example\") st.markdown(contnet) epa_station = Air().get_data(src=\"OBS:EPA\") from pandas import json_normalize df_air = json_normalize(epa_station) geodata_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') with st.expander(\"See source code\"): with st.echo(): m = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m.add_gdf(geodata_air, layer_name=\"EPA Station\") m.to_streamlit() For the second part, we use the following instructions:\n!streamlit run app.py \u0026 npx localtunnel --port 8501 After execution, an execution result similar to the following will appear:\nThen you can click on the URL after the string “your url is:”, and something similar to the following will appear in the browser\nFinally, we click “Click to Continue” to execute the Python code packaged in app.py. In this example, we can see the distribution map of EPA’s air quality monitoring stations presented by the leafmap package.\nConclusion In this article, we introduced the Leafmap package to render geographic data and integrate external resources, and demonstrated the combination of the Leagmap and Streamlit packages to build a simple web-based GIS service on the Google Colab platform. It should be noted that Leafmap also has many more advanced functions, which are not introduced in this article. You can refer to the following references for more in-depth and extensive learning.\nReferences Leafmap Tutorial (https://www.youtube.com/watch?v=-UPt7x3Gn60\u0026list=PLAxJ4-o7ZoPeMITwB8eyynOG0-CY3CMdw) leafmap: A Python package for geospatial analysis and interactive mapping in a Jupyter environment (https://leafmap.org/) Streamlit Tutorial (https://www.youtube.com/watch?v=fTzlyayFXBM) Map features - OpenStreetMap Wiki (https://wiki.openstreetmap.org/wiki/Map_features) Heat map - Wikipedia (https://en.wikipedia.org/wiki/Heat_map) ",
    "description": "We introduce the capability of leafmap package to use different types of data for geographic information representation and spatial analysis in Civil IoT Taiwan Data Service Platform, and demonstrate the combination of leafmap and streamlit packages to build Web GIS applications. Through cross-domain and cross-tool resource integration, readers will be able to expand their imagination of the future of data analysis and information services.",
    "tags": [
      "Python",
      "Air",
      "Quake"
    ],
    "title": "7.3. Leafmap Applications",
    "uri": "/en/ch7/ch7.3/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "advanced",
    "uri": "/en/levels/advanced/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Air",
    "uri": "/en/tags/air/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "API",
    "uri": "/en/tags/api/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Author",
    "uri": "/en/author/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "beginner",
    "uri": "/en/levels/beginner/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/en/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Disaster",
    "uri": "/en/tags/disaster/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Huei-Jun Kao",
    "uri": "/en/author/huei-jun-kao/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "intermediate",
    "uri": "/en/levels/intermediate/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/en/tags/introduction/"
  },
  {
    "content": "Learning Civil IoT Taiwan Open Data and Its Applications The “Civil IoT Taiwan” project is developed to address the four urgent needs of the public in order to integrate and close to the public services related to people’s livelihood, including air quality, earthquake, water resources, disaster prevention and other issues. In the “Digital Technology” section of the “Forward-looking Infrastructure Development Program” project, the Ministry of Science and Technology, the Ministry of Communications, the Ministry of Economic Affairs, the Ministry of the Interior, the Environmental Protection Agency, the Academia Sinica, and the Council of Agriculture have jointly constructed a large-scale inter-ministerial government project, which applies big data, artificial intelligence, and Internet of things technologies to build various smart life service systems to help the government and the public face the challenges brought about by environmental changes; at the same time, this project also considers the experience of different end-users, including government decision-making units, academia, industry, and the general public. The goal is to enhance intelligent governance, assist the development of industry/academia, and improve the happiness of the public.\nThe Civil IoT Taiwan project has also developed the “Civil IoT Taiwan Data Service Platform”, which provides real-time data access and historical data query services in a unified data format, improves user browsing and search speed, and promotes applications of model-based scientific computing and artificial intelligence. The platform not only collects various types of data generated by the Civil IoT Taiwan project, but also provides reliable and high-quality sensing data for various environmental management purposes. In addition, it narrows the gap of environmental information and provides more real-time and comprehensive environmental data, so that the public can view real-time sensor information at any time and observe the temporal and spatial changes of the surrounding environment. The data service platform also serves as the basis for the development of value-added applications, giving full play to the creativity of the people and producing high-quality services that solve the problems of the masses.\nIn order to maintain the good basis for the development of the Civil IoT Taiwan project and its data platform, and to continue to take root and cultivate more diverse user groups to step into the various application areas of the Civil IoT Taiwan project, there are three goals of this project: 1) Rooted downward, providing step-by-step self-study learning materials for college and high school students, and conducting cross-domain learning across the fields of information, geography, earth science, and humanities and society; 2) Demonstration application, aiming at the existing applications of Civil IoT Taiwan, in the way of stripping the cocoon, lowering the entry threshold and guiding deeper innovation of production system; 3) Horizontal expansion, providing additional data access methods, using the popular Python language, redeveloping the data access suite, drawing on more diverse technical energy, and enriching the user group of the data platform.\n",
    "description": "",
    "tags": null,
    "title": "Learning Civil IoT Taiwan Open Data and Its Applications",
    "uri": "/en/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Levels",
    "uri": "/en/levels/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Ling-Jyh Chen",
    "uri": "/en/author/ling-jyh-chen/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Ming-Kuang Chung",
    "uri": "/en/author/ming-kuang-chung/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Python",
    "uri": "/en/tags/python/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Quake",
    "uri": "/en/tags/quake/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Quen Luo",
    "uri": "/en/author/quen-luo/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Sky Hung",
    "uri": "/en/author/sky-hung/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/en/tags/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Water",
    "uri": "/en/tags/water/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Yu-Chi Peng",
    "uri": "/en/author/yu-chi-peng/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Yu-Shen Cheng",
    "uri": "/en/author/yu-shen-cheng/"
  }
]
